{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]"},"docs":[{"location":"","text":"Quix Documentation Portal Tutorials Use these guides to learn how to build data-driven apps and integrate Quix with external systems. Sentiment Analysis Stream data from Twitter and build a Sentiment analysis pipeline Sentiment Analysis Image Processing Real time image processing using Londons 'Jam Cams' Image Processing Core resources Take a look under the hood and get to know our SDK and APIs Connect to Quix - SDK Discover how to connect Quix and your application using our SDK Learn more Writing data - SDK Read how to send real-time data to Kafka topics using the Quix SDK Learn more Reading data - SDK Learn how to receive real-time data in your application using the Quix SDK Learn more Streaming Writer API Stream data to Quix Kafka topics via HTTP with this API Learn more Streaming Reader API Work with this API to receive live data in your Web applications from Quix Kafka topics via Websockets Learn more Data Catalogue API Query historic time-series data in Quix using HTTP interface Learn more","title":"Intro"},{"location":"#quix-documentation-portal","text":"","title":"Quix Documentation Portal"},{"location":"#tutorials","text":"Use these guides to learn how to build data-driven apps and integrate Quix with external systems. Sentiment Analysis Stream data from Twitter and build a Sentiment analysis pipeline Sentiment Analysis Image Processing Real time image processing using Londons 'Jam Cams' Image Processing","title":"Tutorials"},{"location":"#core-resources","text":"Take a look under the hood and get to know our SDK and APIs Connect to Quix - SDK Discover how to connect Quix and your application using our SDK Learn more Writing data - SDK Read how to send real-time data to Kafka topics using the Quix SDK Learn more Reading data - SDK Learn how to receive real-time data in your application using the Quix SDK Learn more Streaming Writer API Stream data to Quix Kafka topics via HTTP with this API Learn more Streaming Reader API Work with this API to receive live data in your Web applications from Quix Kafka topics via Websockets Learn more Data Catalogue API Query historic time-series data in Quix using HTTP interface Learn more","title":"Core resources"},{"location":"apis/","text":"The Quix Platform provides the following APIs. Data Catalogue The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities. Streaming Writer The Streaming Writer API allows you to stream data into the Quix platform via HTTP. It\u2019s an alternative to using our C# and Python SDKs. You can use the Streaming Writer API from any HTTP-capable language. Streaming Reader As an alternative to the SDK, the Quix platform supports real-time data streaming over WebSockets, via the Streaming Reader API . Clients can receive updates on data and definitions for parameters and events, as they happen. The examples use the Microsoft SignalR JavaScript client library. Portal API The Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects.","title":"Index"},{"location":"apis/#data-catalogue","text":"The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities.","title":"Data Catalogue"},{"location":"apis/#streaming-writer","text":"The Streaming Writer API allows you to stream data into the Quix platform via HTTP. It\u2019s an alternative to using our C# and Python SDKs. You can use the Streaming Writer API from any HTTP-capable language.","title":"Streaming Writer"},{"location":"apis/#streaming-reader","text":"As an alternative to the SDK, the Quix platform supports real-time data streaming over WebSockets, via the Streaming Reader API . Clients can receive updates on data and definitions for parameters and events, as they happen. The examples use the Microsoft SignalR JavaScript client library.","title":"Streaming Reader"},{"location":"apis/#portal-api","text":"The Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects.","title":"Portal API"},{"location":"apis/portal-api/","text":"Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects. Refer to Portal API Swagger for more information.","title":"Portal api"},{"location":"apis/data-catalogue-api/aggregate-tags/","text":"Aggregate data by tags If you need to compare data across different values for a given tag, you\u2019ll want to group results by that tag. You can do so via the /parameters/data endpoint. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Using the groupBy property You can supply a list of Tags in the groupBy array to aggregate results by. For example, you could group a set of Speed readings by the LapNumber they occurred on using something like: { \"from\" : 1612191286000000000 , \"to\" : 1612191386000000000 , \"numericParameters\" : [{ \"parameterName\" : \"Speed\" }], \"groupBy\" : [ \"LapNumber\" ] } With these settings alone, we\u2019ll get the LapNumber tag included in our results, alongside the existing timestamps and requested parameters, e.g. { \"timestamps\" : [ 1612191286000000000 , 1612191287000000000 , ... ], \"numericValues\" : { \"Speed\" : [ 307.8333333333333 , 313.8421052631579 , ... ] }, \"tagValues\" : { \"LapNumber\" : [ \"3.0\" , \"4.0\" , ... ] } } Using aggregationType For meaningful aggregations, you should specify a type of aggregation function for each parameter. When specifying the parameters to receive, include the aggregationType in each parameter object like so: \"numericParameters\" : [{ \"parameterName\" : \"Speed\" , \"aggregationType\" : \"mean\" }] Ten standard aggregation functions are provided including max , count , and spread . When you group by a tag and specify how to aggregate parameter values, the result will represent that aggregation. For example, the following results demonstrate the average speed that was recorded against each lap: { \"timestamps\" : [ 1612191286000000000 , 1612191286000000000 ], \"numericValues\" : { \"mean(Speed)\" : [ 213.36765142704252 , 173.77710595934278 ] }, \"stringValues\" : {}, \"binaryValues\" : {}, \"tagValues\" : { \"LapNumber\" : [ \"3.0\" , \"4.0\" ] } }","title":"Aggregate data by tags"},{"location":"apis/data-catalogue-api/aggregate-tags/#aggregate-data-by-tags","text":"If you need to compare data across different values for a given tag, you\u2019ll want to group results by that tag. You can do so via the /parameters/data endpoint.","title":"Aggregate data by tags"},{"location":"apis/data-catalogue-api/aggregate-tags/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/aggregate-tags/#using-the-groupby-property","text":"You can supply a list of Tags in the groupBy array to aggregate results by. For example, you could group a set of Speed readings by the LapNumber they occurred on using something like: { \"from\" : 1612191286000000000 , \"to\" : 1612191386000000000 , \"numericParameters\" : [{ \"parameterName\" : \"Speed\" }], \"groupBy\" : [ \"LapNumber\" ] } With these settings alone, we\u2019ll get the LapNumber tag included in our results, alongside the existing timestamps and requested parameters, e.g. { \"timestamps\" : [ 1612191286000000000 , 1612191287000000000 , ... ], \"numericValues\" : { \"Speed\" : [ 307.8333333333333 , 313.8421052631579 , ... ] }, \"tagValues\" : { \"LapNumber\" : [ \"3.0\" , \"4.0\" , ... ] } }","title":"Using the groupBy property"},{"location":"apis/data-catalogue-api/aggregate-tags/#using-aggregationtype","text":"For meaningful aggregations, you should specify a type of aggregation function for each parameter. When specifying the parameters to receive, include the aggregationType in each parameter object like so: \"numericParameters\" : [{ \"parameterName\" : \"Speed\" , \"aggregationType\" : \"mean\" }] Ten standard aggregation functions are provided including max , count , and spread . When you group by a tag and specify how to aggregate parameter values, the result will represent that aggregation. For example, the following results demonstrate the average speed that was recorded against each lap: { \"timestamps\" : [ 1612191286000000000 , 1612191286000000000 ], \"numericValues\" : { \"mean(Speed)\" : [ 213.36765142704252 , 173.77710595934278 ] }, \"stringValues\" : {}, \"binaryValues\" : {}, \"tagValues\" : { \"LapNumber\" : [ \"3.0\" , \"4.0\" ] } }","title":"Using aggregationType"},{"location":"apis/data-catalogue-api/aggregate-time/","text":"Aggregate data by time You can downsample and upsample data from the catalogue using the /parameters/data endpoint. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Aggregating and interpolating The JSON payload can include a groupByTime property, an object with the following members: timeBucketDuration The duration, in nanoseconds, for one aggregated value. interpolationType Specify how additional values should be generated when interpolating. For example, imagine you have a set of speed data, with values recorded at 1-second intervals. You can group such data into 2-second intervals, aggregated by mean average, with the following: { \"groupByTime\" : { \"timeBucketDuration\" : 2000000000 , }, \"numericParameters\" : [{ \"parameterName\" : \"Speed\" , \"aggregationType\" : \"Mean\" }] } You can specify an interpolationType to define how any missing values are generated. Linear will provide a value in linear proportion, whilst Previous will repeat the value before the one that was missing. { \"from\" : 1612191286000000000 , \"to\" : 1612191295000000000 , \"numericParameters\" : [{ \"parameterName\" : \"Speed\" , \"aggregationType\" : \"First\" }], \"groupByTime\" : { \"timeBucketDuration\" : 2000000000 , \"interpolationType\" : \"None\" }, \"streamIds\" : [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" ] }","title":"Aggregate data by time"},{"location":"apis/data-catalogue-api/aggregate-time/#aggregate-data-by-time","text":"You can downsample and upsample data from the catalogue using the /parameters/data endpoint.","title":"Aggregate data by time"},{"location":"apis/data-catalogue-api/aggregate-time/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/aggregate-time/#aggregating-and-interpolating","text":"The JSON payload can include a groupByTime property, an object with the following members: timeBucketDuration The duration, in nanoseconds, for one aggregated value. interpolationType Specify how additional values should be generated when interpolating. For example, imagine you have a set of speed data, with values recorded at 1-second intervals. You can group such data into 2-second intervals, aggregated by mean average, with the following: { \"groupByTime\" : { \"timeBucketDuration\" : 2000000000 , }, \"numericParameters\" : [{ \"parameterName\" : \"Speed\" , \"aggregationType\" : \"Mean\" }] } You can specify an interpolationType to define how any missing values are generated. Linear will provide a value in linear proportion, whilst Previous will repeat the value before the one that was missing. { \"from\" : 1612191286000000000 , \"to\" : 1612191295000000000 , \"numericParameters\" : [{ \"parameterName\" : \"Speed\" , \"aggregationType\" : \"First\" }], \"groupByTime\" : { \"timeBucketDuration\" : 2000000000 , \"interpolationType\" : \"None\" }, \"streamIds\" : [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" ] }","title":"Aggregating and interpolating"},{"location":"apis/data-catalogue-api/authenticate/","text":"Authenticate Before you begin Sign up on the Quix Portal Get a Personal Access Token You should authenticate requests to the Catalogue API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary. Follow these steps to generate a PAT: Click the user icon in the top-right of the Portal and select the Tokens menu. Click GENERATE TOKEN . Choose a name to describe the token\u2019s purpose, and an expiration date, then click CREATE . Copy the token and store it in a secure place. Warning You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy. Warning Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs. Sign all your requests using this token Make sure you accompany each request to the API with an Authorization header using your PAT as a bearer token, as follows: Authorization: bearer <token> Replace <token> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the -H flag: curl -H \"Authorization: bearer <token>\" ... Warning If you fail to send a valid Authorization header, the API will respond with a 401 UNAUTHORIZED status code.","title":"Authenticate"},{"location":"apis/data-catalogue-api/authenticate/#authenticate","text":"","title":"Authenticate"},{"location":"apis/data-catalogue-api/authenticate/#before-you-begin","text":"Sign up on the Quix Portal","title":"Before you begin"},{"location":"apis/data-catalogue-api/authenticate/#get-a-personal-access-token","text":"You should authenticate requests to the Catalogue API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary. Follow these steps to generate a PAT: Click the user icon in the top-right of the Portal and select the Tokens menu. Click GENERATE TOKEN . Choose a name to describe the token\u2019s purpose, and an expiration date, then click CREATE . Copy the token and store it in a secure place. Warning You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy. Warning Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.","title":"Get a Personal Access Token"},{"location":"apis/data-catalogue-api/authenticate/#sign-all-your-requests-using-this-token","text":"Make sure you accompany each request to the API with an Authorization header using your PAT as a bearer token, as follows: Authorization: bearer <token> Replace <token> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the -H flag: curl -H \"Authorization: bearer <token>\" ... Warning If you fail to send a valid Authorization header, the API will respond with a 401 UNAUTHORIZED status code.","title":"Sign all your requests using this token"},{"location":"apis/data-catalogue-api/filter-tags/","text":"Tag filtering If you supply Tags with your parameter data, they will act as indexes, so they can be used to efficiently filter data. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Using tag filters When calling the /parameters/data endpoint, you can include a tagFilters property in your payload. This property references an array of objects, each with the following structure: tag The name of the tag to filter by operator A comparison operator value The value to compare against For example, to fetch only the data recorded on the second lap, we can filter on a LapNumber tag as follows: { \"tagFilters\" : [{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : \"2.0\" }] } Note that the value can also be an array, in which case data that matches the chosen operator for any value is returned: { \"tagFilters\" : [{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : [ \"2.0\" , \"4.0\" ] }] } But also note that multiple filters for the same tag apply in combination, so: { \"tagFilters\" : [{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : \"2.0\" },{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : \"4.0\" }] } Is useless because a LapNumber cannot be both \"2.0\" and \"4.0\". Supported operators Each object in the tagFilters array can support the following operator values: Equal NotEqual Like NotLike Equal and NotEqual test for true/false exact string matches. Like and NotLike will perform a regular expression match, so you can search by pattern. For example, to get the Speed parameter values tagged with a LapNumber which is either 2 or 4, you can use the expression \"^[24]\\.\" to match values 2.0 and 4.0: curl \"https://telemetry-query-testing-quickstart.platform.quix.ai/parameters/data\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"tagFilters\": [{ \"tag\": \"LapNumber\", \"operator\": \"Like\", \"value\": \"^[24]\\\\.\" }], \"numericParameters\": [{\"parameterName\": \"Speed\"}], \"from\": 1612191182000000000, \"to\": 1612191189000000000 }'","title":"Tag filtering"},{"location":"apis/data-catalogue-api/filter-tags/#tag-filtering","text":"If you supply Tags with your parameter data, they will act as indexes, so they can be used to efficiently filter data.","title":"Tag filtering"},{"location":"apis/data-catalogue-api/filter-tags/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/filter-tags/#using-tag-filters","text":"When calling the /parameters/data endpoint, you can include a tagFilters property in your payload. This property references an array of objects, each with the following structure: tag The name of the tag to filter by operator A comparison operator value The value to compare against For example, to fetch only the data recorded on the second lap, we can filter on a LapNumber tag as follows: { \"tagFilters\" : [{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : \"2.0\" }] } Note that the value can also be an array, in which case data that matches the chosen operator for any value is returned: { \"tagFilters\" : [{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : [ \"2.0\" , \"4.0\" ] }] } But also note that multiple filters for the same tag apply in combination, so: { \"tagFilters\" : [{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : \"2.0\" },{ \"tag\" : \"LapNumber\" , \"operator\" : \"Equal\" , \"value\" : \"4.0\" }] } Is useless because a LapNumber cannot be both \"2.0\" and \"4.0\".","title":"Using tag filters"},{"location":"apis/data-catalogue-api/filter-tags/#supported-operators","text":"Each object in the tagFilters array can support the following operator values: Equal NotEqual Like NotLike Equal and NotEqual test for true/false exact string matches. Like and NotLike will perform a regular expression match, so you can search by pattern. For example, to get the Speed parameter values tagged with a LapNumber which is either 2 or 4, you can use the expression \"^[24]\\.\" to match values 2.0 and 4.0: curl \"https://telemetry-query-testing-quickstart.platform.quix.ai/parameters/data\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"tagFilters\": [{ \"tag\": \"LapNumber\", \"operator\": \"Like\", \"value\": \"^[24]\\\\.\" }], \"numericParameters\": [{\"parameterName\": \"Speed\"}], \"from\": 1612191182000000000, \"to\": 1612191189000000000 }'","title":"Supported operators"},{"location":"apis/data-catalogue-api/get-swagger/","text":"Getting Swagger url You can access documentation and a basic playground for the Data Catalogue API via Swagger . The exact URL is workspace-specific, and follows this pattern: https://telemetry-query-${organisation}-${workspace}.platform.quix.ai/ The workspace ID is a combination based on your organisation and workspace names. For example, for an \"acme\" organisation with a \"weather\" workspace, the final URL might look something like: https://telemetry-query-acme-weather.platform.quix.ai/ To determine the final URL, you can find out how to get a workspace id , or follow these instructions: Click the Data Explorer icon Data Explorer in the main menu. Click on CODE tab in the right top corner. Using the \"Select code language\" drop down on the right of this page, choose \"Swagger\". Click the link to access the Swagger documentation.","title":"Getting Swagger url"},{"location":"apis/data-catalogue-api/get-swagger/#getting-swagger-url","text":"You can access documentation and a basic playground for the Data Catalogue API via Swagger . The exact URL is workspace-specific, and follows this pattern: https://telemetry-query-${organisation}-${workspace}.platform.quix.ai/ The workspace ID is a combination based on your organisation and workspace names. For example, for an \"acme\" organisation with a \"weather\" workspace, the final URL might look something like: https://telemetry-query-acme-weather.platform.quix.ai/ To determine the final URL, you can find out how to get a workspace id , or follow these instructions: Click the Data Explorer icon Data Explorer in the main menu. Click on CODE tab in the right top corner. Using the \"Select code language\" drop down on the right of this page, choose \"Swagger\". Click the link to access the Swagger documentation.","title":"Getting Swagger url"},{"location":"apis/data-catalogue-api/intro/","text":"Introduction The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities. The API is fully described in our Swagger documentation . Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using curl . Preparation Before using any of the endpoints, you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API . You\u2019ll also need to have some data stored in the Quix platform for API use to be meaningful. You can use any Source of our Quix Library to do this using the Quix portal. Topics covered Topic Endpoint Examples Streams, paged /streams Get all streams in groups of ten per page Streams, filtered /streams Get a single stream, by ID Get only the streams with LapNumber data Streams & models /streams/models Get stream hierarchy Raw data /parameters/data Get all the Speed readings Get Speed data between timestamps Aggregated data by time /parameters/data Downsample or upsample data Aggregated by tags /parameters/data Show average Speed by LapNumber Tag filtering /parameters/data Get data for just one Lap","title":"Introduction"},{"location":"apis/data-catalogue-api/intro/#introduction","text":"The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities. The API is fully described in our Swagger documentation . Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using curl .","title":"Introduction"},{"location":"apis/data-catalogue-api/intro/#preparation","text":"Before using any of the endpoints, you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API . You\u2019ll also need to have some data stored in the Quix platform for API use to be meaningful. You can use any Source of our Quix Library to do this using the Quix portal.","title":"Preparation"},{"location":"apis/data-catalogue-api/intro/#topics-covered","text":"Topic Endpoint Examples Streams, paged /streams Get all streams in groups of ten per page Streams, filtered /streams Get a single stream, by ID Get only the streams with LapNumber data Streams & models /streams/models Get stream hierarchy Raw data /parameters/data Get all the Speed readings Get Speed data between timestamps Aggregated data by time /parameters/data Downsample or upsample data Aggregated by tags /parameters/data Show average Speed by LapNumber Tag filtering /parameters/data Get data for just one Lap","title":"Topics covered"},{"location":"apis/data-catalogue-api/raw-data/","text":"Raw data Access persisted raw data by specifyng the parameters you\u2019re interested in. Add restrictions based on Stream or timings for finer-grained results. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Using the /parameters/data endpoint Raw telemetry data is available via the /parameters/data endpoint. Fetching specific parameters Request You can filter by a number of different factors but, at minimum, you\u2019ll need to supply one or more parameters to fetch: { \"numericParameters\" : [{ \"parameterName\" : \"Speed\" }] } In this example, we\u2019re requesting a single numeric parameter, Speed . Each array of parameters is indexed based on parameter type, which can be numericParameters , stringParameters or binaryParameters . Parameters are returned in a union, so if you request several, you\u2019ll get back all parameters that match. Example curl \"https:// ${ domain } .platform.quix.ai/parameters/data\" \\ -H \"accept: text/plain\" \\ -H \"Authorization: bearer <token>\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numericParameters\":[{\"parameterName\":\"Speed\"}]}' If you just had a single parameter value in the catalogue, the response from the above call might look something like this: { \"timestamps\" : [ 1612191100000000000 ], \"numericValues\" : { \"Speed\" : [ 104.22222222222224 ] }, \"stringValues\" : {}, \"binaryValues\" : {}, \"tagValues\" : {}, } Restricting by Stream or time In reality, you\u2019ll have far more data in the catalogue, so you\u2019ll want to filter it. Three remaining properties of the request object allow you to do so: streamIds from to Each stream you create has a unique ID. You can view the ID of a persisted via the Data section of the Quix Portal. Supply a list of stream IDs to restrict fetched data to just those streams: { \"streamIds\" : [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" , \"9feb07ac-b0b2-4591-bc7f-8f0c1295ed7c\" ] } You can also restrict data to a certain time span using the from and to properties. These each expect a timestamp in nanoseconds, for example: { \"from\" : 1612191286000000000 , \"to\" : 1612191386000000000 } These timestamps cover a range of 100 seconds.","title":"Raw data"},{"location":"apis/data-catalogue-api/raw-data/#raw-data","text":"Access persisted raw data by specifyng the parameters you\u2019re interested in. Add restrictions based on Stream or timings for finer-grained results.","title":"Raw data"},{"location":"apis/data-catalogue-api/raw-data/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/raw-data/#using-the-parametersdata-endpoint","text":"Raw telemetry data is available via the /parameters/data endpoint.","title":"Using the /parameters/data endpoint"},{"location":"apis/data-catalogue-api/raw-data/#fetching-specific-parameters","text":"","title":"Fetching specific parameters"},{"location":"apis/data-catalogue-api/raw-data/#request","text":"You can filter by a number of different factors but, at minimum, you\u2019ll need to supply one or more parameters to fetch: { \"numericParameters\" : [{ \"parameterName\" : \"Speed\" }] } In this example, we\u2019re requesting a single numeric parameter, Speed . Each array of parameters is indexed based on parameter type, which can be numericParameters , stringParameters or binaryParameters . Parameters are returned in a union, so if you request several, you\u2019ll get back all parameters that match.","title":"Request"},{"location":"apis/data-catalogue-api/raw-data/#example","text":"curl \"https:// ${ domain } .platform.quix.ai/parameters/data\" \\ -H \"accept: text/plain\" \\ -H \"Authorization: bearer <token>\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numericParameters\":[{\"parameterName\":\"Speed\"}]}' If you just had a single parameter value in the catalogue, the response from the above call might look something like this: { \"timestamps\" : [ 1612191100000000000 ], \"numericValues\" : { \"Speed\" : [ 104.22222222222224 ] }, \"stringValues\" : {}, \"binaryValues\" : {}, \"tagValues\" : {}, }","title":"Example"},{"location":"apis/data-catalogue-api/raw-data/#restricting-by-stream-or-time","text":"In reality, you\u2019ll have far more data in the catalogue, so you\u2019ll want to filter it. Three remaining properties of the request object allow you to do so: streamIds from to Each stream you create has a unique ID. You can view the ID of a persisted via the Data section of the Quix Portal. Supply a list of stream IDs to restrict fetched data to just those streams: { \"streamIds\" : [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" , \"9feb07ac-b0b2-4591-bc7f-8f0c1295ed7c\" ] } You can also restrict data to a certain time span using the from and to properties. These each expect a timestamp in nanoseconds, for example: { \"from\" : 1612191286000000000 , \"to\" : 1612191386000000000 } These timestamps cover a range of 100 seconds.","title":"Restricting by Stream or time"},{"location":"apis/data-catalogue-api/request/","text":"Forming a request How you send requests to the Data Catalogue API will vary depending on the client or language you\u2019re using. But the API still has behaviour and expectations that is common across all clients. Tip The examples in this section show how to use the popular curl command line tool. Before you begin Sign up on the Quix Portal Read about Authenticating with the Data Catalogue API Endpoint URLs The Data Catalogue API is available on a per-workspace basis, so the subdomain is based on a combination of your organisation and workspace names. See How to get a workspace ID to find out how to get the exact hostname required. It will be in this format: https://telemetry-query-${organisation}-${workspace}.platform.quix.ai/ So your final endpoint URL will look something like: https://telemetry-query-acme-weather.platform.quix.ai/ Method Most endpoints use the POST method, even those that just fetch data. Ensure your HTTP client sends POST requests as appropriate. Using curl , the -X POST flag specifies a POST request. Note that this is optional if you\u2019re using the -d flag to send a payload (see below). curl -X POST ... Payload For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending: curl -H \"Content-Type: application/json\" ... Warning You must specify the content type of your payload. Failing to include this header will result in a 415 UNSUPPORTED MEDIA TYPE status code. You can send data via a POST request using the curl flag -d . This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data. curl -d '{\"key\": \"value\"}' ... curl -d \"@data.json\" ... Complete curl example You should structure most of your requests to the API around this pattern: curl -H \"Authorization: ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"@data.json\" \\ https:// ${ domain } .platform.quix.ai/ ${ endpoint }","title":"Forming a request"},{"location":"apis/data-catalogue-api/request/#forming-a-request","text":"How you send requests to the Data Catalogue API will vary depending on the client or language you\u2019re using. But the API still has behaviour and expectations that is common across all clients. Tip The examples in this section show how to use the popular curl command line tool.","title":"Forming a request"},{"location":"apis/data-catalogue-api/request/#before-you-begin","text":"Sign up on the Quix Portal Read about Authenticating with the Data Catalogue API","title":"Before you begin"},{"location":"apis/data-catalogue-api/request/#endpoint-urls","text":"The Data Catalogue API is available on a per-workspace basis, so the subdomain is based on a combination of your organisation and workspace names. See How to get a workspace ID to find out how to get the exact hostname required. It will be in this format: https://telemetry-query-${organisation}-${workspace}.platform.quix.ai/ So your final endpoint URL will look something like: https://telemetry-query-acme-weather.platform.quix.ai/","title":"Endpoint URLs"},{"location":"apis/data-catalogue-api/request/#method","text":"Most endpoints use the POST method, even those that just fetch data. Ensure your HTTP client sends POST requests as appropriate. Using curl , the -X POST flag specifies a POST request. Note that this is optional if you\u2019re using the -d flag to send a payload (see below). curl -X POST ...","title":"Method"},{"location":"apis/data-catalogue-api/request/#payload","text":"For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending: curl -H \"Content-Type: application/json\" ... Warning You must specify the content type of your payload. Failing to include this header will result in a 415 UNSUPPORTED MEDIA TYPE status code. You can send data via a POST request using the curl flag -d . This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data. curl -d '{\"key\": \"value\"}' ... curl -d \"@data.json\" ...","title":"Payload"},{"location":"apis/data-catalogue-api/request/#complete-curl-example","text":"You should structure most of your requests to the API around this pattern: curl -H \"Authorization: ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"@data.json\" \\ https:// ${ domain } .platform.quix.ai/ ${ endpoint }","title":"Complete curl example"},{"location":"apis/data-catalogue-api/streams-filtered/","text":"Filtered streams To fetch specific streams, you can include various filters with your request to the /streams endpoint. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Fetch a single stream via ID The most basic filter matches against a stream\u2019s ID. curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"streamIds\": [\"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\"]}' Note that you can supply multiple IDs in the streamIds array to match multiple streams. Filtering streams on basic properties The location of a stream defines its position in a hierarchy. A stream location looks just like a filesystem path. You can filter streams based on the start of this path, so you can easily find streams contained within any point in the hierarchy. For example, this query will find streams with a location of /one but it will also find streams with a /one/two location: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"location\": \"/one\"}' Warning Since this is just a basic prefix match, filtering on a location named /one will also bring back matches for the location /one111 as well as the location /one/111 . If you want to strictly filter on an exact directory (and below), make sure to include a trailing slash, e.g. /one/ . Note Filtering on topic uses a case insensitive Equals match. Filtering on a topic named \"MyTopic\" will match \"mytopic\" but will not match \"MyTopic123\" You can filter streams based on their use of a given parameter with the parameterIds property. For example, to find all streams that contain at least one single occurence of Gear data: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"parameterIds\": [ \"Gear\"] }' You can filter based on the presence or absence of a certain stream status , for example, if the stream is Open or was Interrupted . The includeStatuses and excludeStatuses properties each take an array of values to act on. So to get all streams that aren\u2019t Interrupted or Closed, use this query: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"excludeStatuses\": [ \"Interrupted\", \"Closed\" ]}' Filtering streams on metadata You can associate metadata with your streams. This can be used, for example, to store the circuit a car has travelled around, or the player of a particular run of a game. To filter on metadata, include the metadata property in the JSON object in your request body. This property\u2019s value is an array of objects, each of which has two properties, key and value : key The exact, case-sensitive key of the metadata you\u2019re interested in. value The exact, case-sensitive value of the metadata to match on If you have a metadata entry keyed as \"circuit\", you can match against it for an example value with this payload: \"metadata\" : [{ \"key\" : \"circuit\" , \"value\" : \"Sakhir Short\" }] As before, the response is an array of Stream objects: [{ \"streamId\" : \"e6545c18-d20d-47bd-8997-f3f825c1a45c\" , \"name\" : \"cardata\" , \"topic\" : \"cardata\" , \"createdAt\" : \"2021-03-31T13:04:43.368Z\" , \"lastUpdate\" : \"2021-03-31T13:04:44.53Z\" , \"dataStart\" : 1612191099000000000 , \"dataEnd\" : 1612191371000000000 , \"status\" : \"Closed\" , \"metadata\" :{ \"circuit\" : \"Sakhir Short\" , \"player\" : \"Swal\" , \"game\" : \"Codemasters F1 2019\" }, \"parents\" :[], \"location\" : \"/static data/\" }] Ordering results Calls to the /streams endpoint can include an ordering property in the payload. This references an array of properties to sort on, each one an object with the following properties: by A string representing the property to order by. direction A string, either \"Asc\" or \"Desc\", to define the sort direction. For example, to sort all streams in ascending order by topic: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"ordering\": [{ \"by\": \"topic\", \"direction\": \"asc\" }]}'","title":"Filtered streams"},{"location":"apis/data-catalogue-api/streams-filtered/#filtered-streams","text":"To fetch specific streams, you can include various filters with your request to the /streams endpoint.","title":"Filtered streams"},{"location":"apis/data-catalogue-api/streams-filtered/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/streams-filtered/#fetch-a-single-stream-via-id","text":"The most basic filter matches against a stream\u2019s ID. curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"streamIds\": [\"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\"]}' Note that you can supply multiple IDs in the streamIds array to match multiple streams.","title":"Fetch a single stream via ID"},{"location":"apis/data-catalogue-api/streams-filtered/#filtering-streams-on-basic-properties","text":"The location of a stream defines its position in a hierarchy. A stream location looks just like a filesystem path. You can filter streams based on the start of this path, so you can easily find streams contained within any point in the hierarchy. For example, this query will find streams with a location of /one but it will also find streams with a /one/two location: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"location\": \"/one\"}' Warning Since this is just a basic prefix match, filtering on a location named /one will also bring back matches for the location /one111 as well as the location /one/111 . If you want to strictly filter on an exact directory (and below), make sure to include a trailing slash, e.g. /one/ . Note Filtering on topic uses a case insensitive Equals match. Filtering on a topic named \"MyTopic\" will match \"mytopic\" but will not match \"MyTopic123\" You can filter streams based on their use of a given parameter with the parameterIds property. For example, to find all streams that contain at least one single occurence of Gear data: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"parameterIds\": [ \"Gear\"] }' You can filter based on the presence or absence of a certain stream status , for example, if the stream is Open or was Interrupted . The includeStatuses and excludeStatuses properties each take an array of values to act on. So to get all streams that aren\u2019t Interrupted or Closed, use this query: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"excludeStatuses\": [ \"Interrupted\", \"Closed\" ]}'","title":"Filtering streams on basic properties"},{"location":"apis/data-catalogue-api/streams-filtered/#filtering-streams-on-metadata","text":"You can associate metadata with your streams. This can be used, for example, to store the circuit a car has travelled around, or the player of a particular run of a game. To filter on metadata, include the metadata property in the JSON object in your request body. This property\u2019s value is an array of objects, each of which has two properties, key and value : key The exact, case-sensitive key of the metadata you\u2019re interested in. value The exact, case-sensitive value of the metadata to match on If you have a metadata entry keyed as \"circuit\", you can match against it for an example value with this payload: \"metadata\" : [{ \"key\" : \"circuit\" , \"value\" : \"Sakhir Short\" }] As before, the response is an array of Stream objects: [{ \"streamId\" : \"e6545c18-d20d-47bd-8997-f3f825c1a45c\" , \"name\" : \"cardata\" , \"topic\" : \"cardata\" , \"createdAt\" : \"2021-03-31T13:04:43.368Z\" , \"lastUpdate\" : \"2021-03-31T13:04:44.53Z\" , \"dataStart\" : 1612191099000000000 , \"dataEnd\" : 1612191371000000000 , \"status\" : \"Closed\" , \"metadata\" :{ \"circuit\" : \"Sakhir Short\" , \"player\" : \"Swal\" , \"game\" : \"Codemasters F1 2019\" }, \"parents\" :[], \"location\" : \"/static data/\" }]","title":"Filtering streams on metadata"},{"location":"apis/data-catalogue-api/streams-filtered/#ordering-results","text":"Calls to the /streams endpoint can include an ordering property in the payload. This references an array of properties to sort on, each one an object with the following properties: by A string representing the property to order by. direction A string, either \"Asc\" or \"Desc\", to define the sort direction. For example, to sort all streams in ascending order by topic: curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"ordering\": [{ \"by\": \"topic\", \"direction\": \"asc\" }]}'","title":"Ordering results"},{"location":"apis/data-catalogue-api/streams-models/","text":"Streams with models One stream can derive from another, for example, acting as a model in a pipeline. This relationship can be inspected using the /streams/models endpoint. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Fetching model data The hierarchy is represented as a parent/child structure where a stream can have an optional parent and any number of children. The /streams/models endpoint will return data in the same structure as the /streams endpoint , with an additional property for each stream: children . This is an array of stream objects which may have their own children. The payload requirements are the same as those for /streams . You can fetch model information across all streams with an empty payload: curl \"https:// ${ domain } .platform.quix.ai/streams/models\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"{}\" Here\u2019s an example result for a stream with two children: [{ \"children\" : [{ \"children\" : [], \"streamId\" : \"79bbed17-5c71-4b0e-99f6-3596577b46d8\" , \"name\" : \"new-child\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T15:27:09.19Z\" , \"lastUpdate\" : \"2021-04-13T10:21:52.572Z\" , \"status\" : \"Open\" , \"metadata\" : {}, \"parents\" : [ \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" ], \"location\" : \"/\" },{ \"children\" : [], \"streamId\" : \"f003c1dd-9abe-49dd-afd2-f194d3d96035\" , \"name\" : \"example1\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-12T11:50:38.504Z\" , \"lastUpdate\" : \"2021-04-12T12:00:40.482Z\" , \"status\" : \"Interrupted\" , \"metadata\" : { \"rain\" : \"light\" }, \"parents\" : [ \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" ], \"location\" : \"/examples/first/\" }], \"streamId\" : \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T14:12:29.807Z\" , \"lastUpdate\" : \"2021-04-12T13:45:08.377Z\" , \"timeOfRecording\" : \"2021-04-12T00:00:00Z\" , \"dataStart\" : 0 , \"dataEnd\" : 1618233869000000000 , \"status\" : \"Interrupted\" , \"metadata\" : {}, \"parents\" : [], \"location\" : \"/\" }] And here\u2019s an example with a child and a grandchild: [{ \"children\" : [{ \"children\" : [{ \"children\" : [], \"streamId\" : \"79bbed17-5c71-4b0e-99f6-3596577b46d8\" , \"name\" : \"new-child\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T15:27:09.19Z\" , \"lastUpdate\" : \"2021-04-13T10:30:11.495Z\" , \"status\" : \"Open\" , \"metadata\" : {}, \"parents\" : [ \"f003c1dd-9abe-49dd-afd2-f194d3d96035\" ], \"location\" : \"/\" } ], \"streamId\" : \"f003c1dd-9abe-49dd-afd2-f194d3d96035\" , \"name\" : \"example1\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-12T11:50:38.504Z\" , \"lastUpdate\" : \"2021-04-12T12:00:40.482Z\" , \"status\" : \"Interrupted\" , \"metadata\" : { \"rain\" : \"light\" }, \"parents\" : [ \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" ], \"location\" : \"/examples/first/\" } ], \"streamId\" : \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T14:12:29.807Z\" , \"lastUpdate\" : \"2021-04-12T13:45:08.377Z\" , \"timeOfRecording\" : \"2021-04-12T00:00:00Z\" , \"dataStart\" : 0 , \"dataEnd\" : 1618233869000000000 , \"status\" : \"Interrupted\" , \"metadata\" : {}, \"parents\" : [], \"location\" : \"/\" }]","title":"Streams with models"},{"location":"apis/data-catalogue-api/streams-models/#streams-with-models","text":"One stream can derive from another, for example, acting as a model in a pipeline. This relationship can be inspected using the /streams/models endpoint.","title":"Streams with models"},{"location":"apis/data-catalogue-api/streams-models/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/streams-models/#fetching-model-data","text":"The hierarchy is represented as a parent/child structure where a stream can have an optional parent and any number of children. The /streams/models endpoint will return data in the same structure as the /streams endpoint , with an additional property for each stream: children . This is an array of stream objects which may have their own children. The payload requirements are the same as those for /streams . You can fetch model information across all streams with an empty payload: curl \"https:// ${ domain } .platform.quix.ai/streams/models\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"{}\" Here\u2019s an example result for a stream with two children: [{ \"children\" : [{ \"children\" : [], \"streamId\" : \"79bbed17-5c71-4b0e-99f6-3596577b46d8\" , \"name\" : \"new-child\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T15:27:09.19Z\" , \"lastUpdate\" : \"2021-04-13T10:21:52.572Z\" , \"status\" : \"Open\" , \"metadata\" : {}, \"parents\" : [ \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" ], \"location\" : \"/\" },{ \"children\" : [], \"streamId\" : \"f003c1dd-9abe-49dd-afd2-f194d3d96035\" , \"name\" : \"example1\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-12T11:50:38.504Z\" , \"lastUpdate\" : \"2021-04-12T12:00:40.482Z\" , \"status\" : \"Interrupted\" , \"metadata\" : { \"rain\" : \"light\" }, \"parents\" : [ \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" ], \"location\" : \"/examples/first/\" }], \"streamId\" : \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T14:12:29.807Z\" , \"lastUpdate\" : \"2021-04-12T13:45:08.377Z\" , \"timeOfRecording\" : \"2021-04-12T00:00:00Z\" , \"dataStart\" : 0 , \"dataEnd\" : 1618233869000000000 , \"status\" : \"Interrupted\" , \"metadata\" : {}, \"parents\" : [], \"location\" : \"/\" }] And here\u2019s an example with a child and a grandchild: [{ \"children\" : [{ \"children\" : [{ \"children\" : [], \"streamId\" : \"79bbed17-5c71-4b0e-99f6-3596577b46d8\" , \"name\" : \"new-child\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T15:27:09.19Z\" , \"lastUpdate\" : \"2021-04-13T10:30:11.495Z\" , \"status\" : \"Open\" , \"metadata\" : {}, \"parents\" : [ \"f003c1dd-9abe-49dd-afd2-f194d3d96035\" ], \"location\" : \"/\" } ], \"streamId\" : \"f003c1dd-9abe-49dd-afd2-f194d3d96035\" , \"name\" : \"example1\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-12T11:50:38.504Z\" , \"lastUpdate\" : \"2021-04-12T12:00:40.482Z\" , \"status\" : \"Interrupted\" , \"metadata\" : { \"rain\" : \"light\" }, \"parents\" : [ \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" ], \"location\" : \"/examples/first/\" } ], \"streamId\" : \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" , \"topic\" : \"cars\" , \"createdAt\" : \"2021-04-08T14:12:29.807Z\" , \"lastUpdate\" : \"2021-04-12T13:45:08.377Z\" , \"timeOfRecording\" : \"2021-04-12T00:00:00Z\" , \"dataStart\" : 0 , \"dataEnd\" : 1618233869000000000 , \"status\" : \"Interrupted\" , \"metadata\" : {}, \"parents\" : [], \"location\" : \"/\" }]","title":"Fetching model data"},{"location":"apis/data-catalogue-api/streams-paged/","text":"Paged streams You can fetch all streams within a workspace , across topics and locations, with a single call. If you\u2019re working with a large number of streams, you can use pagination parameters to group the results into smaller pages. Before you begin If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request. Fetching all streams The /streams endpoint provides read access to all streams within the workspace. Sending an empty JSON object in your request body will return all streams. Warning Even if you\u2019re not supplying any parameters, you must still send a valid empty object as JSON data in the body of your request. Example request curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"{}\" Example response The JSON returned consists of an array of Stream objects: [{ \"streamId\" : \"e6545c18-d20d-47bd-8997-f3f825c1a45c\" , \"name\" : \"cardata\" , \"topic\" : \"cardata\" , \"createdAt\" : \"2021-03-31T13:04:43.368Z\" , \"lastUpdate\" : \"2021-03-31T13:04:44.53Z\" , \"dataStart\" : 1612191099000000000 , \"dataEnd\" : 1612191371000000000 , \"status\" : \"Closed\" , \"metadata\" :{}, \"parents\" :[], \"location\" : \"/static data/\" }] Fetching streams page by page To reduce the size of the response, you should page these results with the paging property. Include this in the JSON object you send in the body of your request. The value of this property is an object with two members, index and length : index The index of the page you want returned. length The number of items (i.e. streams) per page. For example, to group all streams in pages of 10 and receive the 2nd page, use this value: \"paging\" : { \"index\" : 1 , \"length\" : 10 } Example request curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"paging\":{\"index\": 1,\"length\": 10}}'","title":"Paged streams"},{"location":"apis/data-catalogue-api/streams-paged/#paged-streams","text":"You can fetch all streams within a workspace , across topics and locations, with a single call. If you\u2019re working with a large number of streams, you can use pagination parameters to group the results into smaller pages.","title":"Paged streams"},{"location":"apis/data-catalogue-api/streams-paged/#before-you-begin","text":"If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up. Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/data-catalogue-api/streams-paged/#fetching-all-streams","text":"The /streams endpoint provides read access to all streams within the workspace. Sending an empty JSON object in your request body will return all streams. Warning Even if you\u2019re not supplying any parameters, you must still send a valid empty object as JSON data in the body of your request.","title":"Fetching all streams"},{"location":"apis/data-catalogue-api/streams-paged/#example-request","text":"curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"{}\"","title":"Example request"},{"location":"apis/data-catalogue-api/streams-paged/#example-response","text":"The JSON returned consists of an array of Stream objects: [{ \"streamId\" : \"e6545c18-d20d-47bd-8997-f3f825c1a45c\" , \"name\" : \"cardata\" , \"topic\" : \"cardata\" , \"createdAt\" : \"2021-03-31T13:04:43.368Z\" , \"lastUpdate\" : \"2021-03-31T13:04:44.53Z\" , \"dataStart\" : 1612191099000000000 , \"dataEnd\" : 1612191371000000000 , \"status\" : \"Closed\" , \"metadata\" :{}, \"parents\" :[], \"location\" : \"/static data/\" }]","title":"Example response"},{"location":"apis/data-catalogue-api/streams-paged/#fetching-streams-page-by-page","text":"To reduce the size of the response, you should page these results with the paging property. Include this in the JSON object you send in the body of your request. The value of this property is an object with two members, index and length : index The index of the page you want returned. length The number of items (i.e. streams) per page. For example, to group all streams in pages of 10 and receive the 2nd page, use this value: \"paging\" : { \"index\" : 1 , \"length\" : 10 }","title":"Fetching streams page by page"},{"location":"apis/data-catalogue-api/streams-paged/#example-request_1","text":"curl \"https:// ${ domain } .platform.quix.ai/streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"paging\":{\"index\": 1,\"length\": 10}}'","title":"Example request"},{"location":"apis/streaming-reader-api/authenticate/","text":"Authenticate Before you begin Sign up on the Quix Portal Get a Personal Access Token You should authenticate requests to the Streaming Reader API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary. Follow these steps to generate a PAT: Click the user icon in the top-right of the Portal and select the Tokens menu. Click GENERATE TOKEN . Choose a name to describe the token\u2019s purpose, and an expiration date, then click CREATE . Copy the token and store it in a secure place. Warning You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy. Warning Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.","title":"Authenticate"},{"location":"apis/streaming-reader-api/authenticate/#authenticate","text":"","title":"Authenticate"},{"location":"apis/streaming-reader-api/authenticate/#before-you-begin","text":"Sign up on the Quix Portal","title":"Before you begin"},{"location":"apis/streaming-reader-api/authenticate/#get-a-personal-access-token","text":"You should authenticate requests to the Streaming Reader API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary. Follow these steps to generate a PAT: Click the user icon in the top-right of the Portal and select the Tokens menu. Click GENERATE TOKEN . Choose a name to describe the token\u2019s purpose, and an expiration date, then click CREATE . Copy the token and store it in a secure place. Warning You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy. Warning Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.","title":"Get a Personal Access Token"},{"location":"apis/streaming-reader-api/intro/","text":"Introduction As an alternative to the SDK , the Quix platform supports real-time data streaming over WebSockets. Clients can receive updates on data and definitions for parameters and events, as they happen. The examples shown use the Microsoft SignalR JavaScript client library. Topics Set up SignalR Authenticate Reading data Subscription & Event reference","title":"Introduction"},{"location":"apis/streaming-reader-api/intro/#introduction","text":"As an alternative to the SDK , the Quix platform supports real-time data streaming over WebSockets. Clients can receive updates on data and definitions for parameters and events, as they happen. The examples shown use the Microsoft SignalR JavaScript client library.","title":"Introduction"},{"location":"apis/streaming-reader-api/intro/#topics","text":"Set up SignalR Authenticate Reading data Subscription & Event reference","title":"Topics"},{"location":"apis/streaming-reader-api/reading-data/","text":"Reading data Before you can read data from a stream, you need to subscribe to an event of the Streaming Reader service like ParameterData or EventData. You can get a full list of Subscriptions and Events available in this of section . Example The following code sample shows how to use the SignalR client library to: Establish a connection to Quix Subscribe to a parameter data stream Receive data from that stream Unsubscribe from the event var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'YOUR_ACCESS_TOKEN' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then (() => { console . log ( \"Connected to Quix.\" ); // Subscribe to parameter data stream. connection . invoke ( \"SubscribeToParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); // Read data from the stream. connection . on ( \"ParameterDataReceived\" , data => { let model = JSON . parse ( data ); console . log ( \"Received data from stream: \" + model . streamId ); // Unsubscribe from stream. connection . invoke ( \"UnsubscribeFromParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); }); });","title":"Reading data"},{"location":"apis/streaming-reader-api/reading-data/#reading-data","text":"Before you can read data from a stream, you need to subscribe to an event of the Streaming Reader service like ParameterData or EventData. You can get a full list of Subscriptions and Events available in this of section .","title":"Reading data"},{"location":"apis/streaming-reader-api/reading-data/#example","text":"The following code sample shows how to use the SignalR client library to: Establish a connection to Quix Subscribe to a parameter data stream Receive data from that stream Unsubscribe from the event var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'YOUR_ACCESS_TOKEN' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then (() => { console . log ( \"Connected to Quix.\" ); // Subscribe to parameter data stream. connection . invoke ( \"SubscribeToParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); // Read data from the stream. connection . on ( \"ParameterDataReceived\" , data => { let model = JSON . parse ( data ); console . log ( \"Received data from stream: \" + model . streamId ); // Unsubscribe from stream. connection . invoke ( \"UnsubscribeFromParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); }); });","title":"Example"},{"location":"apis/streaming-reader-api/signalr/","text":"Set up SignalR Before you begin Get a PAT for Authentication Ensure you know your workspace ID Installation If you are using a package manager like npm , you can install SignalR using npm install @microsoft/signalr . For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation . Testing the connection Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication. You should replace the text YOUR_ACCESS_TOKEN with the PAT obtained from Authenticating with the Streaming Reader API . You should also replace YOUR_WORKSPACE_ID with the appropriate identifier, a combination of your organistation and workspace names. This can be located in one of the following ways: Portal URL Look in the browsers URL when you are logged into the Portal and inside the Workspace you want to work with. The URL contains the workspace id. e.g everything after \"workspace=\" till the next & Topics Page In the Portal, inside the Workspace you want to work with, click the Topics menu and then click the expand icon on any topic. Here you will see a Username under the Broker Settings. This Username is also the Workspace Id. var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'YOUR_ACCESS_TOKEN' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\" , options ) . build (); connection . start (). then (() => console . log ( \"SignalR connected.\" )); If the connection is successful, you should see the console log \u201cSignalR connected\u201d.","title":"Set up SignalR"},{"location":"apis/streaming-reader-api/signalr/#set-up-signalr","text":"","title":"Set up SignalR"},{"location":"apis/streaming-reader-api/signalr/#before-you-begin","text":"Get a PAT for Authentication Ensure you know your workspace ID","title":"Before you begin"},{"location":"apis/streaming-reader-api/signalr/#installation","text":"If you are using a package manager like npm , you can install SignalR using npm install @microsoft/signalr . For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation .","title":"Installation"},{"location":"apis/streaming-reader-api/signalr/#testing-the-connection","text":"Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication. You should replace the text YOUR_ACCESS_TOKEN with the PAT obtained from Authenticating with the Streaming Reader API . You should also replace YOUR_WORKSPACE_ID with the appropriate identifier, a combination of your organistation and workspace names. This can be located in one of the following ways: Portal URL Look in the browsers URL when you are logged into the Portal and inside the Workspace you want to work with. The URL contains the workspace id. e.g everything after \"workspace=\" till the next & Topics Page In the Portal, inside the Workspace you want to work with, click the Topics menu and then click the expand icon on any topic. Here you will see a Username under the Broker Settings. This Username is also the Workspace Id. var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'YOUR_ACCESS_TOKEN' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\" , options ) . build (); connection . start (). then (() => console . log ( \"SignalR connected.\" )); If the connection is successful, you should see the console log \u201cSignalR connected\u201d.","title":"Testing the connection"},{"location":"apis/streaming-reader-api/subscriptions/","text":"Subscription & Event reference The Quix SignalR hub provides the following subscriptions and events. Subscriptions You can subscribe to the following hub methods via the invoke method of a HubConnection : SubscribeToParameter(topicName, streamId, parameterId) : Subscribe to a parameter data stream. SubscribeToEvent(topicName, streamId, eventId) : Subscribes to an event data stream. IList<ActiveStream> SubscribeToActiveStreams(topicName) : Subscribe to Active Streams List changes. The subscription method returns an initial list of the active streams existing in the topic. IList<TopicMetrics> SubscribeToTopicMetrics(topicName) : Subscribe to Topic metrics updates. The subscription method returns an initial list of the last 5 minutes of topic metrics. SubscribeToPackages(string topicName) : Subscribe to Topic packages. A package is an abstraction for any message received in the topic. Each Subscribe method has its own Unsubscribe. Use them once you don\u2019t need the subscriptions anymore to avoid receiving data unnecessarily: UnsubscribeFromParameter(topicname, streamId, parameterId) : Unsubscribe from a parameter data stream. UnsubscribeFromEvent(topicName, streamId, eventId) Unsubscribe from an event data stream. UnsubscribeFromActiveStreams(string topicName) : Unsubscribe from Streams List changes. UnsubscribeFromTopicMetrics(topicName) : Unsubscribe from Topic metrics updates. UnsubscribeFromPackages(string topicName) : Unsubscribe from Topic packages. UnsubscribeFromStream(topicName, streamId) : Unsubscribes from all subscriptions of the specified stream. Tip You should pass the method\u2019s name as the first argument to invoke , followed by the method-specific arguments. For example, to call: SubscribeToParameter(topicName, streamId, parameterId) Use the following: connection . invoke ( \"SubscribeToParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); SignalR events You can register a handler for SignalR events using the on method of a HubConnection . The following events are available: ParameterDataReceived(parameterData) EventDataReceived(eventData) ActiveStreamsChanged(stream, action) TopicMetricsUpdated(metrics) PackageReceived(package) Tip You should pass the event\u2019s name as the first argument to on , followed by a function callback. For example, to react to the ParameterDataReceived event, use the following: connection . on ( \"ParameterDataReceived\" , data => { // process payload data }); ParameterDataReceived Add a listener to ParameterDataReceived event to receive data from a SubscribeToParameter subscription. One event is generated each time a ParameterData package is received in the Topic and the data contains the Parameter the user has subscribed for. Example payload: { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' , timestamps : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], numericValues : { ParameterA : [ 1 , 2 , 3 ] }, stringValues : {}, tagValues : { ParameterA : [ null , null , 'tag-1' ] } } EventDataReceived Add a listener to EventDataReceived event to receive data from a SubscribeToEvent subscription. One event is generated each time a EventData package is received in the Topic and the data contains the Event the user has subscribed for. Example payload: { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' id : 'EventA' , timestamp : 1591733990000000000 , value : 'val-a' , tags : { tag1 : 'val1' } } ActiveStreamChanged This event is generated each time a change has been produced in the list of Active streams of a Topic. Add a listener to ActiveStreamChanged event to receive data from a SubscribeToActiveStreams subscription. This SignalR event contains 2 arguments on it: stream : Payload of the stream that has been changed. action : It describes the type of operation has been applied to the list of active streams: AddUpdate : Stream added or updated Remove : Stream removed Stream payload example: { \"streamId\" : \"5ecfc7ce-906c-4d3a-811c-a85ea75a24b3\" , \"topicName\" : \"f1-data\" , \"name\" : \"F1 Game - Swal - Sakhir Short 2022-04-08-09:00:39\" , \"location\" : \"/Game/Codemasters/F1-2019/Sakhir Short\" , \"metadata\" : { \"GameVersion\" : \"1.22\" , \"PacketFormat\" : \"2019\" , \"Track\" : \"Sakhir Short\" , \"SessionId\" : \"237322236454500810\" , \"Player0_Name\" : \"Swal\" , \"Player0_Id\" : \"100\" }, \"parents\" : [], \"timeOfRecording\" : \"2022-04-08T09:00:39.3971666Z\" , \"parameters\" : { \"EngineRPM\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : 0 , \"maximumValue\" : 20000 , \"location\" : \"/Player/Telemetry/Engine\" }, \"LapDistance\" : { \"dataType\" : \"Numeric\" , \"unit\" : \"m\" , \"location\" : \"/Player/Telemetry/Misc\" }, \"Brake\" : { \"description\" : \"Amount of brake applied\" , \"dataType\" : \"Numeric\" , \"minimumValue\" : 0 , \"maximumValue\" : 1 , \"location\" : \"/Player/Telemetry/Input\" }, \"Throttle\" : { \"dataType\" : \"Unknown\" , \"minimumValue\" : 0 , \"maximumValue\" : 1 , \"location\" : \"/Player/Telemetry/Input\" }, \"Gear\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : - 1 , \"maximumValue\" : 8 , \"location\" : \"/Player/Telemetry/Engine\" }, \"Speed\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : 0 , \"maximumValue\" : 400 , \"location\" : \"/Player/Telemetry/Engine\" }, \"Steer\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : - 1 , \"maximumValue\" : 1 , \"location\" : \"/Player/Telemetry/Input\" }, }, \"events\" : { \"Player_NewLap\" : { \"name\" : \"Player NewLap\" , \"level\" : \"Information\" , \"location\" : \"\" }, \"Player_Position_Changed\" : { \"name\" : \"Player Position Changed\" , \"level\" : \"Critical\" , \"location\" : \"\" }, \"RaceWinner\" : { \"name\" : \"Race Winner\" , \"level\" : \"Critical\" , \"location\" : \"\" }, }, \"firstSeen\" : \"2022-04-08T08:57:40.3406586Z\" , \"lastSeen\" : \"2022-04-08T09:00:39.6308255Z\" , \"status\" : \"Receiving\" , \"lastData\" : \"2022-04-08T09:00:39.6237312Z\" } TopicMetricsUpdated This event is generated periodically by the service to provide basic metrics about a Topic, like \"Bytes per Second\" or \"Number of Active Streams\". Add a listener to TopicMetricsUpdated event to receive data from a SubscribeToTopicMetrics subscription. Topic Metrics payload example: { \"timestamp\" : \"2022-04-10T19:26:49.1417825Z\" , \"topicName\" : \"f1-data\" , \"bytesPerSecond\" : 14877 , \"activeStreams\" : 1 } PackageReceived Add a listener to PackageReceived event to receive data from a SubscribeToPackages subscription. One event is generated each time a package is received in the topic. Type: Indicates the Quix Sdk model used to deserialize the package. Value: Deserialized package object represented as a Json string format. Package payload example: { \"topicName\" : \"f1-data\" , \"streamId\" : \"dec481d7-7ae4-403a-9d20-a1cabdcd3275\" , \"type\" : \"Quix.Sdk.Process.Models.ParameterDataRaw\" , \"value\" : \"{\\\"Epoch\\\":0,\\\"Timestamps\\\":[1649623155716050700],\\\"NumericValues\\\":{\\\"LapDistance\\\":[542.504638671875],\\\"TotalLapDistance\\\":[4368.53271484375]},\\\"StringValues\\\":{},\\\"BinaryValues\\\":{},\\\"TagValues\\\":{\\\"LapValidity\\\":[\\\"Valid\\\"],\\\"LapNumber\\\":[\\\"2\\\"],\\\"PitStatus\\\":[\\\"None\\\"],\\\"Sector\\\":[\\\"0\\\"],\\\"DriverStatus\\\":[\\\"Flying_lap\\\"]}}\" , \"dateTime\" : \"2022-04-10T20:39:16.63Z\" }","title":"Subscription & Event reference"},{"location":"apis/streaming-reader-api/subscriptions/#subscription-event-reference","text":"The Quix SignalR hub provides the following subscriptions and events.","title":"Subscription &amp; Event reference"},{"location":"apis/streaming-reader-api/subscriptions/#subscriptions","text":"You can subscribe to the following hub methods via the invoke method of a HubConnection : SubscribeToParameter(topicName, streamId, parameterId) : Subscribe to a parameter data stream. SubscribeToEvent(topicName, streamId, eventId) : Subscribes to an event data stream. IList<ActiveStream> SubscribeToActiveStreams(topicName) : Subscribe to Active Streams List changes. The subscription method returns an initial list of the active streams existing in the topic. IList<TopicMetrics> SubscribeToTopicMetrics(topicName) : Subscribe to Topic metrics updates. The subscription method returns an initial list of the last 5 minutes of topic metrics. SubscribeToPackages(string topicName) : Subscribe to Topic packages. A package is an abstraction for any message received in the topic. Each Subscribe method has its own Unsubscribe. Use them once you don\u2019t need the subscriptions anymore to avoid receiving data unnecessarily: UnsubscribeFromParameter(topicname, streamId, parameterId) : Unsubscribe from a parameter data stream. UnsubscribeFromEvent(topicName, streamId, eventId) Unsubscribe from an event data stream. UnsubscribeFromActiveStreams(string topicName) : Unsubscribe from Streams List changes. UnsubscribeFromTopicMetrics(topicName) : Unsubscribe from Topic metrics updates. UnsubscribeFromPackages(string topicName) : Unsubscribe from Topic packages. UnsubscribeFromStream(topicName, streamId) : Unsubscribes from all subscriptions of the specified stream. Tip You should pass the method\u2019s name as the first argument to invoke , followed by the method-specific arguments. For example, to call: SubscribeToParameter(topicName, streamId, parameterId) Use the following: connection . invoke ( \"SubscribeToParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" );","title":"Subscriptions"},{"location":"apis/streaming-reader-api/subscriptions/#signalr-events","text":"You can register a handler for SignalR events using the on method of a HubConnection . The following events are available: ParameterDataReceived(parameterData) EventDataReceived(eventData) ActiveStreamsChanged(stream, action) TopicMetricsUpdated(metrics) PackageReceived(package) Tip You should pass the event\u2019s name as the first argument to on , followed by a function callback. For example, to react to the ParameterDataReceived event, use the following: connection . on ( \"ParameterDataReceived\" , data => { // process payload data });","title":"SignalR events"},{"location":"apis/streaming-reader-api/subscriptions/#parameterdatareceived","text":"Add a listener to ParameterDataReceived event to receive data from a SubscribeToParameter subscription. One event is generated each time a ParameterData package is received in the Topic and the data contains the Parameter the user has subscribed for. Example payload: { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' , timestamps : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], numericValues : { ParameterA : [ 1 , 2 , 3 ] }, stringValues : {}, tagValues : { ParameterA : [ null , null , 'tag-1' ] } }","title":"ParameterDataReceived"},{"location":"apis/streaming-reader-api/subscriptions/#eventdatareceived","text":"Add a listener to EventDataReceived event to receive data from a SubscribeToEvent subscription. One event is generated each time a EventData package is received in the Topic and the data contains the Event the user has subscribed for. Example payload: { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' id : 'EventA' , timestamp : 1591733990000000000 , value : 'val-a' , tags : { tag1 : 'val1' } }","title":"EventDataReceived"},{"location":"apis/streaming-reader-api/subscriptions/#activestreamchanged","text":"This event is generated each time a change has been produced in the list of Active streams of a Topic. Add a listener to ActiveStreamChanged event to receive data from a SubscribeToActiveStreams subscription. This SignalR event contains 2 arguments on it: stream : Payload of the stream that has been changed. action : It describes the type of operation has been applied to the list of active streams: AddUpdate : Stream added or updated Remove : Stream removed Stream payload example: { \"streamId\" : \"5ecfc7ce-906c-4d3a-811c-a85ea75a24b3\" , \"topicName\" : \"f1-data\" , \"name\" : \"F1 Game - Swal - Sakhir Short 2022-04-08-09:00:39\" , \"location\" : \"/Game/Codemasters/F1-2019/Sakhir Short\" , \"metadata\" : { \"GameVersion\" : \"1.22\" , \"PacketFormat\" : \"2019\" , \"Track\" : \"Sakhir Short\" , \"SessionId\" : \"237322236454500810\" , \"Player0_Name\" : \"Swal\" , \"Player0_Id\" : \"100\" }, \"parents\" : [], \"timeOfRecording\" : \"2022-04-08T09:00:39.3971666Z\" , \"parameters\" : { \"EngineRPM\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : 0 , \"maximumValue\" : 20000 , \"location\" : \"/Player/Telemetry/Engine\" }, \"LapDistance\" : { \"dataType\" : \"Numeric\" , \"unit\" : \"m\" , \"location\" : \"/Player/Telemetry/Misc\" }, \"Brake\" : { \"description\" : \"Amount of brake applied\" , \"dataType\" : \"Numeric\" , \"minimumValue\" : 0 , \"maximumValue\" : 1 , \"location\" : \"/Player/Telemetry/Input\" }, \"Throttle\" : { \"dataType\" : \"Unknown\" , \"minimumValue\" : 0 , \"maximumValue\" : 1 , \"location\" : \"/Player/Telemetry/Input\" }, \"Gear\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : - 1 , \"maximumValue\" : 8 , \"location\" : \"/Player/Telemetry/Engine\" }, \"Speed\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : 0 , \"maximumValue\" : 400 , \"location\" : \"/Player/Telemetry/Engine\" }, \"Steer\" : { \"dataType\" : \"Numeric\" , \"minimumValue\" : - 1 , \"maximumValue\" : 1 , \"location\" : \"/Player/Telemetry/Input\" }, }, \"events\" : { \"Player_NewLap\" : { \"name\" : \"Player NewLap\" , \"level\" : \"Information\" , \"location\" : \"\" }, \"Player_Position_Changed\" : { \"name\" : \"Player Position Changed\" , \"level\" : \"Critical\" , \"location\" : \"\" }, \"RaceWinner\" : { \"name\" : \"Race Winner\" , \"level\" : \"Critical\" , \"location\" : \"\" }, }, \"firstSeen\" : \"2022-04-08T08:57:40.3406586Z\" , \"lastSeen\" : \"2022-04-08T09:00:39.6308255Z\" , \"status\" : \"Receiving\" , \"lastData\" : \"2022-04-08T09:00:39.6237312Z\" }","title":"ActiveStreamChanged"},{"location":"apis/streaming-reader-api/subscriptions/#topicmetricsupdated","text":"This event is generated periodically by the service to provide basic metrics about a Topic, like \"Bytes per Second\" or \"Number of Active Streams\". Add a listener to TopicMetricsUpdated event to receive data from a SubscribeToTopicMetrics subscription. Topic Metrics payload example: { \"timestamp\" : \"2022-04-10T19:26:49.1417825Z\" , \"topicName\" : \"f1-data\" , \"bytesPerSecond\" : 14877 , \"activeStreams\" : 1 }","title":"TopicMetricsUpdated"},{"location":"apis/streaming-reader-api/subscriptions/#packagereceived","text":"Add a listener to PackageReceived event to receive data from a SubscribeToPackages subscription. One event is generated each time a package is received in the topic. Type: Indicates the Quix Sdk model used to deserialize the package. Value: Deserialized package object represented as a Json string format. Package payload example: { \"topicName\" : \"f1-data\" , \"streamId\" : \"dec481d7-7ae4-403a-9d20-a1cabdcd3275\" , \"type\" : \"Quix.Sdk.Process.Models.ParameterDataRaw\" , \"value\" : \"{\\\"Epoch\\\":0,\\\"Timestamps\\\":[1649623155716050700],\\\"NumericValues\\\":{\\\"LapDistance\\\":[542.504638671875],\\\"TotalLapDistance\\\":[4368.53271484375]},\\\"StringValues\\\":{},\\\"BinaryValues\\\":{},\\\"TagValues\\\":{\\\"LapValidity\\\":[\\\"Valid\\\"],\\\"LapNumber\\\":[\\\"2\\\"],\\\"PitStatus\\\":[\\\"None\\\"],\\\"Sector\\\":[\\\"0\\\"],\\\"DriverStatus\\\":[\\\"Flying_lap\\\"]}}\" , \"dateTime\" : \"2022-04-10T20:39:16.63Z\" }","title":"PackageReceived"},{"location":"apis/streaming-writer-api/authenticate/","text":"Authenticate Before you begin Sign up on the Quix Portal Get a Personal Access Token You should authenticate requests to the Streaming Writer API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary. Follow these steps to generate a PAT: Click the user icon in the top-right of the Portal and select the Tokens menu. Click GENERATE TOKEN . Choose a name to describe the token\u2019s purpose, and an expiration date, then click CREATE . Copy the token and store it in a secure place. Warning You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy. Warning Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs. Sign all your requests using this token Make sure you accompany each request to the API with an Authorization header using your PAT as a bearer token, as follows: Authorization: bearer <token> Replace <token> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the -H flag: curl -H \"Authorization: bearer <token>\" ... Warning If you fail to send a valid Authorization header, the API will respond with a 401 UNAUTHORIZED status code.","title":"Authenticate"},{"location":"apis/streaming-writer-api/authenticate/#authenticate","text":"","title":"Authenticate"},{"location":"apis/streaming-writer-api/authenticate/#before-you-begin","text":"Sign up on the Quix Portal","title":"Before you begin"},{"location":"apis/streaming-writer-api/authenticate/#get-a-personal-access-token","text":"You should authenticate requests to the Streaming Writer API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary. Follow these steps to generate a PAT: Click the user icon in the top-right of the Portal and select the Tokens menu. Click GENERATE TOKEN . Choose a name to describe the token\u2019s purpose, and an expiration date, then click CREATE . Copy the token and store it in a secure place. Warning You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy. Warning Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.","title":"Get a Personal Access Token"},{"location":"apis/streaming-writer-api/authenticate/#sign-all-your-requests-using-this-token","text":"Make sure you accompany each request to the API with an Authorization header using your PAT as a bearer token, as follows: Authorization: bearer <token> Replace <token> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the -H flag: curl -H \"Authorization: bearer <token>\" ... Warning If you fail to send a valid Authorization header, the API will respond with a 401 UNAUTHORIZED status code.","title":"Sign all your requests using this token"},{"location":"apis/streaming-writer-api/create-stream/","text":"Create a new Stream You can create a new stream by specifying a topic to create it in, and supplying any other additional properties required. Tip This method is optional. You can also create a stream implicitly by sending data to a stream that doesn\u2019t already exist. But creating a stream using the method on this page avoids having to determine a unique stream id yourself. Before you begin You should have a Workspace set up with at least one Topic . Get a Personal Access Token to authenticate each request. Using the /streams endpoint To create a new stream, send a POST request to: /topics/${topicName}/streams You should replace $\\{topicName} in the endpoint URL with the name of the Topic you wish to create the stream in. For example, if your topic is named \u201ccars\u201d, your endpoint url will be /topics/cars/streams . Example request You can create a new Stream with an absolute minimum of effort by passing an empty JSON object in the payload: curl curl \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{}' Node.js const https = require ( 'https' ); const data = \"{}\" ; const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams' , method : 'POST' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { res . on ( 'data' , d => { let streamId = JSON . parse ( d ). streamId ; console . log ( streamId ); }); }); req . write ( data ); req . end (); For most real-world cases, you\u2019ll also want to provide some or all of the following: name location metadata parents timeOfRecording For example, here\u2019s a more useful payload: { \"name\" : \"cardata\" , \"location\" : \"simulations/trials\" , \"metadata\" : { \"rain\" : \"light\" } } Example response The JSON returned is an object with a single property, streamId . This contains the unique identifier of your newly created stream, and will look something like this: { \"streamId\" : \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" } Tip If you\u2019re following these guides in order, you\u2019ll want to take note of that stream id. For curl examples, it\u2019s convenient to keep it in an environment variable, e.g. $ streamId = 66fb0a2f-eb70-494e-9df7-c06d275aeb7c Using SignalR var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let streamDetails = { \"name\" : \"cardata\" , \"location\" : \"simulations/trials\" , \"metadata\" : { \"rain\" : \"light\" } } // Send create details console . log ( \"Creating stream\" ); let createdDetails = await connection . invoke ( \"CreateStream\" , topic , streamDetails ); let streamId = createdDetails . streamId console . log ( \"Created stream \" + streamId ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/cLno68fs/","title":"Create a new Stream"},{"location":"apis/streaming-writer-api/create-stream/#create-a-new-stream","text":"You can create a new stream by specifying a topic to create it in, and supplying any other additional properties required. Tip This method is optional. You can also create a stream implicitly by sending data to a stream that doesn\u2019t already exist. But creating a stream using the method on this page avoids having to determine a unique stream id yourself.","title":"Create a new Stream"},{"location":"apis/streaming-writer-api/create-stream/#before-you-begin","text":"You should have a Workspace set up with at least one Topic . Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/streaming-writer-api/create-stream/#using-the-streams-endpoint","text":"To create a new stream, send a POST request to: /topics/${topicName}/streams You should replace $\\{topicName} in the endpoint URL with the name of the Topic you wish to create the stream in. For example, if your topic is named \u201ccars\u201d, your endpoint url will be /topics/cars/streams .","title":"Using the /streams endpoint"},{"location":"apis/streaming-writer-api/create-stream/#example-request","text":"You can create a new Stream with an absolute minimum of effort by passing an empty JSON object in the payload: curl curl \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{}' Node.js const https = require ( 'https' ); const data = \"{}\" ; const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams' , method : 'POST' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { res . on ( 'data' , d => { let streamId = JSON . parse ( d ). streamId ; console . log ( streamId ); }); }); req . write ( data ); req . end (); For most real-world cases, you\u2019ll also want to provide some or all of the following: name location metadata parents timeOfRecording For example, here\u2019s a more useful payload: { \"name\" : \"cardata\" , \"location\" : \"simulations/trials\" , \"metadata\" : { \"rain\" : \"light\" } }","title":"Example request"},{"location":"apis/streaming-writer-api/create-stream/#example-response","text":"The JSON returned is an object with a single property, streamId . This contains the unique identifier of your newly created stream, and will look something like this: { \"streamId\" : \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\" } Tip If you\u2019re following these guides in order, you\u2019ll want to take note of that stream id. For curl examples, it\u2019s convenient to keep it in an environment variable, e.g. $ streamId = 66fb0a2f-eb70-494e-9df7-c06d275aeb7c","title":"Example response"},{"location":"apis/streaming-writer-api/create-stream/#using-signalr","text":"var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let streamDetails = { \"name\" : \"cardata\" , \"location\" : \"simulations/trials\" , \"metadata\" : { \"rain\" : \"light\" } } // Send create details console . log ( \"Creating stream\" ); let createdDetails = await connection . invoke ( \"CreateStream\" , topic , streamDetails ); let streamId = createdDetails . streamId console . log ( \"Created stream \" + streamId ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/cLno68fs/","title":"Using SignalR"},{"location":"apis/streaming-writer-api/get-swagger/","text":"Getting Swagger url You can access documentation and a basic playground for the Streaming Writer API via Swagger . The exact URL is workspace-specific, and follows this pattern: https://writer-${organisation}-${workspace}.platform.quix.ai/ The workspace ID is a combination based on your organisation and workspace names. For example, for an \"acme\" organisation with a \"weather\" workspace, the final URL might look something like: https://writer-acme-weather.platform.quix.ai/ To determine the final URL, you can find out how to get a workspace id , or follow these instructions: Click the Library icon Library in the main menu. Search for \"HTTP API - Shell Script\" item in the search bar and select the item. Find in the Code Preview the Url on any of the POST methods of the sample.","title":"Getting Swagger url"},{"location":"apis/streaming-writer-api/get-swagger/#getting-swagger-url","text":"You can access documentation and a basic playground for the Streaming Writer API via Swagger . The exact URL is workspace-specific, and follows this pattern: https://writer-${organisation}-${workspace}.platform.quix.ai/ The workspace ID is a combination based on your organisation and workspace names. For example, for an \"acme\" organisation with a \"weather\" workspace, the final URL might look something like: https://writer-acme-weather.platform.quix.ai/ To determine the final URL, you can find out how to get a workspace id , or follow these instructions: Click the Library icon Library in the main menu. Search for \"HTTP API - Shell Script\" item in the search bar and select the item. Find in the Code Preview the Url on any of the POST methods of the sample.","title":"Getting Swagger url"},{"location":"apis/streaming-writer-api/intro/","text":"Introduction The Streaming Writer API allows you to stream data into the Quix platform via HTTP endpoints or SignalR. It\u2019s an alternative to using our C# and Python SDKs. You can use the Streaming Writer API from any HTTP-capable language. The API is fully documented in our Swagger documentation . Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using curl. Preparation If you plan on using the HTTP endpoins, then you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API . If you would rather use the SignalR api, which is suggested for high frequency data streaming, then see SignalR setup . Topics covered Stream Create a new Stream Add Stream metadata Parameters Send Parameter data Events Send an Event","title":"Introduction"},{"location":"apis/streaming-writer-api/intro/#introduction","text":"The Streaming Writer API allows you to stream data into the Quix platform via HTTP endpoints or SignalR. It\u2019s an alternative to using our C# and Python SDKs. You can use the Streaming Writer API from any HTTP-capable language. The API is fully documented in our Swagger documentation . Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using curl.","title":"Introduction"},{"location":"apis/streaming-writer-api/intro/#preparation","text":"If you plan on using the HTTP endpoins, then you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API . If you would rather use the SignalR api, which is suggested for high frequency data streaming, then see SignalR setup .","title":"Preparation"},{"location":"apis/streaming-writer-api/intro/#topics-covered","text":"Stream Create a new Stream Add Stream metadata Parameters Send Parameter data Events Send an Event","title":"Topics covered"},{"location":"apis/streaming-writer-api/request/","text":"Forming a request How you send requests to the Streaming Writer API will vary depending on the client or language you\u2019re using. But the API still has behaviour and expectations that is common across all clients. Tip The examples in this section show how to use the popular curl command line tool. Before you begin Sign up on the Quix Portal Read about Authenticating with the Streaming Writer API Endpoint URLs The Streaming Writer API is available on a per-workspace basis, so the subdomain is based on a combination of your organisation and workspace names. See the Swagger documentation to find out how to get the exact hostname required. It will be in this format: https://writer-${organisation}-${workspace}.platform.quix.ai So your final endpoint URL will look something like: https://writer-acme-weather.platform.quix.ai/ Method Endpoints in this API use the POST and PUT methods. Ensure your HTTP client sends the correct request method. Using curl , you can specify the request method with the -X <POST|PUT> flag, for example: curl -X PUT ... Payload For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending: curl -H \"Content-Type: application/json\" ... Warning You must specify the content type of your payload. Failing to include this header will result in a 415 UNSUPPORTED MEDIA TYPE status code. You can send data using the curl flag -d . This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data. curl -d '{\"key\": \"value\"}' ... curl -d \"@data.json\" ... Tip By default, -d will send a POST request, so -X POST becomes unnecessary. Complete curl example You should structure most of your requests to the API around this pattern: curl -H \"Authorization: ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"@data.json\" \\ https:// ${ domain } .platform.quix.ai/ ${ endpoint }","title":"Forming a request"},{"location":"apis/streaming-writer-api/request/#forming-a-request","text":"How you send requests to the Streaming Writer API will vary depending on the client or language you\u2019re using. But the API still has behaviour and expectations that is common across all clients. Tip The examples in this section show how to use the popular curl command line tool.","title":"Forming a request"},{"location":"apis/streaming-writer-api/request/#before-you-begin","text":"Sign up on the Quix Portal Read about Authenticating with the Streaming Writer API","title":"Before you begin"},{"location":"apis/streaming-writer-api/request/#endpoint-urls","text":"The Streaming Writer API is available on a per-workspace basis, so the subdomain is based on a combination of your organisation and workspace names. See the Swagger documentation to find out how to get the exact hostname required. It will be in this format: https://writer-${organisation}-${workspace}.platform.quix.ai So your final endpoint URL will look something like: https://writer-acme-weather.platform.quix.ai/","title":"Endpoint URLs"},{"location":"apis/streaming-writer-api/request/#method","text":"Endpoints in this API use the POST and PUT methods. Ensure your HTTP client sends the correct request method. Using curl , you can specify the request method with the -X <POST|PUT> flag, for example: curl -X PUT ...","title":"Method"},{"location":"apis/streaming-writer-api/request/#payload","text":"For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending: curl -H \"Content-Type: application/json\" ... Warning You must specify the content type of your payload. Failing to include this header will result in a 415 UNSUPPORTED MEDIA TYPE status code. You can send data using the curl flag -d . This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data. curl -d '{\"key\": \"value\"}' ... curl -d \"@data.json\" ... Tip By default, -d will send a POST request, so -X POST becomes unnecessary.","title":"Payload"},{"location":"apis/streaming-writer-api/request/#complete-curl-example","text":"You should structure most of your requests to the API around this pattern: curl -H \"Authorization: ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d \"@data.json\" \\ https:// ${ domain } .platform.quix.ai/ ${ endpoint }","title":"Complete curl example"},{"location":"apis/streaming-writer-api/send-data/","text":"Send Parameter data You can send telemetry data using the Streaming Writer API. Select a topic and a stream to send the data to. In your payload, you can include numeric, string, or binary parameter data, with nanosecond-level timestamps. Before you begin You should have a Workspace set up with at least one Topic . Get a Personal Access Token to authenticate each request. Sending structured data to the endpoint Send a POST request together with a JSON payload representing the data you\u2019re sending to: /topics/${topicName}/streams/${streamId}/parameters/data You should replace $\\{topicName} with the name of the topic your stream belongs to, and $\\{streamId} with the id of the stream you wish to send data to. For example: /topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/parameters/data Tip You can create a new stream by supplying a $\\{streamId} that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately. Example request Your payload should include an array of timestamps with one timestamp for each item of data you\u2019re sending. Actual data values should be keyed on their name, in the object that corresponds to their type, one of numericValues , stringValues , or binaryValues . The payload is in this structure: { \"timestamps\" : [ ... ], \"numericValues\" : { ... }, \"stringValues\" : { ... }, \"binaryValues\" : { ... }, \"tagValues\" : { ... } } Any data types that are unused can be omitted. So a final request using curl might look something like this: curl Node.js curl -X POST \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams/ ${ streamId } /parameters/data\" \\ -H \"Authorization: Bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000], \"numericValues\": { \"SomeParameter1\": [10.01, 202.02, 303.03], \"SomeParameter2\": [400.04, 50.05, 60.06] } }' const https = require ( 'https' ); const data = JSON . stringify ({ \"timestamps\" : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], \"numericValues\" : { \"SomeParameter1\" : [ 10.01 , 202.02 , 303.03 ], \"SomeParameter2\" : [ 400.04 , 50.05 , 60.06 ] } }); const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams/' + streamId + '/parameters/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options ); req . write ( data ); req . end (); Response No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong. Using SignalR var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const streamId = \"ID_OF_STREAM_TO_WRITE_TO\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let parameterData = { \"epoch\" : Date . now () * 1000000 , // set now as time starting point, in nanoseconds \"timestamps\" : [ 0 , 5000000000 , // 5 seconds from now (see epoch) 8000000000 ], \"numericValues\" : { \"NumericParameter1\" : [ 13.37 , 42 , 24.72 ] }, \"stringValues\" : { \"StringParameter1\" : [ \"Hello\" , \"World\" , \"!\" ] }, \"binaryValues\" : { \"BinaryParameter1\" : [ btoa ( \"Hello\" ), // send binary array as base64 btoa ( \"World\" ), btoa ( \"!\" ) ] }, \"tagValues\" : { \"Tag1\" : [ \"A\" , \"B\" , null ] } } // Send stream update details console . log ( \"Sending parameter data\" ); await connection . invoke ( \"SendParameterData\" , topic , streamId , parameterData ); console . log ( \"Sent parameter data\" ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/a41b8x0t/","title":"Send Parameter data"},{"location":"apis/streaming-writer-api/send-data/#send-parameter-data","text":"You can send telemetry data using the Streaming Writer API. Select a topic and a stream to send the data to. In your payload, you can include numeric, string, or binary parameter data, with nanosecond-level timestamps.","title":"Send Parameter data"},{"location":"apis/streaming-writer-api/send-data/#before-you-begin","text":"You should have a Workspace set up with at least one Topic . Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/streaming-writer-api/send-data/#sending-structured-data-to-the-endpoint","text":"Send a POST request together with a JSON payload representing the data you\u2019re sending to: /topics/${topicName}/streams/${streamId}/parameters/data You should replace $\\{topicName} with the name of the topic your stream belongs to, and $\\{streamId} with the id of the stream you wish to send data to. For example: /topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/parameters/data Tip You can create a new stream by supplying a $\\{streamId} that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately.","title":"Sending structured data to the endpoint"},{"location":"apis/streaming-writer-api/send-data/#example-request","text":"Your payload should include an array of timestamps with one timestamp for each item of data you\u2019re sending. Actual data values should be keyed on their name, in the object that corresponds to their type, one of numericValues , stringValues , or binaryValues . The payload is in this structure: { \"timestamps\" : [ ... ], \"numericValues\" : { ... }, \"stringValues\" : { ... }, \"binaryValues\" : { ... }, \"tagValues\" : { ... } } Any data types that are unused can be omitted. So a final request using curl might look something like this: curl Node.js curl -X POST \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams/ ${ streamId } /parameters/data\" \\ -H \"Authorization: Bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000], \"numericValues\": { \"SomeParameter1\": [10.01, 202.02, 303.03], \"SomeParameter2\": [400.04, 50.05, 60.06] } }' const https = require ( 'https' ); const data = JSON . stringify ({ \"timestamps\" : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], \"numericValues\" : { \"SomeParameter1\" : [ 10.01 , 202.02 , 303.03 ], \"SomeParameter2\" : [ 400.04 , 50.05 , 60.06 ] } }); const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams/' + streamId + '/parameters/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options ); req . write ( data ); req . end ();","title":"Example request"},{"location":"apis/streaming-writer-api/send-data/#response","text":"No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong.","title":"Response"},{"location":"apis/streaming-writer-api/send-data/#using-signalr","text":"var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const streamId = \"ID_OF_STREAM_TO_WRITE_TO\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let parameterData = { \"epoch\" : Date . now () * 1000000 , // set now as time starting point, in nanoseconds \"timestamps\" : [ 0 , 5000000000 , // 5 seconds from now (see epoch) 8000000000 ], \"numericValues\" : { \"NumericParameter1\" : [ 13.37 , 42 , 24.72 ] }, \"stringValues\" : { \"StringParameter1\" : [ \"Hello\" , \"World\" , \"!\" ] }, \"binaryValues\" : { \"BinaryParameter1\" : [ btoa ( \"Hello\" ), // send binary array as base64 btoa ( \"World\" ), btoa ( \"!\" ) ] }, \"tagValues\" : { \"Tag1\" : [ \"A\" , \"B\" , null ] } } // Send stream update details console . log ( \"Sending parameter data\" ); await connection . invoke ( \"SendParameterData\" , topic , streamId , parameterData ); console . log ( \"Sent parameter data\" ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/a41b8x0t/","title":"Using SignalR"},{"location":"apis/streaming-writer-api/send-event/","text":"Send an Event You can add Events to your stream data to record discrete actions for future reference. Before you begin Get a Personal Access Token to authenticate each request. If you don\u2019t already have a Stream in your workspace, add one using the API . Sending event data To send event data to a stream, use the POST method with this endpoint: /topics/${topicName}/streams/${streamId}/events/data You should replace ${topicName} with the name of the topic your stream belongs to, and ${streamId} with the id of the stream you wish to send data to. For example: /topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/events/data Tip You can create a new stream by supplying a $\\{streamId} that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately. Your payload should be an array of events. Each event is an object containing the following properties: id a unique identifier for the event timestamp the nanosecond-precise timestamp at which the event occurred tags a object containing key-value string pairs representing tag values value a string value associated with the event Example request This example call adds a single event to a stream. The event has an example value and demonstrates use of a tag to include additional information. curl Node.js curl -i \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams/ ${ streamId } /events/data\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '[{ \"id\": \"Alert\", \"timestamp\": 1618133869000000000, \"tags\": { \"capacity\": \"over\" }, \"value\": \"Help\" }]' const https = require ( 'https' ); const data = JSON . stringify ({ \"id\" : \"Alert\" , \"timestamp\" : 1618133869000000000 , \"tags\" : { \"capacity\" : \"over\" }, \"value\" : \"Help\" }); const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams/' + streamId + '/events/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options ); req . write ( data ); req . end (); Response No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong. Using SignalR var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const streamId = \"ID_OF_STREAM_TO_WRITE_TO\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let eventData = [ { \"timestamp\" : Date . now () * 1000000 , // set now in nanoseconds, \"tags\" : { \"capacity\" : \"over\" }, \"id\" : \"Alert\" , \"value\" : \"Successful sample run\" } ] // Send stream update details console . log ( \"Sending event data\" ); await connection . invoke ( \"SendEventData\" , topic , streamId , eventData ); console . log ( \"Sent event data\" ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/h4fztrns/","title":"Send an Event"},{"location":"apis/streaming-writer-api/send-event/#send-an-event","text":"You can add Events to your stream data to record discrete actions for future reference.","title":"Send an Event"},{"location":"apis/streaming-writer-api/send-event/#before-you-begin","text":"Get a Personal Access Token to authenticate each request. If you don\u2019t already have a Stream in your workspace, add one using the API .","title":"Before you begin"},{"location":"apis/streaming-writer-api/send-event/#sending-event-data","text":"To send event data to a stream, use the POST method with this endpoint: /topics/${topicName}/streams/${streamId}/events/data You should replace ${topicName} with the name of the topic your stream belongs to, and ${streamId} with the id of the stream you wish to send data to. For example: /topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/events/data Tip You can create a new stream by supplying a $\\{streamId} that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately. Your payload should be an array of events. Each event is an object containing the following properties: id a unique identifier for the event timestamp the nanosecond-precise timestamp at which the event occurred tags a object containing key-value string pairs representing tag values value a string value associated with the event","title":"Sending event data"},{"location":"apis/streaming-writer-api/send-event/#example-request","text":"This example call adds a single event to a stream. The event has an example value and demonstrates use of a tag to include additional information. curl Node.js curl -i \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams/ ${ streamId } /events/data\" \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '[{ \"id\": \"Alert\", \"timestamp\": 1618133869000000000, \"tags\": { \"capacity\": \"over\" }, \"value\": \"Help\" }]' const https = require ( 'https' ); const data = JSON . stringify ({ \"id\" : \"Alert\" , \"timestamp\" : 1618133869000000000 , \"tags\" : { \"capacity\" : \"over\" }, \"value\" : \"Help\" }); const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams/' + streamId + '/events/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options ); req . write ( data ); req . end ();","title":"Example request"},{"location":"apis/streaming-writer-api/send-event/#response","text":"No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong.","title":"Response"},{"location":"apis/streaming-writer-api/send-event/#using-signalr","text":"var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const streamId = \"ID_OF_STREAM_TO_WRITE_TO\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let eventData = [ { \"timestamp\" : Date . now () * 1000000 , // set now in nanoseconds, \"tags\" : { \"capacity\" : \"over\" }, \"id\" : \"Alert\" , \"value\" : \"Successful sample run\" } ] // Send stream update details console . log ( \"Sending event data\" ); await connection . invoke ( \"SendEventData\" , topic , streamId , eventData ); console . log ( \"Sent event data\" ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/h4fztrns/","title":"Using SignalR"},{"location":"apis/streaming-writer-api/signalr/","text":"Set up SignalR Before you begin Get a PAT for Authentication Ensure you know your workspace ID Installation If you are using a package manager like npm , you can install SignalR using npm install @microsoft/signalr . For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation . Testing the connection Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication. You should replace the text YOUR_ACCESS_TOKEN with the PAT obtained from Authenticating with the Streaming Writer API . You should also replace YOUR_WORKSPACE_ID with the appropriate identifier, a combination of your organistation and workspace names. This can be located in one of the following ways: Portal URL Look in the browsers URL when you are logged into the Portal and inside the Workspace you want to work with. The URL contains the workspace id. e.g everything after \"workspace=\" till the next & Topics Page In the Portal, inside the Workspace you want to work with, click the Topics menu and then click the expand icon on any topic. Here you will see a Username under the Broker Settings. This Username is also the Workspace Id. var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); connection . start (). then (() => console . log ( \"SignalR connected.\" )); If the connection is successful, you should see the console log \u201cSignalR connected\u201d. Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/L9ha4p5j/","title":"Set up SignalR"},{"location":"apis/streaming-writer-api/signalr/#set-up-signalr","text":"","title":"Set up SignalR"},{"location":"apis/streaming-writer-api/signalr/#before-you-begin","text":"Get a PAT for Authentication Ensure you know your workspace ID","title":"Before you begin"},{"location":"apis/streaming-writer-api/signalr/#installation","text":"If you are using a package manager like npm , you can install SignalR using npm install @microsoft/signalr . For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation .","title":"Installation"},{"location":"apis/streaming-writer-api/signalr/#testing-the-connection","text":"Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication. You should replace the text YOUR_ACCESS_TOKEN with the PAT obtained from Authenticating with the Streaming Writer API . You should also replace YOUR_WORKSPACE_ID with the appropriate identifier, a combination of your organistation and workspace names. This can be located in one of the following ways: Portal URL Look in the browsers URL when you are logged into the Portal and inside the Workspace you want to work with. The URL contains the workspace id. e.g everything after \"workspace=\" till the next & Topics Page In the Portal, inside the Workspace you want to work with, click the Topics menu and then click the expand icon on any topic. Here you will see a Username under the Broker Settings. This Username is also the Workspace Id. var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); connection . start (). then (() => console . log ( \"SignalR connected.\" )); If the connection is successful, you should see the console log \u201cSignalR connected\u201d. Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/L9ha4p5j/","title":"Testing the connection"},{"location":"apis/streaming-writer-api/stream-metadata/","text":"Add Stream metadata You can add arbitrary string metadata to any stream. You can also create a new stream by sending metadata using a stream id that does not already exist. Before you begin You should have a Workspace set up with at least one Topic . Get a Personal Access Token to authenticate each request. How to add metadata to a stream Send a PUT request to the following endpoint to update a stream with the given properties: /topics/${topicName}/streams/${streamId} You should replace $\\{topicName} with the name of the topic your stream belongs to, and $\\{streamId} with the id of the stream you wish to update. For example: /topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c Tip You can create a new stream by supplying a $\\{streamId} that doesn\u2019t already exist. It will be initialised with the data you provide in the payload, and the id you use in the endpoint. This avoids the need to call the create stream endpoint separately. Your request should contain a payload consisting of JSON data containing the desired metadata. Example request Below is an example payload demonstrating how to set a single item of metadata. Note that the metadata property references an object which contains key/value string-based metadata. curl curl \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams/ ${ streamId } \" \\ -X PUT \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"metadata\":{\"fruit\":\"apple\"}}' Node.js const https = require ( 'https' ); const data = JSON . stringify ({ metadata : { fruit : \"apple\" }}); const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams/' + streamId , method : 'PUT' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options ); req . write ( data ); req . end (); Since this is a PUT request, it will replace all the stream data with the payload contents. To maintain existing data, you should include it in the payload alongside your metadata, e.g. { \"name\" : \"Example stream\" , \"location\" : \"/sub/dir\" , \"metadata\" : { \"fruit\" : \"apple\" } } Response No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong. For example, you\u2019ll see a 405 code if you forget to specify the correct PUT method. Using SignalR var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const streamId = \"ID_OF_STREAM_TO_UPDATE\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let streamDetails = { \"name\" : \"Example stream\" , \"location\" : \"/sub/dir\" , \"metadata\" : { \"fruit\" : \"apple\" } } // Send stream update details console . log ( \"Updating stream\" ); await connection . invoke ( \"UpdateStream\" , topic , streamDetails ); console . log ( \"Updated stream\" ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/ruywnz28/","title":"Add Stream metadata"},{"location":"apis/streaming-writer-api/stream-metadata/#add-stream-metadata","text":"You can add arbitrary string metadata to any stream. You can also create a new stream by sending metadata using a stream id that does not already exist.","title":"Add Stream metadata"},{"location":"apis/streaming-writer-api/stream-metadata/#before-you-begin","text":"You should have a Workspace set up with at least one Topic . Get a Personal Access Token to authenticate each request.","title":"Before you begin"},{"location":"apis/streaming-writer-api/stream-metadata/#how-to-add-metadata-to-a-stream","text":"Send a PUT request to the following endpoint to update a stream with the given properties: /topics/${topicName}/streams/${streamId} You should replace $\\{topicName} with the name of the topic your stream belongs to, and $\\{streamId} with the id of the stream you wish to update. For example: /topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c Tip You can create a new stream by supplying a $\\{streamId} that doesn\u2019t already exist. It will be initialised with the data you provide in the payload, and the id you use in the endpoint. This avoids the need to call the create stream endpoint separately. Your request should contain a payload consisting of JSON data containing the desired metadata.","title":"How to add metadata to a stream"},{"location":"apis/streaming-writer-api/stream-metadata/#example-request","text":"Below is an example payload demonstrating how to set a single item of metadata. Note that the metadata property references an object which contains key/value string-based metadata. curl curl \"https:// ${ domain } .platform.quix.ai/topics/ ${ topicName } /streams/ ${ streamId } \" \\ -X PUT \\ -H \"Authorization: bearer ${ token } \" \\ -H \"Content-Type: application/json\" \\ -d '{\"metadata\":{\"fruit\":\"apple\"}}' Node.js const https = require ( 'https' ); const data = JSON . stringify ({ metadata : { fruit : \"apple\" }}); const options = { hostname : domain + '.platform.quix.ai' , path : '/topics/' + topicName + '/streams/' + streamId , method : 'PUT' , headers : { 'Authorization' : 'Bearer ' + token , 'Content-Type' : 'application/json' } }; const req = https . request ( options ); req . write ( data ); req . end (); Since this is a PUT request, it will replace all the stream data with the payload contents. To maintain existing data, you should include it in the payload alongside your metadata, e.g. { \"name\" : \"Example stream\" , \"location\" : \"/sub/dir\" , \"metadata\" : { \"fruit\" : \"apple\" } }","title":"Example request"},{"location":"apis/streaming-writer-api/stream-metadata/#response","text":"No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong. For example, you\u2019ll see a 405 code if you forget to specify the correct PUT method.","title":"Response"},{"location":"apis/streaming-writer-api/stream-metadata/#using-signalr","text":"var signalR = require ( \"@microsoft/signalr\" ); const token = \"YOUR_TOKEN\" const workspaceId = \"YOUR_WORKSPACE_ID\" const topic = \"YOUR_TOPIC_NAME\" const streamId = \"ID_OF_STREAM_TO_UPDATE\" const options = { accessTokenFactory : () => token }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://writer-\" + workspaceId + \".platform.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then ( async () => { console . log ( \"Connected to Quix.\" ); // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model. let streamDetails = { \"name\" : \"Example stream\" , \"location\" : \"/sub/dir\" , \"metadata\" : { \"fruit\" : \"apple\" } } // Send stream update details console . log ( \"Updating stream\" ); await connection . invoke ( \"UpdateStream\" , topic , streamDetails ); console . log ( \"Updated stream\" ); }); Tip Also available as JsFiddle at https://jsfiddle.net/QuixAI/ruywnz28/","title":"Using SignalR"},{"location":"platform/MLOps/","text":"MLOPs There are a number of barriers that prevent companies from successfully implementing data and ML projects. It\u2019s generally considered to be significantly harder than implementing software projects due to the cross functional complexity of data and ML pipelines. As a result, even if you hire the best data scientists, their work often fails and many companies give-up on their projects before they\u2019ve begun to see the value of the technologies. Solving these challenges is a new field of expertise called MLOps We are working to incorporate MLOps into Quix so that your data team has a seamless journey from concept to production. The key steps are: Discover and access data Any member of any team can quickly access data in the Catalogue without support from software or regulatory teams. Develop features in historic data Use Visualise to discover, segment, label and store significant features in the catalogue. Build & train models on historic data Use Develop and Deploy to: Write model code in Python using their favourite IDE. Train models on historic data. Evaluate results against raw data and results from other models. Rapidly iterate models with GIT version control. Test models on live data Connect models to live input topics to test them against live data sources. Review the results in Visualise. Build a production pipeline Use Develop and Deploy to: Connect validated models to live output topics. Daisy-chain models using input and output topics. Work seamlessly with engineers to hook-up software services. Deploy production models With one click, data engineers can deploy their Python models to production without support from software engineering or DevOps teams. Monitor production models Data teams can: Ensure that components in a production pipeline operate correctly through the product lifecycle. Build and deploy services that detect data drift or unexpected results.","title":"MLOps"},{"location":"platform/MLOps/#mlops","text":"There are a number of barriers that prevent companies from successfully implementing data and ML projects. It\u2019s generally considered to be significantly harder than implementing software projects due to the cross functional complexity of data and ML pipelines. As a result, even if you hire the best data scientists, their work often fails and many companies give-up on their projects before they\u2019ve begun to see the value of the technologies. Solving these challenges is a new field of expertise called MLOps We are working to incorporate MLOps into Quix so that your data team has a seamless journey from concept to production. The key steps are:","title":"MLOPs"},{"location":"platform/MLOps/#discover-and-access-data","text":"Any member of any team can quickly access data in the Catalogue without support from software or regulatory teams.","title":"Discover and access data"},{"location":"platform/MLOps/#develop-features-in-historic-data","text":"Use Visualise to discover, segment, label and store significant features in the catalogue.","title":"Develop features in historic data"},{"location":"platform/MLOps/#build-train-models-on-historic-data","text":"Use Develop and Deploy to: Write model code in Python using their favourite IDE. Train models on historic data. Evaluate results against raw data and results from other models. Rapidly iterate models with GIT version control.","title":"Build &amp; train models on historic data"},{"location":"platform/MLOps/#test-models-on-live-data","text":"Connect models to live input topics to test them against live data sources. Review the results in Visualise.","title":"Test models on live data"},{"location":"platform/MLOps/#build-a-production-pipeline","text":"Use Develop and Deploy to: Connect validated models to live output topics. Daisy-chain models using input and output topics. Work seamlessly with engineers to hook-up software services.","title":"Build a production pipeline"},{"location":"platform/MLOps/#deploy-production-models","text":"With one click, data engineers can deploy their Python models to production without support from software engineering or DevOps teams.","title":"Deploy production models"},{"location":"platform/MLOps/#monitor-production-models","text":"Data teams can: Ensure that components in a production pipeline operate correctly through the product lifecycle. Build and deploy services that detect data drift or unexpected results.","title":"Monitor production models"},{"location":"platform/definitions/","text":"Definitions The following is a list of definitions to aid understanding of how to work with Quix and streaming data. Workspace A Workspace is an instance of a complete streaming infrastructure isolated from the rest of your Organization in terms of performance and security. It contains his own dedicated API instances and Quix internal services. You can imagine a Workspace as the streaming infrastructure of your company or your team, where you don\u2019t want other operations except the ones being developed in that workspace affecting the performance or the stability of your application. You can also have different workspaces to separate different stages of your development process like Development, Staging, and Production. Topics A Topic is a channel of real-time data. You can imagine a topic as the pipe we use to interconnect our streaming applications. It is highly recommended to organize the data of a topic with some kind of grouping context for the telemetry data coming from a single source. Very simplified, a topic is similar to a folder in a filesystem, the streams are the files in that folder, and your data is the contents of each file. For example: Car engine data Game data Telemetry from one ECU on a Boeing 737 Topics are key for scalability and good data governance. Use them to organise your data by: Grouping incoming data by type or source Maintaining separate topics for raw, clean or processed data Stream A stream is a collection of data (parameters, events, binary blobs and metadata) that belong to a single session of a single source. For example: One journey for one car One game session for one player One flight for one aeroplane Timestamp A timestamp is the primary key for all data in a stream. We support nanosecond precision; that\u2019s 1 x 10-9 seconds or one-billionth of a second! Nanosecond precision is at the bleeding edge of real-time computing and is primarily driven by innovation with hardware and networking technology; kudos to you if you have an application for it! Data Types We currently support any parameter, event, metadata or blob that consist of numeric (double precision), string (UTF-8) and binary data (blobs). Parameters Parameters are values that develop over time. The Quix SDK supports numeric and string values. For example: Crank revolution and oil temperature are two discrete engine parameters that begin to define the engine system. Player position in X, Y and Z are three discreet parameters that begin to define the player location in a game. Altitude, GPS LAT, GPS LONG and Speed are four parameters that begin to define the location and velocity of a plane in the sky. Referring back to topics as a grouping context: we would recommend that each of these examples would be grouped into a single topic to maintain context. Events Events are a discrete occurrence of a thing that happens or takes place. For example: Engine start, engine stop, warning light activated Game started, match made, kill made, player won the race, lap completed, track limits exceeded, task completed Takeoff, landing, missile launched, fuel low, autopilot engaged, pilot ejected Events are typically things that occur less frequently. They are streamed into the same topics as their respective parameters and act to provide some context to what is happening. Start and stop events mark the beginning and end of data streams. Metadata Metadata describes additional information or context about a stream. For example: License plate number, car manufacturer, car model, car engine type, driver ID, Game version, player name, game session type, game session settings, race car set-up Flight number, destination, airport of origin, pilot ID, airplane type Metadata typically has no time context, rather it exists as a constant throughout one or more streams. For example, your metadata could be the configuration of a car that is sold from a dealership (such as engine size, transmission type, wheel size, tyre model etc); you could create a stream every time that car is driven by the owner, but the engine size and transmission type won\u2019t change. Metadata is key to data governance and becomes very useful in down-stream data processing and analytics. Binary data Quix also supports any binary blob data. With this data you can stream, process and store any type of audio, image, video or lidar data, or anything that isn\u2019t supported with our parameter, event or metadata types. Project A set of code which can be edited, compiled, executed and deployed as one Docker image. Online IDE We provide an online integrated development environment for python projects. When you open any python project, you will see the Run button and a console during runtime in addition to the intellisense for python files. Deployment An instance of a Project running in the serverless environment. Service Any application code that is continuously running in the serverless environment. For example, a bridge, a function, a backend operation, or an integration to a third party service like Twilio. Job Any application code that is run once. For example, use a job to run a batch import of data from an existing data store (CSV, DB or DataLake etc). SDK Quix SDK is the main library we use to send and receive real-time data in our streaming applications. API\u2019s Streaming Writer API A HTTP API used to send telemetry data from any source to a topic in the Quix platform. It should be used when it is not possible to use directly our SDK . Streaming Reader API A Websockets API used to stream any data directly from a topic to an external application. Most commonly used to read the results of a model or service to a real-time web application. Data Catalogue API An HTTP API used to query historic data in the Data Catalogue. Most commonly used for dashboarding, analytics and training ML models. Also useful to call historic data when running an ML model or to call historic data from an external application. Portal API An HTTP API used to interact with most portal-related features such as creation of Workspaces, Users, Deployments, etc.","title":"Definitions"},{"location":"platform/definitions/#definitions","text":"The following is a list of definitions to aid understanding of how to work with Quix and streaming data.","title":"Definitions"},{"location":"platform/definitions/#workspace","text":"A Workspace is an instance of a complete streaming infrastructure isolated from the rest of your Organization in terms of performance and security. It contains his own dedicated API instances and Quix internal services. You can imagine a Workspace as the streaming infrastructure of your company or your team, where you don\u2019t want other operations except the ones being developed in that workspace affecting the performance or the stability of your application. You can also have different workspaces to separate different stages of your development process like Development, Staging, and Production.","title":"Workspace"},{"location":"platform/definitions/#topics","text":"A Topic is a channel of real-time data. You can imagine a topic as the pipe we use to interconnect our streaming applications. It is highly recommended to organize the data of a topic with some kind of grouping context for the telemetry data coming from a single source. Very simplified, a topic is similar to a folder in a filesystem, the streams are the files in that folder, and your data is the contents of each file. For example: Car engine data Game data Telemetry from one ECU on a Boeing 737 Topics are key for scalability and good data governance. Use them to organise your data by: Grouping incoming data by type or source Maintaining separate topics for raw, clean or processed data","title":"Topics"},{"location":"platform/definitions/#stream","text":"A stream is a collection of data (parameters, events, binary blobs and metadata) that belong to a single session of a single source. For example: One journey for one car One game session for one player One flight for one aeroplane","title":"Stream"},{"location":"platform/definitions/#timestamp","text":"A timestamp is the primary key for all data in a stream. We support nanosecond precision; that\u2019s 1 x 10-9 seconds or one-billionth of a second! Nanosecond precision is at the bleeding edge of real-time computing and is primarily driven by innovation with hardware and networking technology; kudos to you if you have an application for it!","title":"Timestamp"},{"location":"platform/definitions/#data-types","text":"We currently support any parameter, event, metadata or blob that consist of numeric (double precision), string (UTF-8) and binary data (blobs).","title":"Data Types"},{"location":"platform/definitions/#parameters","text":"Parameters are values that develop over time. The Quix SDK supports numeric and string values. For example: Crank revolution and oil temperature are two discrete engine parameters that begin to define the engine system. Player position in X, Y and Z are three discreet parameters that begin to define the player location in a game. Altitude, GPS LAT, GPS LONG and Speed are four parameters that begin to define the location and velocity of a plane in the sky. Referring back to topics as a grouping context: we would recommend that each of these examples would be grouped into a single topic to maintain context.","title":"Parameters"},{"location":"platform/definitions/#events","text":"Events are a discrete occurrence of a thing that happens or takes place. For example: Engine start, engine stop, warning light activated Game started, match made, kill made, player won the race, lap completed, track limits exceeded, task completed Takeoff, landing, missile launched, fuel low, autopilot engaged, pilot ejected Events are typically things that occur less frequently. They are streamed into the same topics as their respective parameters and act to provide some context to what is happening. Start and stop events mark the beginning and end of data streams.","title":"Events"},{"location":"platform/definitions/#metadata","text":"Metadata describes additional information or context about a stream. For example: License plate number, car manufacturer, car model, car engine type, driver ID, Game version, player name, game session type, game session settings, race car set-up Flight number, destination, airport of origin, pilot ID, airplane type Metadata typically has no time context, rather it exists as a constant throughout one or more streams. For example, your metadata could be the configuration of a car that is sold from a dealership (such as engine size, transmission type, wheel size, tyre model etc); you could create a stream every time that car is driven by the owner, but the engine size and transmission type won\u2019t change. Metadata is key to data governance and becomes very useful in down-stream data processing and analytics.","title":"Metadata"},{"location":"platform/definitions/#binary-data","text":"Quix also supports any binary blob data. With this data you can stream, process and store any type of audio, image, video or lidar data, or anything that isn\u2019t supported with our parameter, event or metadata types.","title":"Binary data"},{"location":"platform/definitions/#project","text":"A set of code which can be edited, compiled, executed and deployed as one Docker image.","title":"Project"},{"location":"platform/definitions/#online-ide","text":"We provide an online integrated development environment for python projects. When you open any python project, you will see the Run button and a console during runtime in addition to the intellisense for python files.","title":"Online IDE"},{"location":"platform/definitions/#deployment","text":"An instance of a Project running in the serverless environment.","title":"Deployment"},{"location":"platform/definitions/#service","text":"Any application code that is continuously running in the serverless environment. For example, a bridge, a function, a backend operation, or an integration to a third party service like Twilio.","title":"Service"},{"location":"platform/definitions/#job","text":"Any application code that is run once. For example, use a job to run a batch import of data from an existing data store (CSV, DB or DataLake etc).","title":"Job"},{"location":"platform/definitions/#sdk","text":"Quix SDK is the main library we use to send and receive real-time data in our streaming applications.","title":"SDK"},{"location":"platform/definitions/#apis","text":"","title":"API\u2019s"},{"location":"platform/definitions/#streaming-writer-api","text":"A HTTP API used to send telemetry data from any source to a topic in the Quix platform. It should be used when it is not possible to use directly our SDK .","title":"Streaming Writer API"},{"location":"platform/definitions/#streaming-reader-api","text":"A Websockets API used to stream any data directly from a topic to an external application. Most commonly used to read the results of a model or service to a real-time web application.","title":"Streaming Reader API"},{"location":"platform/definitions/#data-catalogue-api","text":"An HTTP API used to query historic data in the Data Catalogue. Most commonly used for dashboarding, analytics and training ML models. Also useful to call historic data when running an ML model or to call historic data from an external application.","title":"Data Catalogue API"},{"location":"platform/definitions/#portal-api","text":"An HTTP API used to interact with most portal-related features such as creation of Workspaces, Users, Deployments, etc.","title":"Portal API"},{"location":"platform/intro/","text":"What is Quix? Quix is a platform for developing and deploying applications with streaming data. We architected Quix natively around a message broker (specifically Kafka ) because we know databases are in the way of building low-latency applications that scale cost-effectively. Instead of working with data on a disk, developers could work with live data in-memory, if broker technologies were easier to use. But they are not easy to use, especially for Python developers who are at the forefront of data science but cannot easily work with streaming data. Quix provides everything a developer needs to build applications with streaming data. By using Quix you can build new products faster whilst keeping your data in-memory, helping to achieve lower latencies and lower operating costs. From the top-down, our stack provides a Web UI, API\u2019s and SDK that abstract developers off our underlying infrastructure, including fully-managed Kafka topics, serverless compute environment and a metadata-driven data catalogue (time-series database with steroids). Web UI With the Quix Portal we are striving to make a beautiful software experience that facilitates DevOps/MLOps best-practices for less-experienced development teams. Our goals are to: Help less expert people access live data Help them create and manage complex infrastructure and write application code without support from expert engineering teams, and Help to accelerate the development lifecycle by enabling developers to test and iterate code in an always-live environment. To achieve these goals Quix Portal includes the following features: Online IDE : Develop and Run your streaming applications directly on the browser without setting up a local environment. Library : Choose between hundreds of autogenerated code examples ready to run and deploy from our Online IDE. One click deployments : Deploy and manage your streaming applications on production with a simple user interface. Monitoring tools : Monitor in real-time the status and the data flow of your streaming applications. Broker management : Create, Delete, Explore or Configure your message broker infrastructure with just a click of a button. Pipeline view : Visualize your pipeline architecture with the information provided from the deployment variables. Data Explorer : Explore Live and Historical data of your applications to test that your code is working as expected. API\u2019s We have provided four API\u2019s to help you work with streaming data. These include: Stream Writer API : helps you send any data to a Kafka topic in Quix using HTTP. This API handles encryption, serialisation and conversion to the Quix SDK format ensuring efficiency and performance of down-stream processing regardless of the data source. Stream Reader API : helps you push live data from a Quix topic to your application ensuring super low latency by avoiding any disk operations. Data Catalogue API : lets you query historic data streams in the data catalogue to train ML models, build dashboards and export data to other systems. Portal API : lets you automate Portal tasks like creating workspaces, topics and deployments. SDK Python is the dominant language for data science and machine learning, but it is quite incompatible with streaming technologies (like Kafka ) which are predominantly written in Java and Scala. Our Quix streaming SDK is a client library that abstracts Python developers off streaming-centric complexities like learning Java or dealing with buffering, serialisation and encryption. Instead, SDK serves you streaming data in a data frame so you can write any simple or complex data processing logic and connect it directly to the broker. There are just a few key streaming concepts that you must learn. You can read about them here . Serverless compute Quix provides an easy way to run code in an elastic serverless compute environment. It automatically builds code in GIT into a docker image and deploys containers to Kubernetes. This otherwise very complicated procedure is done by a couple of clicks in the Quix web portal. Architecture Git integration Source code for workspace projects (models, connectors and services) is hosted in GIT repositories. Developers can check out repositories and develop locally and collaborate using GIT protocol. Code is deployed to the Quix serverless environment using tags in GIT. Quix builds service will build selected GIT commit into a docker image. Docker integration Each code example generated using the Quix library is shipped with a Dockerfile that is designed to work in the Quix serverless compute environment powered by Kubernetes . You can alter this file if necessary. When you deploy a service with Quix, a code reference to GIT with a build request is sent to the build queue. The build service will build a docker image and save it in the docker registry. In the next step, this image is deployed to Kubernetes. Tip If there is any problem with the docker build process, you can check the build logs . Tip Hover over the deployment in the deployments page to download the docker image of the deployed service for local testing or custom deployment. Kubernetes integration Quix manages an elastic compute environment so you don\u2019t need to worry about servers, nodes, memory, CPU, etc. Quix will make sure that your container is deployed to the right server in the cluster. We provide the following integrations with Kubernetes: Logs from container accessible in the portal or via portal API. Environment variables allows passing variables into the docker image deployment. So code can be parameterized. Replica number for horizontal scale. CPU limit. Memory limit. Deployment type - Options of one-time job or continuously running service, Ingress - Optional ingress mapped to port 80. Tip If a deployment reference is already built and deployed to a service, the build process is skipped and the docker image from the container registry is used instead. DNS integration The Quix serverless environment offers DNS routing for services on port 80. That means that any API or frontend can be hosted in Quix with no extra complexity. Load balancing is provided out of the box, just increase the replica count to provide resiliency to your deployed API or frontend. Warning A newly deployed service with DNS routing takes up to 10 minutes to propagate to all DNS servers in the network. Managed Kafka topics Quix provides fully managed Kafka topics which are used to stream data and build data processing pipelines by daisy-chaining models together. Our topics are multi-tenant which means you don\u2019t have to build and maintain an entire cluster to stream a few bytes of data. Instead, you can start quickly and cheaply by creating one topic for your application and only pay for the resources consumed when streaming that data. When your solution grows in data volume or complexity you can just add more topics without concern for the underlying infrastructure which is handled by us. Together with our SDK and serverless compute, you can connect your models directly to our topics to read and write data using the pub/sub pattern. This keeps the data in-memory to deliver low-latency and cost effective stream processing capabilities. Note Quix also provides the ability to connect external infrastructure components like your own message broker infrastructure. Data Catalogue We provide a data catalogue for long-term storage, analytics and data science activities. We have combined what we know to be the best database technologies for each data type into a unified catalogue. There\u2019s a timeseries database for recording your events and parameter values, blob storage for your binary data, and a NoSQL DB for recording your metadata. Our data catalogue technology has two advantages: It allocates each data type to the optimal database technology for that type. This increases read/write and query performance which reduces operating costs. It uses your metadata to record your context. This makes your data more usable for more people across your organisation who only need to know your business context to navigate vast quantities of data.","title":"What is Quix?"},{"location":"platform/intro/#what-is-quix","text":"Quix is a platform for developing and deploying applications with streaming data. We architected Quix natively around a message broker (specifically Kafka ) because we know databases are in the way of building low-latency applications that scale cost-effectively. Instead of working with data on a disk, developers could work with live data in-memory, if broker technologies were easier to use. But they are not easy to use, especially for Python developers who are at the forefront of data science but cannot easily work with streaming data. Quix provides everything a developer needs to build applications with streaming data. By using Quix you can build new products faster whilst keeping your data in-memory, helping to achieve lower latencies and lower operating costs. From the top-down, our stack provides a Web UI, API\u2019s and SDK that abstract developers off our underlying infrastructure, including fully-managed Kafka topics, serverless compute environment and a metadata-driven data catalogue (time-series database with steroids).","title":"What is Quix?"},{"location":"platform/intro/#web-ui","text":"With the Quix Portal we are striving to make a beautiful software experience that facilitates DevOps/MLOps best-practices for less-experienced development teams. Our goals are to: Help less expert people access live data Help them create and manage complex infrastructure and write application code without support from expert engineering teams, and Help to accelerate the development lifecycle by enabling developers to test and iterate code in an always-live environment. To achieve these goals Quix Portal includes the following features: Online IDE : Develop and Run your streaming applications directly on the browser without setting up a local environment. Library : Choose between hundreds of autogenerated code examples ready to run and deploy from our Online IDE. One click deployments : Deploy and manage your streaming applications on production with a simple user interface. Monitoring tools : Monitor in real-time the status and the data flow of your streaming applications. Broker management : Create, Delete, Explore or Configure your message broker infrastructure with just a click of a button. Pipeline view : Visualize your pipeline architecture with the information provided from the deployment variables. Data Explorer : Explore Live and Historical data of your applications to test that your code is working as expected.","title":"Web UI"},{"location":"platform/intro/#apis","text":"We have provided four API\u2019s to help you work with streaming data. These include: Stream Writer API : helps you send any data to a Kafka topic in Quix using HTTP. This API handles encryption, serialisation and conversion to the Quix SDK format ensuring efficiency and performance of down-stream processing regardless of the data source. Stream Reader API : helps you push live data from a Quix topic to your application ensuring super low latency by avoiding any disk operations. Data Catalogue API : lets you query historic data streams in the data catalogue to train ML models, build dashboards and export data to other systems. Portal API : lets you automate Portal tasks like creating workspaces, topics and deployments.","title":"API\u2019s"},{"location":"platform/intro/#sdk","text":"Python is the dominant language for data science and machine learning, but it is quite incompatible with streaming technologies (like Kafka ) which are predominantly written in Java and Scala. Our Quix streaming SDK is a client library that abstracts Python developers off streaming-centric complexities like learning Java or dealing with buffering, serialisation and encryption. Instead, SDK serves you streaming data in a data frame so you can write any simple or complex data processing logic and connect it directly to the broker. There are just a few key streaming concepts that you must learn. You can read about them here .","title":"SDK"},{"location":"platform/intro/#serverless-compute","text":"Quix provides an easy way to run code in an elastic serverless compute environment. It automatically builds code in GIT into a docker image and deploys containers to Kubernetes. This otherwise very complicated procedure is done by a couple of clicks in the Quix web portal.","title":"Serverless compute"},{"location":"platform/intro/#architecture","text":"","title":"Architecture"},{"location":"platform/intro/#git-integration","text":"Source code for workspace projects (models, connectors and services) is hosted in GIT repositories. Developers can check out repositories and develop locally and collaborate using GIT protocol. Code is deployed to the Quix serverless environment using tags in GIT. Quix builds service will build selected GIT commit into a docker image.","title":"Git integration"},{"location":"platform/intro/#docker-integration","text":"Each code example generated using the Quix library is shipped with a Dockerfile that is designed to work in the Quix serverless compute environment powered by Kubernetes . You can alter this file if necessary. When you deploy a service with Quix, a code reference to GIT with a build request is sent to the build queue. The build service will build a docker image and save it in the docker registry. In the next step, this image is deployed to Kubernetes. Tip If there is any problem with the docker build process, you can check the build logs . Tip Hover over the deployment in the deployments page to download the docker image of the deployed service for local testing or custom deployment.","title":"Docker integration"},{"location":"platform/intro/#kubernetes-integration","text":"Quix manages an elastic compute environment so you don\u2019t need to worry about servers, nodes, memory, CPU, etc. Quix will make sure that your container is deployed to the right server in the cluster. We provide the following integrations with Kubernetes: Logs from container accessible in the portal or via portal API. Environment variables allows passing variables into the docker image deployment. So code can be parameterized. Replica number for horizontal scale. CPU limit. Memory limit. Deployment type - Options of one-time job or continuously running service, Ingress - Optional ingress mapped to port 80. Tip If a deployment reference is already built and deployed to a service, the build process is skipped and the docker image from the container registry is used instead.","title":"Kubernetes integration"},{"location":"platform/intro/#dns-integration","text":"The Quix serverless environment offers DNS routing for services on port 80. That means that any API or frontend can be hosted in Quix with no extra complexity. Load balancing is provided out of the box, just increase the replica count to provide resiliency to your deployed API or frontend. Warning A newly deployed service with DNS routing takes up to 10 minutes to propagate to all DNS servers in the network.","title":"DNS integration"},{"location":"platform/intro/#managed-kafka-topics","text":"Quix provides fully managed Kafka topics which are used to stream data and build data processing pipelines by daisy-chaining models together. Our topics are multi-tenant which means you don\u2019t have to build and maintain an entire cluster to stream a few bytes of data. Instead, you can start quickly and cheaply by creating one topic for your application and only pay for the resources consumed when streaming that data. When your solution grows in data volume or complexity you can just add more topics without concern for the underlying infrastructure which is handled by us. Together with our SDK and serverless compute, you can connect your models directly to our topics to read and write data using the pub/sub pattern. This keeps the data in-memory to deliver low-latency and cost effective stream processing capabilities. Note Quix also provides the ability to connect external infrastructure components like your own message broker infrastructure.","title":"Managed Kafka topics"},{"location":"platform/intro/#data-catalogue","text":"We provide a data catalogue for long-term storage, analytics and data science activities. We have combined what we know to be the best database technologies for each data type into a unified catalogue. There\u2019s a timeseries database for recording your events and parameter values, blob storage for your binary data, and a NoSQL DB for recording your metadata. Our data catalogue technology has two advantages: It allocates each data type to the optimal database technology for that type. This increases read/write and query performance which reduces operating costs. It uses your metadata to record your context. This makes your data more usable for more people across your organisation who only need to know your business context to navigate vast quantities of data.","title":"Data Catalogue"},{"location":"platform/landing-page/","text":"Get Started If you\u2019re new to Quix, these resources are the best place to get you up and running quickly. What is Quix?. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Quick start guides. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Code sample library. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Core resources Take a look under the hood of Quix and get to know our SDK and APIs. Streaming SDK. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Streaming APIs. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Automation API. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Build your project Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Docs. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Model deployment. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Stream live data. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Work with the catalogue. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Model training. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Integrate to client apps. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Learn from the community Take a look at our community resources to learn how other developers and data teams are using Quix to solve their software engineering and data science problems. Discourse. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Slack. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Github. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More","title":"Get Started"},{"location":"platform/landing-page/#get-started","text":"If you\u2019re new to Quix, these resources are the best place to get you up and running quickly. What is Quix?. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Quick start guides. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Code sample library. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More","title":"Get Started"},{"location":"platform/landing-page/#core-resources","text":"Take a look under the hood of Quix and get to know our SDK and APIs. Streaming SDK. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Streaming APIs. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Automation API. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More","title":"Core resources"},{"location":"platform/landing-page/#build-your-project","text":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Docs. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Model deployment. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Stream live data. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Work with the catalogue. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Model training. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Integrate to client apps. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More","title":"Build your project"},{"location":"platform/landing-page/#learn-from-the-community","text":"Take a look at our community resources to learn how other developers and data teams are using Quix to solve their software engineering and data science problems. Discourse. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Slack. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More Github. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy. Learn More","title":"Learn from the community"},{"location":"platform/how-to/create-dlq/","text":"Create a Dead Letter Queue When your code identifies a dead letter, or unprocessable message, simply send the message to the dead letter topic and continue processing other messages. What you do with messages in the dead letter topic is up to you. Example Take a look at the short example. It describes how to open topics, create streams and also send messages to a dedicated dead letter topic or queue. Python C# # open topics input_topic = client . open_input_topic ( 'INPUT_DATA' ) output_topic = client . open_output_topic ( 'OUTPUT_DATA' ) dead_letter_topic = client . open_output_topic ( 'UNPROCESSABLE' ) # create streams output_stream = output_topic . create_stream () dlq_stream = dead_letter_topic . create_stream () # open the input stream # and start handling messages def on_stream_received_handler ( new_stream : StreamReader ): def on_parameter_data_handler ( data_in : ParameterData ): try : # get a data value data_to_process = data_in . timestamps [ 0 ] . parameters [ 'ParameterA' ] . numeric_value # prepare the data packet for onward processing data_out = ParameterData () data_out . add_timestamp_nanoseconds ( 1 ) \\ . add_value ( \"Speed\" , data_to_process ) # it was ok so pass to the output stream # to be processed by the next stage in the pipeline output_stream . parameters . write ( data_out ) except Exception : # There was an error during processing. # Print the error and forward data into dead letter queue. print ( traceback . format_exc ()) dlq_stream . parameters . write ( data_in ) # hook up on read handler new_stream . on_read += on_parameter_data_handler input_topic . on_stream_received += on_stream_received_handler input_topic . start_reading () var inputTopic = client . OpenInputTopic ( \"INPUT_DATA\" ); var outputTopic = client . OpenOutputTopic ( \"OUTPUT_DATA\" ); var deadLetterTopic = client . OpenOutputTopic ( \"UNPROCESSABLE\" ); // create streams var outputStream = outputTopic . CreateStream (); var dlqStream = deadLetterTopic . CreateStream (); // open the input stream inputTopic . OnStreamReceived += ( s , streamReader ) => { streamReader . Parameters . OnRead += dataIn => { try { // get the data to process var numValue = dataIn . Timestamps [ 0 ]. Parameters [ ' ParameterA ' ]. NumericValue ; // create the data for onward processing var data = new ParameterData (); data . AddTimestampNanoseconds ( 1 ) . AddValue ( \"Speed\" , numValue ); // it was ok so pass to the output stream // to be processed by the next stage in the pipeline outputStream . Parameters . Write ( data ); } catch ( Exception e ) { // There was an error during processing. // Print the error and forward data into dead letter queue. System . Console . WriteLine ( e ); dlqStream . Parameters . Write ( dataIn ); } }; }; inputTopic . StartReading ();","title":"Create a Dead Letter Queue"},{"location":"platform/how-to/create-dlq/#create-a-dead-letter-queue","text":"When your code identifies a dead letter, or unprocessable message, simply send the message to the dead letter topic and continue processing other messages. What you do with messages in the dead letter topic is up to you.","title":"Create a Dead Letter Queue"},{"location":"platform/how-to/create-dlq/#example","text":"Take a look at the short example. It describes how to open topics, create streams and also send messages to a dedicated dead letter topic or queue. Python C# # open topics input_topic = client . open_input_topic ( 'INPUT_DATA' ) output_topic = client . open_output_topic ( 'OUTPUT_DATA' ) dead_letter_topic = client . open_output_topic ( 'UNPROCESSABLE' ) # create streams output_stream = output_topic . create_stream () dlq_stream = dead_letter_topic . create_stream () # open the input stream # and start handling messages def on_stream_received_handler ( new_stream : StreamReader ): def on_parameter_data_handler ( data_in : ParameterData ): try : # get a data value data_to_process = data_in . timestamps [ 0 ] . parameters [ 'ParameterA' ] . numeric_value # prepare the data packet for onward processing data_out = ParameterData () data_out . add_timestamp_nanoseconds ( 1 ) \\ . add_value ( \"Speed\" , data_to_process ) # it was ok so pass to the output stream # to be processed by the next stage in the pipeline output_stream . parameters . write ( data_out ) except Exception : # There was an error during processing. # Print the error and forward data into dead letter queue. print ( traceback . format_exc ()) dlq_stream . parameters . write ( data_in ) # hook up on read handler new_stream . on_read += on_parameter_data_handler input_topic . on_stream_received += on_stream_received_handler input_topic . start_reading () var inputTopic = client . OpenInputTopic ( \"INPUT_DATA\" ); var outputTopic = client . OpenOutputTopic ( \"OUTPUT_DATA\" ); var deadLetterTopic = client . OpenOutputTopic ( \"UNPROCESSABLE\" ); // create streams var outputStream = outputTopic . CreateStream (); var dlqStream = deadLetterTopic . CreateStream (); // open the input stream inputTopic . OnStreamReceived += ( s , streamReader ) => { streamReader . Parameters . OnRead += dataIn => { try { // get the data to process var numValue = dataIn . Timestamps [ 0 ]. Parameters [ ' ParameterA ' ]. NumericValue ; // create the data for onward processing var data = new ParameterData (); data . AddTimestampNanoseconds ( 1 ) . AddValue ( \"Speed\" , numValue ); // it was ok so pass to the output stream // to be processed by the next stage in the pipeline outputStream . Parameters . Write ( data ); } catch ( Exception e ) { // There was an error during processing. // Print the error and forward data into dead letter queue. System . Console . WriteLine ( e ); dlqStream . Parameters . Write ( dataIn ); } }; }; inputTopic . StartReading ();","title":"Example"},{"location":"platform/how-to/get-workspace-id/","text":"Get Workspace ID Occasionally, you\u2019ll need to obtain an ID based on a specific workspace. For example, endpoints for the Data Catalogue API use a domain with the following pattern: https://telemetry-query-${workspace-id}.platform.quix.ai/ The workspace ID is a combination of your organisation and workspace names, converted to URL friendly values. The easist way to get hold of it is as follows: Go to the Portal home . Locate the workspace you\u2019re interested in and click OPEN . At this point, take note of the URL. It should be in the form: https://portal.platform.quix.ai/home?workspace=**{workspace-id}** Copy that value and use it wherever you need a workspace ID.","title":"Get Workspace ID"},{"location":"platform/how-to/get-workspace-id/#get-workspace-id","text":"Occasionally, you\u2019ll need to obtain an ID based on a specific workspace. For example, endpoints for the Data Catalogue API use a domain with the following pattern: https://telemetry-query-${workspace-id}.platform.quix.ai/ The workspace ID is a combination of your organisation and workspace names, converted to URL friendly values. The easist way to get hold of it is as follows: Go to the Portal home . Locate the workspace you\u2019re interested in and click OPEN . At this point, take note of the URL. It should be in the form: https://portal.platform.quix.ai/home?workspace=**{workspace-id}** Copy that value and use it wherever you need a workspace ID.","title":"Get Workspace ID"},{"location":"platform/how-to/jupyter-nb/","text":"Use Jupyter notebooks In this article, you will learn how to use Jupyter Notebook to analyse data persisted in the Quix platform Why this is important Although Quix is a realtime platform, to build realtime in-memory models and data processing pipelines, we need to understand data first. To do that, Quix offers a Data catalogue that makes data discovery and analysis so much easier. Preparation You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix . You will also need Python 3 environment set up in your local environment. Install Jupyter notebooks as directed here . Create a new notebook file You can now run jupyter from the Windows start menu or with the following command in an Anaconda Powershell Prompt, or the equivalent for your operating system. jupyter notebook Then create a new Python3 notebook Connecting Jupyter notebook to Data Catalogue The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix. You need to be logged into the platform for this: Select workspace (you likley only have one) Go to the Data Explorer Add a query to visualize some data. Select parameters, events, aggregation and time range Select the Code tab Ensure Python is the selected language Copy the Python code to your Jupyter notebook and execute. Tip If you want to use this generated code for a long time, replace the temporary token with PAT token . See authenticate your requests how to do that. Too much data If you find that the query results in more data than can be handled by Jupyter Notebooks try using the aggregation feature to reduce the amount of data returned. For more info on aggregation check out this short video .","title":"Use Jupyter notebooks"},{"location":"platform/how-to/jupyter-nb/#use-jupyter-notebooks","text":"In this article, you will learn how to use Jupyter Notebook to analyse data persisted in the Quix platform","title":"Use Jupyter notebooks"},{"location":"platform/how-to/jupyter-nb/#why-this-is-important","text":"Although Quix is a realtime platform, to build realtime in-memory models and data processing pipelines, we need to understand data first. To do that, Quix offers a Data catalogue that makes data discovery and analysis so much easier.","title":"Why this is important"},{"location":"platform/how-to/jupyter-nb/#preparation","text":"You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix . You will also need Python 3 environment set up in your local environment. Install Jupyter notebooks as directed here .","title":"Preparation"},{"location":"platform/how-to/jupyter-nb/#create-a-new-notebook-file","text":"You can now run jupyter from the Windows start menu or with the following command in an Anaconda Powershell Prompt, or the equivalent for your operating system. jupyter notebook Then create a new Python3 notebook","title":"Create a new notebook file"},{"location":"platform/how-to/jupyter-nb/#connecting-jupyter-notebook-to-data-catalogue","text":"The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix. You need to be logged into the platform for this: Select workspace (you likley only have one) Go to the Data Explorer Add a query to visualize some data. Select parameters, events, aggregation and time range Select the Code tab Ensure Python is the selected language Copy the Python code to your Jupyter notebook and execute. Tip If you want to use this generated code for a long time, replace the temporary token with PAT token . See authenticate your requests how to do that.","title":"Connecting Jupyter notebook to Data Catalogue"},{"location":"platform/how-to/jupyter-nb/#too-much-data","text":"If you find that the query results in more data than can be handled by Jupyter Notebooks try using the aggregation feature to reduce the amount of data returned. For more info on aggregation check out this short video .","title":"Too much data"},{"location":"platform/how-to/use-sdk-token/","text":"Using an SDK token SDK token is a type of bearer token that can be used to authenticate against some of our APIs to access functionality necessary for streaming actions. Think of SDK tokens like a token you use to access portal but very limited in scope. Each workspace comes with two of these tokens, limited in use for that specific workspace. We call them Token 1 and Token 2 , also known as Current and Next token. How to find You can access these tokens by going to your topics and clicking on broker settings . How to use These tokens can be used to authenticate against the API, but their primary intended use is to be used with the SDK QuixStreamingClient . When using it with QuixStreamingClient, you no longer need to provide all broker credentials manually, they\u2019ll be acquired when needed and set up automatically. When deploying or running an online IDE, among other environment variables Quix__Sdk__Token is injected with value of Token 1 . You should always use Token 1 , unless you\u2019re rotating. Caution Your tokens do not have an expiration date. Treat them as you would a password. If you think they\u2019re exposed, rotate them. Rotating Having two keys lets you update your services without interruption, as both Token 1 and Token 2 are always valid. Rotating deactivates Token 1 , Token 2 takes its place and a new Token 2 will be generated. You have two main options regarding how you rotate. The easiest way to rotate comes with some service downtime. This assumes you do not directly set the token for your QuixStreamingClient, instead you let the platform take care of it for you by using the default Quix__Sdk__Token environment variable. In this scenario all you have to do is rotate keys, stop and start all your deployments. Until a service is restarted it\u2019ll try to communicate with the platform using the deactivated token. If you\u2019re using local envinroments, those need updating manually. The alternative option is a bit more labour intense, but you can achieve no downtime. This requires you to set a new environment variable you control. This should point to the token to be used. Provide the value of this environment variable to QuixStreamingClient by passing it as an argument. Once you have that, set the value of this environment variable to Token 2 and start your services. When you\u2019re sure you replaced the tokens for all services, rotate your keys. Note Only users with Admin role can rotate.","title":"Use SDK token"},{"location":"platform/how-to/use-sdk-token/#using-an-sdk-token","text":"SDK token is a type of bearer token that can be used to authenticate against some of our APIs to access functionality necessary for streaming actions. Think of SDK tokens like a token you use to access portal but very limited in scope. Each workspace comes with two of these tokens, limited in use for that specific workspace. We call them Token 1 and Token 2 , also known as Current and Next token.","title":"Using an SDK token"},{"location":"platform/how-to/use-sdk-token/#how-to-find","text":"You can access these tokens by going to your topics and clicking on broker settings .","title":"How to find"},{"location":"platform/how-to/use-sdk-token/#how-to-use","text":"These tokens can be used to authenticate against the API, but their primary intended use is to be used with the SDK QuixStreamingClient . When using it with QuixStreamingClient, you no longer need to provide all broker credentials manually, they\u2019ll be acquired when needed and set up automatically. When deploying or running an online IDE, among other environment variables Quix__Sdk__Token is injected with value of Token 1 . You should always use Token 1 , unless you\u2019re rotating. Caution Your tokens do not have an expiration date. Treat them as you would a password. If you think they\u2019re exposed, rotate them.","title":"How to use"},{"location":"platform/how-to/use-sdk-token/#rotating","text":"Having two keys lets you update your services without interruption, as both Token 1 and Token 2 are always valid. Rotating deactivates Token 1 , Token 2 takes its place and a new Token 2 will be generated. You have two main options regarding how you rotate. The easiest way to rotate comes with some service downtime. This assumes you do not directly set the token for your QuixStreamingClient, instead you let the platform take care of it for you by using the default Quix__Sdk__Token environment variable. In this scenario all you have to do is rotate keys, stop and start all your deployments. Until a service is restarted it\u2019ll try to communicate with the platform using the deactivated token. If you\u2019re using local envinroments, those need updating manually. The alternative option is a bit more labour intense, but you can achieve no downtime. This requires you to set a new environment variable you control. This should point to the token to be used. Provide the value of this environment variable to QuixStreamingClient by passing it as an argument. Once you have that, set the value of this environment variable to Token 2 and start your services. When you\u2019re sure you replaced the tokens for all services, rotate your keys. Note Only users with Admin role can rotate.","title":"Rotating"},{"location":"platform/how-to/webapps/","text":"Develop web applications with Quix Quix can bring real-time web functionality to you client applications. Following types of applications are good candidates to use Quix as their data plane. Dashboard and real-time monitoring applications that show updates as they happen to users like cloud/edge monitoring tools. Applications that require data to be pushed from a backend at high frequency like games and simulations. Social networking applications that require broadcasting updates to many users at high frequency like live sharing of Strava data. NodeJs NodeJs applications can update parameter and event definitions and write data to streams using RESTful APIs. Quix supports WebSockets for clients that want to receive telemetry data and parameters/events updates in real-time. NodeJs clients must authenticate with Quix using personal access tokens .","title":"Develop web applications with Quix"},{"location":"platform/how-to/webapps/#develop-web-applications-with-quix","text":"Quix can bring real-time web functionality to you client applications. Following types of applications are good candidates to use Quix as their data plane. Dashboard and real-time monitoring applications that show updates as they happen to users like cloud/edge monitoring tools. Applications that require data to be pushed from a backend at high frequency like games and simulations. Social networking applications that require broadcasting updates to many users at high frequency like live sharing of Strava data.","title":"Develop web applications with Quix"},{"location":"platform/how-to/webapps/#nodejs","text":"NodeJs applications can update parameter and event definitions and write data to streams using RESTful APIs. Quix supports WebSockets for clients that want to receive telemetry data and parameters/events updates in real-time. NodeJs clients must authenticate with Quix using personal access tokens .","title":"NodeJs"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/","text":"Run ML model in realtime environment In this article, you will learn how to use pickle file trained on historic data in a realtime environment. Ensure you have completed the previous stage first, if not find it here . Why this is important With the Quix platform, you can run and deploy ML models to the leading edge reacting to data coming from the source with milliseconds latency. End result At the end of this article, we will end up with a live model using the pickle file from How to train ML model to process live data on the edge. Preparation You\u2019ll need to complete the How to train ML model article to get pickle file with trained model logic. Run the model Now let's run the model you created in the previous article. If you have your own model and already know how to run the Python to execute it then these steps might also be useful for you. Ensure you are logged into the Quix Portal Navigate to the Library Filter the library by selecting Python under languages and Transformation under pipeline stage Select the Data Filtering Model Tip If you can't see Data Filtering Model you can also use search to find it Info Usually, after clicking on the Data Filtering Model you can look at the code and the readme to ensure it's the correct sample for your needs. Now click Edit code Change the name to \"Prediction Model\" Ensure the input is \"f1-data\" Ensure the output is \"brake-prediction\" Info The platform will automatically create any topics that don't already exist Success The code from the Library sample is now saved to your workspace. You can edit and run the code from here or clone it to your computer and work locally. See more about setting up your local environment here . Upload the model Now you need to upload the ML model created in the previous article and edit this code to run the model. Click the upload file icon at the top of the file list Find the file saved in the previous article. Hint It's called 'decision_tree_5_depth.sav' and should be in \"C:\\Users[USER]\\\" on Windows Warning When you click off the file e.g. onto quix_function.py, the editor might prompt you to save the .sav file. Click \"Do not commit\" Click quix_function.py in the file list (remember do not commit changes to the model file) Modify the code Add the following statements to import the required libraries import pickle import math In the __init__ function add the following lines to load the model ## Import ML model from file self . model = pickle . load ( open ( 'decision_tree_5_depth.sav' , 'rb' )) Under the __init__ function add the following new function This will pre-process the data, a necessary step before passing it to the model. ## To get the correct output, we preprocess data before we feed them to the trained model def preprocess ( self , df ): signal_limits = { \"Speed\" : ( 0 , 400 ), \"Steer\" : ( - 1 , 1 ), \"Gear\" : ( 0 , 8 ), \"Motion_WorldPositionX\" : ( - math . pi , math . pi ), \"Brake\" : ( 0 , 1 ), } def clamp ( n , minn , maxn ): return max ( min ( maxn , n ), minn ) for signal , limits in signal_limits . items (): df [ signal ] = df [ signal ] . map ( lambda x : clamp ( x , limits [ 0 ], limits [ 1 ])) df [ \"Motion_WorldPositionX_sin\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . sin ( x )) df [ \"Motion_WorldPositionX_cos\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . cos ( x )) return df Delete the on_pandas_frame_handler function and paste this code in it's place. # Callback triggered for each new parameter data. def on_pandas_frame_handler ( self , df : pd . DataFrame ): # if no speed column, skip this record if not \"Speed\" in df . columns : return df output_df = pd . DataFrame () # Preprocessing df = self . preprocess ( df ) features = [ \"Motion_WorldPositionX_cos\" , \"Motion_WorldPositionX_sin\" , \"Steer\" , \"Speed\" , \"Gear\" ] X = df [ features ] # Lets shift data into the future by 5 seconds. (Note that time column is in nanoseconds). output_df [ \"time\" ] = df [ \"time\" ] . apply ( lambda x : int ( x ) + int (( 5 * 1000 * 1000 * 1000 ))) output_df [ \"brake-prediction\" ] = self . model . predict ( X ) print ( \"Prediction\" ) print ( output_df [ \"brake-prediction\" ]) # Merge the original brake value into the output data frame output_df = pd . concat ([ df [[ \"time\" , \"Brake\" ]], output_df ]) . sort_values ( \"time\" , ascending = True ) self . output_stream . parameters . buffer . write ( output_df ) # Send filtered data to output topic Update requirements Click on the requirements.txt file and add sklearn on a new line Success You have edited the code to load and run the model. Run the code The fastest way to run the code is to click Run in the top right hand corner. This will install any dependencies into a sandboxed environment and then run the code. In the output console you will see the result of the prediction. In the next few steps you will deploy the code and then see a visualization of the output. Deploy Click Stop if you haven't already done so. To deploy the code, click Deploy. On the dialog that appears click Deploy. Once the code has been built, deployed it will be started automatically. Success Your code is now running in a fully production ready ecosystem. Visualize whats happening To see the output of your model in real time you will use the Data explorer. Click the Data explorer button on the left hand menu. If it's not already selected click the Live data tab at the top Ensure the brake-prediciton topic is selected Select a stream (you should only have one) Select brake-prediction and brake from the parameters list Success You should now see a graphical output for the prediction being output by the model as well as the actual brake value Note Don't forget this exercise was to deploy an ML model in the Quix platform. We didn't promise to train a good model. So the prediciton may not always match the actual brake value.","title":"Run ML model in realtime environment"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#run-ml-model-in-realtime-environment","text":"In this article, you will learn how to use pickle file trained on historic data in a realtime environment. Ensure you have completed the previous stage first, if not find it here .","title":"Run ML model in realtime environment"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#why-this-is-important","text":"With the Quix platform, you can run and deploy ML models to the leading edge reacting to data coming from the source with milliseconds latency.","title":"Why this is important"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#end-result","text":"At the end of this article, we will end up with a live model using the pickle file from How to train ML model to process live data on the edge.","title":"End result"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#preparation","text":"You\u2019ll need to complete the How to train ML model article to get pickle file with trained model logic.","title":"Preparation"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#run-the-model","text":"Now let's run the model you created in the previous article. If you have your own model and already know how to run the Python to execute it then these steps might also be useful for you. Ensure you are logged into the Quix Portal Navigate to the Library Filter the library by selecting Python under languages and Transformation under pipeline stage Select the Data Filtering Model Tip If you can't see Data Filtering Model you can also use search to find it Info Usually, after clicking on the Data Filtering Model you can look at the code and the readme to ensure it's the correct sample for your needs. Now click Edit code Change the name to \"Prediction Model\" Ensure the input is \"f1-data\" Ensure the output is \"brake-prediction\" Info The platform will automatically create any topics that don't already exist Success The code from the Library sample is now saved to your workspace. You can edit and run the code from here or clone it to your computer and work locally. See more about setting up your local environment here .","title":"Run the model"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#upload-the-model","text":"Now you need to upload the ML model created in the previous article and edit this code to run the model. Click the upload file icon at the top of the file list Find the file saved in the previous article. Hint It's called 'decision_tree_5_depth.sav' and should be in \"C:\\Users[USER]\\\" on Windows Warning When you click off the file e.g. onto quix_function.py, the editor might prompt you to save the .sav file. Click \"Do not commit\" Click quix_function.py in the file list (remember do not commit changes to the model file)","title":"Upload the model"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#modify-the-code","text":"Add the following statements to import the required libraries import pickle import math In the __init__ function add the following lines to load the model ## Import ML model from file self . model = pickle . load ( open ( 'decision_tree_5_depth.sav' , 'rb' )) Under the __init__ function add the following new function This will pre-process the data, a necessary step before passing it to the model. ## To get the correct output, we preprocess data before we feed them to the trained model def preprocess ( self , df ): signal_limits = { \"Speed\" : ( 0 , 400 ), \"Steer\" : ( - 1 , 1 ), \"Gear\" : ( 0 , 8 ), \"Motion_WorldPositionX\" : ( - math . pi , math . pi ), \"Brake\" : ( 0 , 1 ), } def clamp ( n , minn , maxn ): return max ( min ( maxn , n ), minn ) for signal , limits in signal_limits . items (): df [ signal ] = df [ signal ] . map ( lambda x : clamp ( x , limits [ 0 ], limits [ 1 ])) df [ \"Motion_WorldPositionX_sin\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . sin ( x )) df [ \"Motion_WorldPositionX_cos\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . cos ( x )) return df Delete the on_pandas_frame_handler function and paste this code in it's place. # Callback triggered for each new parameter data. def on_pandas_frame_handler ( self , df : pd . DataFrame ): # if no speed column, skip this record if not \"Speed\" in df . columns : return df output_df = pd . DataFrame () # Preprocessing df = self . preprocess ( df ) features = [ \"Motion_WorldPositionX_cos\" , \"Motion_WorldPositionX_sin\" , \"Steer\" , \"Speed\" , \"Gear\" ] X = df [ features ] # Lets shift data into the future by 5 seconds. (Note that time column is in nanoseconds). output_df [ \"time\" ] = df [ \"time\" ] . apply ( lambda x : int ( x ) + int (( 5 * 1000 * 1000 * 1000 ))) output_df [ \"brake-prediction\" ] = self . model . predict ( X ) print ( \"Prediction\" ) print ( output_df [ \"brake-prediction\" ]) # Merge the original brake value into the output data frame output_df = pd . concat ([ df [[ \"time\" , \"Brake\" ]], output_df ]) . sort_values ( \"time\" , ascending = True ) self . output_stream . parameters . buffer . write ( output_df ) # Send filtered data to output topic","title":"Modify the code"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#update-requirements","text":"Click on the requirements.txt file and add sklearn on a new line Success You have edited the code to load and run the model.","title":"Update requirements"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#run-the-code","text":"The fastest way to run the code is to click Run in the top right hand corner. This will install any dependencies into a sandboxed environment and then run the code. In the output console you will see the result of the prediction. In the next few steps you will deploy the code and then see a visualization of the output.","title":"Run the code"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#deploy","text":"Click Stop if you haven't already done so. To deploy the code, click Deploy. On the dialog that appears click Deploy. Once the code has been built, deployed it will be started automatically. Success Your code is now running in a fully production ready ecosystem.","title":"Deploy"},{"location":"platform/how-to/train-and-deploy-ml/deploy-ml/#visualize-whats-happening","text":"To see the output of your model in real time you will use the Data explorer. Click the Data explorer button on the left hand menu. If it's not already selected click the Live data tab at the top Ensure the brake-prediciton topic is selected Select a stream (you should only have one) Select brake-prediction and brake from the parameters list Success You should now see a graphical output for the prediction being output by the model as well as the actual brake value Note Don't forget this exercise was to deploy an ML model in the Quix platform. We didn't promise to train a good model. So the prediciton may not always match the actual brake value.","title":"Visualize whats happening"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/","text":"How to train an ML model In this article, you will learn how to manage ML model training with Quix. In this example, we will train a model to predict car braking on a racing circuit 5 seconds ahead of time. Why this is important With the Quix platform, you can leverage historic data to train your model to react to data coming from source with milliseconds latency. End result At the end of this article, we will end up with a pickle file trained on historic data. Preparation You will need Python3 installed. You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix Tip If in doubt, login to the Quix Portal, navigate to the Library and deploy \"Demo Data - Source\". This will provide you with some real-time data for your experiments. You\u2019ll also need a Jupyter notebook environment to run your experiments and load data for training. Please use \"How to work with Jupyter notebook\" . Install required libraries python3 -m pip install seaborn python3 -m pip install sklearn python3 -m pip install mlflow python3 -m pip install matplotlib Note If you get an 'Access Denied' error installing mlflow try adding '--user' to the install command or run the installer from an Anaconda Powershell Prompt (with --user) Tip If you don\u2019t see Python3 kernel in your Jupyter notebook, execute the following commands in your python environment: python3 - m pip install ipykernel python3 - m ipykernel install -- user Necessary imports To execute all code blocks below, you need to start with importing these libraries. Add this code to the top of you Jupyter notebook. import math import matplotlib.pyplot as plt import mlflow import numpy as np import pandas as pd import pickle import seaborn as sns from sklearn import tree from sklearn.model_selection import KFold from sklearn.metrics import confusion_matrix , accuracy_score from sklearn.tree import DecisionTreeClassifier Training ML model Getting training data The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix. You need to be logged into the platform for this: Select workspace (you likley only have one) Go to the Data Explorer Add a query to visualize some data. Select parameters, events, aggregation and time range Note Select Brake , Motion_WorldPositionX , Steer , Speed , Gear parameters and turn off aggregation! Select the Code tab Ensure Python is the selected language Copy the Python code to your Jupyter notebook and execute. Tip If you want to use this generated code for a long time, replace the temporary token with a PAT token . See authenticate your requests for how to do that. Preprocessing of features We will prepare data for training by applying some transformation on the downloaded data. Execute this in your notebook: ## Convert motion to continuous values df [ \"Motion_WorldPositionX_sin\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . sin ( x )) df [ \"Motion_WorldPositionX_cos\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . cos ( x )) Preprocessing of label Here we simplify braking to a boolean value. ## Conversion of label df [ \"Brake_bool\" ] = df [ \"Brake\" ] . map ( lambda x : round ( x )) Generate advanced brake signal for training Now we need to shift breaking 5 seconds ahead to train the model to predict breaking 5 seconds ahead. ## Offset dataset and trim it NUM_PERIODS = - round ( 5e9 / 53852065.77281786 ) df [ \"Brake_shifted_5s\" ] = df [ \"Brake_bool\" ] . shift ( periods = NUM_PERIODS ) df = df . dropna ( axis = 'rows' ) Lets review it in plot: plt . figure ( figsize = ( 15 , 8 )) plt . plot ( df [ \"Brake_shifted_5s\" ]) plt . plot ( df [ \"Brake_bool\" ]) plt . legend ([ 'Shifted' , 'Unshifted' ]) Fit, predict and score a model Calculate class weighting in case we gain any accuracy by performing class balancing. Y = df [ \"Brake_shifted_5s\" ] cw = {} for val in set ( Y ): cw [ val ] = np . sum ( Y != val ) print ( cw ) Experiment In the following code snippet we are executing an experiment using MLflow . Notice in last 3 lines that each experiment is logging MLflow metrics for experiments comparison later. model_accuracy = pd . DataFrame ( columns = [ 'Baseline Training Accuracy' , 'Model Training Accuracy' , 'Baseline Testing Accuracy' , 'Model Testing Accuracy' , ]) kfold = KFold ( 5 , shuffle = True , random_state = 1 ) with mlflow . start_run (): class_weight = None max_depth = 5 features = [ \"Motion_WorldPositionX_cos\" , \"Motion_WorldPositionX_sin\" , \"Steer\" , \"Speed\" , \"Gear\" ] mlflow . log_param ( \"class_weight\" , class_weight ) mlflow . log_param ( \"max_depth\" , max_depth ) mlflow . log_param ( \"features\" , features ) mlflow . log_param ( \"model_type\" , \"DecisionTreeClassifier\" ) X = df [ features ] decision_tree = DecisionTreeClassifier ( class_weight = class_weight , max_depth = max_depth ) for train , test in kfold . split ( X ): X_train = X . iloc [ train ] Y_train = Y . iloc [ train ] X_test = X . iloc [ test ] Y_test = Y . iloc [ test ] # Train model decision_tree . fit ( X_train , Y_train ) Y_pred = decision_tree . predict ( X_test ) # Assess accuracy train_accuracy = round ( decision_tree . score ( X_train , Y_train ) * 100 , 2 ) test_accuracy = round ( decision_tree . score ( X_test , Y_test ) * 100 , 2 ) Y_baseline_zeros = np . zeros ( Y_train . shape ) baseline_train_accuracy = round ( accuracy_score ( Y_train , Y_baseline_zeros ) * 100 , 2 ) Y_baseline_zeros = np . zeros ( Y_test . shape ) baseline_test_accuracy = round ( accuracy_score ( Y_test , Y_baseline_zeros ) * 100 , 2 ) model_accuracy = model_accuracy . append ({ \"Baseline Training Accuracy\" : baseline_train_accuracy , \"Model Training Accuracy\" : train_accuracy , \"Baseline Testing Accuracy\" : baseline_test_accuracy , \"Model Testing Accuracy\" : test_accuracy }, ignore_index = True ) mlflow . log_metric ( \"train_accuracy\" , model_accuracy [ \"Model Training Accuracy\" ] . mean ()) mlflow . log_metric ( \"test_accuracy\" , model_accuracy [ \"Model Testing Accuracy\" ] . mean ()) mlflow . log_metric ( \"fit_quality\" , 1 / abs ( model_accuracy [ \"Model Training Accuracy\" ] . mean () - model_accuracy [ \"Model Testing Accuracy\" ] . mean ())) We review experiment model accuracy: model_accuracy Depth Baseline Training Accuracy Model Training Accuracy Baseline Testing Accuracy Model Testing Accuracy 0 88.97 97.93 86.49 86.49 1 87.59 97.24 91.89 83.78 2 89.04 96.58 86.11 88.89 3 88.36 97.95 88.89 83.33 4 88.36 97.95 88.89 80.56 Table with model accuracy preview Prediction preview Let\u2019s plot actual vs predicted braking using a trained model: f , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , sharey = True , figsize = ( 50 , 8 )) ax1 . plot ( Y ) ax1 . plot ( X [ \"Speed\" ] / X [ \"Speed\" ] . max ()) ax2 . plot ( decision_tree . predict ( X )) ax2 . plot ( X [ \"Speed\" ] / X [ \"Speed\" ] . max ()) Saving model When you are confident with the results, save the model into a file. pickle . dump ( decision_tree , open ( './decision_tree_5_depth.sav' , 'wb' )) Tip Pickle file will be located in folder where jupyter notebook command was executed MLflow To help you with experiments management, you can review experiments in MLflow. Warning MLflow works only on MacOS, Linux or Windows linux subsystem (WSL). Tip To have some meaningful data, run the experiment with 3 different max_depth parameter. Let\u2019s leave Jupyter notebook for now and go back to command line and run MLflow server: mlflow ui Select experiments to compare: Plot metrics from experiments:","title":"How to train an ML model"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#how-to-train-an-ml-model","text":"In this article, you will learn how to manage ML model training with Quix. In this example, we will train a model to predict car braking on a racing circuit 5 seconds ahead of time.","title":"How to train an ML model"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#why-this-is-important","text":"With the Quix platform, you can leverage historic data to train your model to react to data coming from source with milliseconds latency.","title":"Why this is important"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#end-result","text":"At the end of this article, we will end up with a pickle file trained on historic data.","title":"End result"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#preparation","text":"You will need Python3 installed. You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix Tip If in doubt, login to the Quix Portal, navigate to the Library and deploy \"Demo Data - Source\". This will provide you with some real-time data for your experiments. You\u2019ll also need a Jupyter notebook environment to run your experiments and load data for training. Please use \"How to work with Jupyter notebook\" .","title":"Preparation"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#install-required-libraries","text":"python3 -m pip install seaborn python3 -m pip install sklearn python3 -m pip install mlflow python3 -m pip install matplotlib Note If you get an 'Access Denied' error installing mlflow try adding '--user' to the install command or run the installer from an Anaconda Powershell Prompt (with --user) Tip If you don\u2019t see Python3 kernel in your Jupyter notebook, execute the following commands in your python environment: python3 - m pip install ipykernel python3 - m ipykernel install -- user","title":"Install required libraries"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#necessary-imports","text":"To execute all code blocks below, you need to start with importing these libraries. Add this code to the top of you Jupyter notebook. import math import matplotlib.pyplot as plt import mlflow import numpy as np import pandas as pd import pickle import seaborn as sns from sklearn import tree from sklearn.model_selection import KFold from sklearn.metrics import confusion_matrix , accuracy_score from sklearn.tree import DecisionTreeClassifier","title":"Necessary imports"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#training-ml-model","text":"","title":"Training ML model"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#getting-training-data","text":"The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix. You need to be logged into the platform for this: Select workspace (you likley only have one) Go to the Data Explorer Add a query to visualize some data. Select parameters, events, aggregation and time range Note Select Brake , Motion_WorldPositionX , Steer , Speed , Gear parameters and turn off aggregation! Select the Code tab Ensure Python is the selected language Copy the Python code to your Jupyter notebook and execute. Tip If you want to use this generated code for a long time, replace the temporary token with a PAT token . See authenticate your requests for how to do that.","title":"Getting training data"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#preprocessing-of-features","text":"We will prepare data for training by applying some transformation on the downloaded data. Execute this in your notebook: ## Convert motion to continuous values df [ \"Motion_WorldPositionX_sin\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . sin ( x )) df [ \"Motion_WorldPositionX_cos\" ] = df [ \"Motion_WorldPositionX\" ] . map ( lambda x : math . cos ( x ))","title":"Preprocessing of features"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#preprocessing-of-label","text":"Here we simplify braking to a boolean value. ## Conversion of label df [ \"Brake_bool\" ] = df [ \"Brake\" ] . map ( lambda x : round ( x ))","title":"Preprocessing of label"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#generate-advanced-brake-signal-for-training","text":"Now we need to shift breaking 5 seconds ahead to train the model to predict breaking 5 seconds ahead. ## Offset dataset and trim it NUM_PERIODS = - round ( 5e9 / 53852065.77281786 ) df [ \"Brake_shifted_5s\" ] = df [ \"Brake_bool\" ] . shift ( periods = NUM_PERIODS ) df = df . dropna ( axis = 'rows' ) Lets review it in plot: plt . figure ( figsize = ( 15 , 8 )) plt . plot ( df [ \"Brake_shifted_5s\" ]) plt . plot ( df [ \"Brake_bool\" ]) plt . legend ([ 'Shifted' , 'Unshifted' ])","title":"Generate advanced brake signal for training"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#fit-predict-and-score-a-model","text":"Calculate class weighting in case we gain any accuracy by performing class balancing. Y = df [ \"Brake_shifted_5s\" ] cw = {} for val in set ( Y ): cw [ val ] = np . sum ( Y != val ) print ( cw )","title":"Fit, predict and score a model"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#experiment","text":"In the following code snippet we are executing an experiment using MLflow . Notice in last 3 lines that each experiment is logging MLflow metrics for experiments comparison later. model_accuracy = pd . DataFrame ( columns = [ 'Baseline Training Accuracy' , 'Model Training Accuracy' , 'Baseline Testing Accuracy' , 'Model Testing Accuracy' , ]) kfold = KFold ( 5 , shuffle = True , random_state = 1 ) with mlflow . start_run (): class_weight = None max_depth = 5 features = [ \"Motion_WorldPositionX_cos\" , \"Motion_WorldPositionX_sin\" , \"Steer\" , \"Speed\" , \"Gear\" ] mlflow . log_param ( \"class_weight\" , class_weight ) mlflow . log_param ( \"max_depth\" , max_depth ) mlflow . log_param ( \"features\" , features ) mlflow . log_param ( \"model_type\" , \"DecisionTreeClassifier\" ) X = df [ features ] decision_tree = DecisionTreeClassifier ( class_weight = class_weight , max_depth = max_depth ) for train , test in kfold . split ( X ): X_train = X . iloc [ train ] Y_train = Y . iloc [ train ] X_test = X . iloc [ test ] Y_test = Y . iloc [ test ] # Train model decision_tree . fit ( X_train , Y_train ) Y_pred = decision_tree . predict ( X_test ) # Assess accuracy train_accuracy = round ( decision_tree . score ( X_train , Y_train ) * 100 , 2 ) test_accuracy = round ( decision_tree . score ( X_test , Y_test ) * 100 , 2 ) Y_baseline_zeros = np . zeros ( Y_train . shape ) baseline_train_accuracy = round ( accuracy_score ( Y_train , Y_baseline_zeros ) * 100 , 2 ) Y_baseline_zeros = np . zeros ( Y_test . shape ) baseline_test_accuracy = round ( accuracy_score ( Y_test , Y_baseline_zeros ) * 100 , 2 ) model_accuracy = model_accuracy . append ({ \"Baseline Training Accuracy\" : baseline_train_accuracy , \"Model Training Accuracy\" : train_accuracy , \"Baseline Testing Accuracy\" : baseline_test_accuracy , \"Model Testing Accuracy\" : test_accuracy }, ignore_index = True ) mlflow . log_metric ( \"train_accuracy\" , model_accuracy [ \"Model Training Accuracy\" ] . mean ()) mlflow . log_metric ( \"test_accuracy\" , model_accuracy [ \"Model Testing Accuracy\" ] . mean ()) mlflow . log_metric ( \"fit_quality\" , 1 / abs ( model_accuracy [ \"Model Training Accuracy\" ] . mean () - model_accuracy [ \"Model Testing Accuracy\" ] . mean ())) We review experiment model accuracy: model_accuracy Depth Baseline Training Accuracy Model Training Accuracy Baseline Testing Accuracy Model Testing Accuracy 0 88.97 97.93 86.49 86.49 1 87.59 97.24 91.89 83.78 2 89.04 96.58 86.11 88.89 3 88.36 97.95 88.89 83.33 4 88.36 97.95 88.89 80.56 Table with model accuracy preview","title":"Experiment"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#prediction-preview","text":"Let\u2019s plot actual vs predicted braking using a trained model: f , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , sharey = True , figsize = ( 50 , 8 )) ax1 . plot ( Y ) ax1 . plot ( X [ \"Speed\" ] / X [ \"Speed\" ] . max ()) ax2 . plot ( decision_tree . predict ( X )) ax2 . plot ( X [ \"Speed\" ] / X [ \"Speed\" ] . max ())","title":"Prediction preview"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#saving-model","text":"When you are confident with the results, save the model into a file. pickle . dump ( decision_tree , open ( './decision_tree_5_depth.sav' , 'wb' )) Tip Pickle file will be located in folder where jupyter notebook command was executed","title":"Saving model"},{"location":"platform/how-to/train-and-deploy-ml/train-ml-model/#mlflow","text":"To help you with experiments management, you can review experiments in MLflow. Warning MLflow works only on MacOS, Linux or Windows linux subsystem (WSL). Tip To have some meaningful data, run the experiment with 3 different max_depth parameter. Let\u2019s leave Jupyter notebook for now and go back to command line and run MLflow server: mlflow ui Select experiments to compare: Plot metrics from experiments:","title":"MLflow"},{"location":"platform/how-to/webapps/read/","text":"Read from Quix with NodeJs Quix supports real-time data streaming over WebSockets. JavaScript clients can receive updates on parameter and event definition updates, parameter data and event data as they happen. Following examples use SignalR client library to connect to Quix over WebSockets. Setting up SignalR If you are using a package manager like npm, you can install SignalR using npm install @microsoft/signalr . For other installation options that don\u2019t depend on a platform like Node.js such as consuming SignalR from a CDN please refer to SignalR documentation . Following code snippet shows how you can connect to Quix after SignalR has been setup. var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'your_access_token' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-your-workspace-id.portal.quix.ai/hub\" , options ) . build (); connection . start (). then (() => console . log ( \"SignalR connected.\" )); If the connection is successful, you should see the console log \"SignalR connected\". Reading data from a stream Before you can read data from a stream, you need to subscribe to an event like parameter definition, event definition, parameter data or event data. Following is an example of establishing a connection to Quix, subscribing to a parameter data stream, reading data from that stream, and unsubscribing from the event using a SignalR client. var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'your_access_token' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-your-workspace-id.portal.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then (() => { console . log ( \"Connected to Quix.\" ); // Subscribe to parameter data stream. connection . invoke ( \"SubscribeToParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); // Read data from the stream. connection . on ( \"ParameterDataReceived\" , data => { let model = JSON . stringify ( data ); console . log ( \"Received data from stream: \" + model ); // Unsubscribe from stream. connection . invoke ( \"UnsubscribeFromParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); }); }); Following is a list of subscriptions available for SignalR clients. SubscribeToParameter(topicName, streamId, parameterId) : Subscribes to a parameter data stream. Use UnsubscribeFromParameter(topicname, streamId, parameterId) to unsubscribe. SubscribeToParameterDefinitions(topicName, streamId) : Subscribes to parameter definition updates. SubscribeToEvent(topicName, streamId, eventId) : Subscribes to an event data stream. Use UnsubscribeFromEvent(topicName, streamId, eventId) to unsubscribe. SubscribeToEventDefinitions(topicName, streamId) : Subscribes to event definition updates. UnsubscribeFromStream(topicName, streamId) : Unsubscribes from all subscriptions of the specified stream. Following is a list of SignalR events supported by Quix and their payloads. ParameterDataReceived : Add a listener to this event to receive parameter data from a stream. Following is a sample payload for this event. { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' , timestamps : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], numericValues : { ParameterA : [ 1 , 2 , 3 ] }, stringValues : {}, tagValues : { ParameterA : [ null , null , 'tag-1' ] } } ParameterDefinitionsUpdated : Add a listener to this event to receive data from SubscribeToParameterDefinitions subscription. Following is a sample payload of this event. { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' , definitions : [ { Id : 'ParameterA' , Name : 'Parameter A' , Description : 'Description of parameter A' , MinimumValue : null , MaximumValue : 100.0 , Unit : 'kmh' , CustomProperties : null , Localtion : '/car/general' } ] } EventDataReceived : Add a listener to this event to receive data from SubscribeToEvent subscription. Following is a sample payload of this event. { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' id : 'EventA' , timestamp : 1591733990000000000 , value : 'val-a' , tags : { tag1 : 'val1' } }","title":"Read from Quix with NodeJs"},{"location":"platform/how-to/webapps/read/#read-from-quix-with-nodejs","text":"Quix supports real-time data streaming over WebSockets. JavaScript clients can receive updates on parameter and event definition updates, parameter data and event data as they happen. Following examples use SignalR client library to connect to Quix over WebSockets.","title":"Read from Quix with NodeJs"},{"location":"platform/how-to/webapps/read/#setting-up-signalr","text":"If you are using a package manager like npm, you can install SignalR using npm install @microsoft/signalr . For other installation options that don\u2019t depend on a platform like Node.js such as consuming SignalR from a CDN please refer to SignalR documentation . Following code snippet shows how you can connect to Quix after SignalR has been setup. var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'your_access_token' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-your-workspace-id.portal.quix.ai/hub\" , options ) . build (); connection . start (). then (() => console . log ( \"SignalR connected.\" )); If the connection is successful, you should see the console log \"SignalR connected\".","title":"Setting up SignalR"},{"location":"platform/how-to/webapps/read/#reading-data-from-a-stream","text":"Before you can read data from a stream, you need to subscribe to an event like parameter definition, event definition, parameter data or event data. Following is an example of establishing a connection to Quix, subscribing to a parameter data stream, reading data from that stream, and unsubscribing from the event using a SignalR client. var signalR = require ( \"@microsoft/signalr\" ); const options = { accessTokenFactory : () => 'your_access_token' }; const connection = new signalR . HubConnectionBuilder () . withUrl ( \"https://reader-your-workspace-id.portal.quix.ai/hub\" , options ) . build (); // Establish connection connection . start (). then (() => { console . log ( \"Connected to Quix.\" ); // Subscribe to parameter data stream. connection . invoke ( \"SubscribeToParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); // Read data from the stream. connection . on ( \"ParameterDataReceived\" , data => { let model = JSON . stringify ( data ); console . log ( \"Received data from stream: \" + model ); // Unsubscribe from stream. connection . invoke ( \"UnsubscribeFromParameter\" , \"your-topic-name\" , \"your-stream-id\" , \"your-parameter-id\" ); }); }); Following is a list of subscriptions available for SignalR clients. SubscribeToParameter(topicName, streamId, parameterId) : Subscribes to a parameter data stream. Use UnsubscribeFromParameter(topicname, streamId, parameterId) to unsubscribe. SubscribeToParameterDefinitions(topicName, streamId) : Subscribes to parameter definition updates. SubscribeToEvent(topicName, streamId, eventId) : Subscribes to an event data stream. Use UnsubscribeFromEvent(topicName, streamId, eventId) to unsubscribe. SubscribeToEventDefinitions(topicName, streamId) : Subscribes to event definition updates. UnsubscribeFromStream(topicName, streamId) : Unsubscribes from all subscriptions of the specified stream. Following is a list of SignalR events supported by Quix and their payloads. ParameterDataReceived : Add a listener to this event to receive parameter data from a stream. Following is a sample payload for this event. { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' , timestamps : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], numericValues : { ParameterA : [ 1 , 2 , 3 ] }, stringValues : {}, tagValues : { ParameterA : [ null , null , 'tag-1' ] } } ParameterDefinitionsUpdated : Add a listener to this event to receive data from SubscribeToParameterDefinitions subscription. Following is a sample payload of this event. { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' , definitions : [ { Id : 'ParameterA' , Name : 'Parameter A' , Description : 'Description of parameter A' , MinimumValue : null , MaximumValue : 100.0 , Unit : 'kmh' , CustomProperties : null , Localtion : '/car/general' } ] } EventDataReceived : Add a listener to this event to receive data from SubscribeToEvent subscription. Following is a sample payload of this event. { topicName : 'topic-1' , streamId : 'b45969d2-4624-4ab7-9779-c8f90ce79420' id : 'EventA' , timestamp : 1591733990000000000 , value : 'val-a' , tags : { tag1 : 'val1' } }","title":"Reading data from a stream"},{"location":"platform/how-to/webapps/write/","text":"Write to Quix with NodeJs Clients write data to Quix using streams opened on existing topics . Therefore, you need to first create a topic in the Portal to hold your data streams. Once you have a topic, your clients can start writing data to Quix by creating a stream in your topic sending data to that stream closing the stream Creating a stream To write data to Quix, you need to open a stream to your topic. Following is an example of creating a stream using JavaScript and Node.js. const https = require ( https ); const data = JSON . stringify ({ Name : \"Your Stream Name\" , Location : \"your/location\" , Metadata : { Property1 : \"Value 1\" , Property2 : \"Value 2\" }, Parents = [ \"parent-stream-1\" , \"parent-stream-2\" ], TimeOfRecording : \"2021-02-06T00:15:15Z\" }); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { res . on ( 'data' , d => { let json = JSON . parse ( d ); let streamId = json . streamId ; }); }); req . write ( data ); req . end (); Upon completing the request successfully, you will receive the stream id in the response body. You are going to need this stream id when you are writing data to the stream. In the request data, Location is also an optional, but an important property. Location allows you to organise your streams under directories in the Data Catalogue. When you are creating the stream, you can add optional metadata about the stream to the stream definition like Property1 and Property2 in the preceding example. Field Parents is also optional. If the current stream is derived from one or more streams (e.g. by transforming data from one stream using an analytics model), you can reference the original streams using this field. TimeOfRecording is an optional field that allows you to specify the actual time the data was recorded. This field is useful if you are streaming data that was recorded in the past. Writing parameter data to a stream After you have created the stream, you can start writing data to that stream using the following HTTP request. const https = require ( https ); const data = JSON . stringify ({ Timestamps : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], NumericValues : { \"ParameterA\" : [ 1 , 2 , 3 ], \"ParameterB\" : [ 5 , 8 , 9 ] }, StringValues : { \"ParameterC\" : [ \"hello\" , \"world\" , \"!\" ] }, BinaryValues : { \"ParameterD\" : [ Buffer . from ( \"hello\" ). toString ( 'base64' ), Buffer . from ( \" Quix\" ). toString ( 'base64' ), Buffer . from ( \"!\" ). toString ( 'base64' ) ] } TagValues : { \"ParameterA\" : [ null , null , \"tag-1\" ] } }); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/parameters/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding example, data has two different parameter types, numeric and strings. If your data only contains numeric data, you do not need to include the StringValues property. In the case of binary values, the items in the array must be a base64 encoded string. TagValues is another optional field in the data request that allows you to add context to data points by means of tagging them. Index of the Timestamps array is used when matching the parameter data values as well as tag values. Therefore, the order of the arrays is important. Defining parameters In the above examples, parameters are created in Quix as you write data to the stream. However, what if you would like to add more information like acceptable value ranges, measurement units, etc. to your parameters? You can use the following HTTP request to update your parameter definitions. const https = require ( https ); const data = JSON . stringify ([ { Id : \"ParameterA\" , Name : \"Parameter A\" , Description : \"Temperature measurements from unit 1234A\" , MinimumValue : 0.0 , MaximumValue : 100.0 , Unit : \"\u00b0C\" , CustomProperties : \"{\\\"OptimalMinimum\\\": 30.0, \\\"OptimalMaximum\\\": 50.0}\" , Location : \"/chassis/engine\" } ]); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/parameters' , method : 'PUT' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding request, the Id must match the parameter id you set when writing data to the stream. Name allows you to set a more readable name for the parameter. You can also add a description, minimum and maximum values, unit of measurement to your parameter. Location allows you to organise/group your parameters in a hierarchical manner like with the streams. If you have a custom parameter definition that is not covered by the primary fields of the request, you can use CustomProperties field to add your custom definition as a string. Writing event data to a stream Writing event data to a stream is similar to writing parameter data using the web api. The main difference in the two requests is in the request body. const data = JSON . stringify ([ { Id : \"EventA\" , Timestamp : 1591733989000000000 , Value : \"Lap1\" , Tags : { TagA : \"val1\" , TagB : \"val2\" } }, { Id : \"EventA\" , Timestamp : 1591734989000000000 , Value : \"Lap2\" , Tags : { TagA : \"val1\" , TagB : \"val2\" } }, { Id : \"EventA\" , Timestamp : 1591735989000000000 , Value : \"Lap3\" , Tags : { TagA : \"val1\" } }, ]); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/events/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding example, tags in the event data request are optional. Tags add context to your data points and help you to execute efficient queries over them on your data like using indexes in traditional databases. Defining events In the above examples, events are created in Quix as you write data to the stream. If you want to add more descriptions to your events, you can use event definitions api similar to parameter definitions to update your events. const https = require ( https ); const data = JSON . stringify ([ { Id : \"EventA\" , Name : \"Event A\" , Description : \"New lap event\" , CustomProperties : \"{\\\"Tarmac\\\": \\\"Open-graded\\\"}\" , Location : \"/drive/lap\" , Level : \"Information\" } ]); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/events' , method : 'PUT' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding request, the Id must match the event id you set when writing events to the stream. Name allows you to set a more readable name for the event. Location allows you to organise/group your events in a hierarchy like with the parameters. If you have a custom event definition that is not covered by the primary fields of the request, you can use CustomProperties field to add your custom definition as a string. You can also set an optional event Level . Accepted event levels are Trace, Debug, Information, Warning, Error and Critical. Event level defaults to Information if not specified. Closing a stream After finishing sending data, you can proceed to close the stream using the request below. const https = require ( https ); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/close' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . end ();","title":"Write to Quix with NodeJs"},{"location":"platform/how-to/webapps/write/#write-to-quix-with-nodejs","text":"Clients write data to Quix using streams opened on existing topics . Therefore, you need to first create a topic in the Portal to hold your data streams. Once you have a topic, your clients can start writing data to Quix by creating a stream in your topic sending data to that stream closing the stream","title":"Write to Quix with NodeJs"},{"location":"platform/how-to/webapps/write/#creating-a-stream","text":"To write data to Quix, you need to open a stream to your topic. Following is an example of creating a stream using JavaScript and Node.js. const https = require ( https ); const data = JSON . stringify ({ Name : \"Your Stream Name\" , Location : \"your/location\" , Metadata : { Property1 : \"Value 1\" , Property2 : \"Value 2\" }, Parents = [ \"parent-stream-1\" , \"parent-stream-2\" ], TimeOfRecording : \"2021-02-06T00:15:15Z\" }); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { res . on ( 'data' , d => { let json = JSON . parse ( d ); let streamId = json . streamId ; }); }); req . write ( data ); req . end (); Upon completing the request successfully, you will receive the stream id in the response body. You are going to need this stream id when you are writing data to the stream. In the request data, Location is also an optional, but an important property. Location allows you to organise your streams under directories in the Data Catalogue. When you are creating the stream, you can add optional metadata about the stream to the stream definition like Property1 and Property2 in the preceding example. Field Parents is also optional. If the current stream is derived from one or more streams (e.g. by transforming data from one stream using an analytics model), you can reference the original streams using this field. TimeOfRecording is an optional field that allows you to specify the actual time the data was recorded. This field is useful if you are streaming data that was recorded in the past.","title":"Creating a stream"},{"location":"platform/how-to/webapps/write/#writing-parameter-data-to-a-stream","text":"After you have created the stream, you can start writing data to that stream using the following HTTP request. const https = require ( https ); const data = JSON . stringify ({ Timestamps : [ 1591733989000000000 , 1591733990000000000 , 1591733991000000000 ], NumericValues : { \"ParameterA\" : [ 1 , 2 , 3 ], \"ParameterB\" : [ 5 , 8 , 9 ] }, StringValues : { \"ParameterC\" : [ \"hello\" , \"world\" , \"!\" ] }, BinaryValues : { \"ParameterD\" : [ Buffer . from ( \"hello\" ). toString ( 'base64' ), Buffer . from ( \" Quix\" ). toString ( 'base64' ), Buffer . from ( \"!\" ). toString ( 'base64' ) ] } TagValues : { \"ParameterA\" : [ null , null , \"tag-1\" ] } }); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/parameters/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding example, data has two different parameter types, numeric and strings. If your data only contains numeric data, you do not need to include the StringValues property. In the case of binary values, the items in the array must be a base64 encoded string. TagValues is another optional field in the data request that allows you to add context to data points by means of tagging them. Index of the Timestamps array is used when matching the parameter data values as well as tag values. Therefore, the order of the arrays is important.","title":"Writing parameter data to a stream"},{"location":"platform/how-to/webapps/write/#defining-parameters","text":"In the above examples, parameters are created in Quix as you write data to the stream. However, what if you would like to add more information like acceptable value ranges, measurement units, etc. to your parameters? You can use the following HTTP request to update your parameter definitions. const https = require ( https ); const data = JSON . stringify ([ { Id : \"ParameterA\" , Name : \"Parameter A\" , Description : \"Temperature measurements from unit 1234A\" , MinimumValue : 0.0 , MaximumValue : 100.0 , Unit : \"\u00b0C\" , CustomProperties : \"{\\\"OptimalMinimum\\\": 30.0, \\\"OptimalMaximum\\\": 50.0}\" , Location : \"/chassis/engine\" } ]); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/parameters' , method : 'PUT' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding request, the Id must match the parameter id you set when writing data to the stream. Name allows you to set a more readable name for the parameter. You can also add a description, minimum and maximum values, unit of measurement to your parameter. Location allows you to organise/group your parameters in a hierarchical manner like with the streams. If you have a custom parameter definition that is not covered by the primary fields of the request, you can use CustomProperties field to add your custom definition as a string.","title":"Defining parameters"},{"location":"platform/how-to/webapps/write/#writing-event-data-to-a-stream","text":"Writing event data to a stream is similar to writing parameter data using the web api. The main difference in the two requests is in the request body. const data = JSON . stringify ([ { Id : \"EventA\" , Timestamp : 1591733989000000000 , Value : \"Lap1\" , Tags : { TagA : \"val1\" , TagB : \"val2\" } }, { Id : \"EventA\" , Timestamp : 1591734989000000000 , Value : \"Lap2\" , Tags : { TagA : \"val1\" , TagB : \"val2\" } }, { Id : \"EventA\" , Timestamp : 1591735989000000000 , Value : \"Lap3\" , Tags : { TagA : \"val1\" } }, ]); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/events/data' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding example, tags in the event data request are optional. Tags add context to your data points and help you to execute efficient queries over them on your data like using indexes in traditional databases.","title":"Writing event data to a stream"},{"location":"platform/how-to/webapps/write/#defining-events","text":"In the above examples, events are created in Quix as you write data to the stream. If you want to add more descriptions to your events, you can use event definitions api similar to parameter definitions to update your events. const https = require ( https ); const data = JSON . stringify ([ { Id : \"EventA\" , Name : \"Event A\" , Description : \"New lap event\" , CustomProperties : \"{\\\"Tarmac\\\": \\\"Open-graded\\\"}\" , Location : \"/drive/lap\" , Level : \"Information\" } ]); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/events' , method : 'PUT' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . write ( data ); req . end (); In the preceding request, the Id must match the event id you set when writing events to the stream. Name allows you to set a more readable name for the event. Location allows you to organise/group your events in a hierarchy like with the parameters. If you have a custom event definition that is not covered by the primary fields of the request, you can use CustomProperties field to add your custom definition as a string. You can also set an optional event Level . Accepted event levels are Trace, Debug, Information, Warning, Error and Critical. Event level defaults to Information if not specified.","title":"Defining events"},{"location":"platform/how-to/webapps/write/#closing-a-stream","text":"After finishing sending data, you can proceed to close the stream using the request below. const https = require ( https ); const options = { hostname : 'your-workspace-id.portal.quix.ai' , path : '/topics/your-topic-name/streams/your-stream-id/close' , method : 'POST' , headers : { 'Authorization' : 'Bearer your_access_token' , 'Content-Type' : 'application/json' } }; const req = https . request ( options , res => { console . log ( `Status Code: ${ res . statusCode } ` ); }); req . end ();","title":"Closing a stream"},{"location":"platform/samples/samples/","text":"Code Samples The Quix Portal includes a Library of templates and sample projects that you can use to start working with the platform. Quix allows explore the samples and save them as a new Project and immidiatelly Run or Deploy them. If you don\u2019t have a Quix account yet, go sign-up to Quix and create one. The backend of the Quix Library is handled by a public Open source repository on GitHub, so you can become a contributor of our Library generating new samples or updating existing ones.","title":"Code Samples"},{"location":"platform/samples/samples/#code-samples","text":"The Quix Portal includes a Library of templates and sample projects that you can use to start working with the platform. Quix allows explore the samples and save them as a new Project and immidiatelly Run or Deploy them. If you don\u2019t have a Quix account yet, go sign-up to Quix and create one. The backend of the Quix Library is handled by a public Open source repository on GitHub, so you can become a contributor of our Library generating new samples or updating existing ones.","title":"Code Samples"},{"location":"platform/security/security/","text":"Security This section describes the basic security features of Quix. Data in flight Authentication Our APIs are authenticated using OAuth 2.0 token. We are using Auth0 as our provider. Each Kafka server is authenticated using certificate, which is provided for each project created and can also be downloaded from topics view. The client is authenticated using SASL (username, password). Authorization The APIs is using RBAC. You are limited in what you can do based on your token and the role configured for your user. Each kafka client is authrozied to only read and write to the topics or query consumer group information regarding topics owned by the organisation the client belongs to. Encryption All our APIs communicate with TLS 1.2 Data at rest Your data is encrypted at rest using cloud provider (Azure) managed keys. Your data is phyisically protected at our cloud provider\u2019s location.","title":"Security"},{"location":"platform/security/security/#security","text":"This section describes the basic security features of Quix.","title":"Security"},{"location":"platform/security/security/#data-in-flight","text":"","title":"Data in flight"},{"location":"platform/security/security/#authentication","text":"Our APIs are authenticated using OAuth 2.0 token. We are using Auth0 as our provider. Each Kafka server is authenticated using certificate, which is provided for each project created and can also be downloaded from topics view. The client is authenticated using SASL (username, password).","title":"Authentication"},{"location":"platform/security/security/#authorization","text":"The APIs is using RBAC. You are limited in what you can do based on your token and the role configured for your user. Each kafka client is authrozied to only read and write to the topics or query consumer group information regarding topics owned by the organisation the client belongs to.","title":"Authorization"},{"location":"platform/security/security/#encryption","text":"All our APIs communicate with TLS 1.2","title":"Encryption"},{"location":"platform/security/security/#data-at-rest","text":"Your data is encrypted at rest using cloud provider (Azure) managed keys. Your data is phyisically protected at our cloud provider\u2019s location.","title":"Data at rest"},{"location":"platform/troubleshooting/troubleshooting/","text":"Troubleshooting This section contains solutions, fixes, hints and tips to help you solve the most common issues encountered when using Quix. Data is not being received into a Topic Ensure the Topic Name or Id is correct in Topics option of Quix Portal. You can check the data in / out rates on the Topics tab. If you want to see the data in the Data Catalogue please make sure you are persisting the data to the Topic otherwise it may appear that there is no data. If you are using a consumer group, check that no other services are using the same group. If you run your code locally and deployed somewhere and they are both using the same consumer group one of them may consume all of the data. Topic Authentication Error If you see errors like these in your service or job logs then you may have used the wrong credentials or it could be that you have specified the wrong Topic Id. Authentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-256 Exception receiving package from Kafka 3/3 brokers are down Broker: Topic authorization failed Check very carefully each of the details. The following must be correct: TopicId or TopicName Sdk Token These can all be found in Topics option of Quix Portal. Broker Transport Failure If you have deployed a service or job and the logs mention broker transport failure then check the workspace name and password in the SecurityOptions. Also check the broker address list. You should have these by default: kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093 401 Error When attempting to access the web API\u2019s you may encounter a 401 error. Check that the bearer token is correct and has not expired. If necessary generate a new bearer token. Example of the error received when trying to connect to the Streaming Reader API with an expired bearer token signalrcore.hub.errors.UnAuthorizedHubError The API\u2019s that require a valid bearer token are: Portal API https://portal-api.platform.quix.ai/swagger/index.html Streaming Writer API https://writer-[YOUR_ORGANISATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/index.html Telemetry Query API https://telemetry-query-[YOUR_ORGANISATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/swagger/index.html Error Handling in the SDK callbacks Errors generated in the SDK callback can be swallowed or hard to read. To prevent this and make it easier to determine the root cause you should use a traceback Begin by importing traceback import traceback Then, inside the SDK callback where you might have an issue place code similar to this: def read_stream ( new_stream : StreamReader ): def on_parameter_data_handler ( data : ParameterData ): try : data . timestamps [ 19191919 ] # this does not exist except Exception : print ( traceback . format_exc ()) new_stream . parameters . create_buffer () . on_read += on_parameter_data_handler input_topic . on_stream_received += read_stream Notice that the try clause is within the handler and the except clause prints a formatted exception (below) Traceback ( most recent call last ): File \"main.py\" , line 20 , in on_parameter_data_handler data . timestamps [ 19191919 ] File \"/usr/local/lib/python3.8/dist-packages/quixstreaming/models/netlist.py\" , line 22 , in __getitem__ item = self . __wrapped [ key ] IndexError : list index out of range Service keeps failing and restarting If your service continually fails and restarts you will not be able to view the logs. Redeploy your service as a job instead. This will allow you to inspect the logs and get a better idea about what is happening. Possible DNS Propagation Errors There are currently 2 scenarios in which you might encounter an issue caused by DNS propagation. 1. Data catalogue has been deployed but DNS entries have not fully propagated. In this scenario you might see a banner when accessing the data catalogue. 2. A dashboard or other publicly visible deployment is not yet accessible, again due to DNS propagation. Tip In these scenarios simply wait while the DNS records propagate. It can take up to 10 minutes for DNS to records to propagate fully. Python Version If you get strange errors when trying to compile your Python code locally please check that you are using Python version 3.8 For example you may encounter a ModuleNotFoundError ModuleNotFoundError : No module named 'quixstreaming' For information on how to setup your IDE for working with Quix please check out this section on the SDK documentation. Jupyter Notebooks If you are having trouble with Jupyter Notebooks or another consumer of Quix data try using aggregation to reduce the number of records returned. For more info on aggregation check out this short video . Process Killed or Out of memory If your deployment\u2019s logs report \"Killed\" or \"Out of memory\" then you may need to increase the amount of memory assgned to the deployment. You may experience this: At build time if you want to load large third party packages into your code At runtime if you are storing large datasets in memory. Missing Dependency in online IDE Currently the online IDE does not use the same docker image as the one used for deployment due to time it would take to build it and make it available to you. (Likely feature for future however) Because of this you might have some OS level dependencies that you need to install from within your python code to be able to make use of the Run feature in the IDE. The section below should give you guidance how to achieve this. In your main.py (or similar) file, add as the first line: import preinstall . Now create the file preinstall.py and add content based on example below: TA-Lib This script will check if TA-Lib is already installed (like from docker deployment). If not then installs it. import os import sys ta_lib_pip_details = os . system ( \"python3 -m pip show TA-Lib\" ) if ta_lib_pip_details == 0 : print ( \"TA-Lib already installed\" ) else : if os . system ( \"apt-get update\" ) != 0 : print ( \"Failed apt-get update\" ) sys . exit ( 1 ) if os . popen ( \"if [ -e ta-lib-0.4.0-src.tar.gz ]; then echo \\\" ok \\\" ; else echo \\\" nok \\\" ; fi\" ) . read () . strip () == \"ok\" : print ( \"TA-Lib already downloaded\" ) else : print ( \"Downloading ta-lib\" ) if os . system ( \"apt-get install curl -y\" ) != 0 : print ( \"Failed apt-get install curl -y\" ) sys . exit ( 1 ) if os . system ( \"curl https://jztkft.dl.sourceforge.net/project/ta-lib/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz -O\" ) != 0 : print ( \"Failed to download ta-lib\" ) sys . exit ( 1 ) zipmdsum = os . popen ( \"md5sum ta-lib-0.4.0-src.tar.gz | cut -d ' ' -f 1\" ) . read () . strip () if zipmdsum == \"308e53b9644213fc29262f36b9d3d9b9\" : print ( \"TA-Lib validated\" ) else : print ( \"TA-Lib has incorrect hash value, can't trust it. Found hash: '\" + str ( zipmdsum ) + \"'\" ) sys . exit ( 1 ) if os . system ( \"tar -xzf ta-lib-0.4.0-src.tar.gz\" ) != 0 : print ( \"Failed to extract TA-Lib zip\" ) sys . exit ( 1 ) if os . system ( \"apt-get install build-essential -y\" ) != 0 : print ( \"Failed apt-get install build-essential -y\" ) sys . exit ( 1 ) os . chdir ( os . path . abspath ( \".\" ) + \"/ta-lib\" ) if os . system ( \"./configure --prefix=/usr\" ) != 0 : print ( \"Failed to configure TA-Lib for build\" ) sys . exit ( 1 ) if os . system ( \"make\" ) != 0 : print ( \"Failed to make TA-Lib\" ) sys . exit ( 1 ) if os . system ( \"make install\" ) != 0 : print ( \"Failed to make install TA-Lib\" ) sys . exit ( 1 ) print ( \"Installed dependencies for TA-Lib pip package\" ) if os . system ( \"python3 -m pip install TA-Lib\" ) != 0 : print ( \"Failed to pip install TA-Lib\" ) sys . exit ( 1 ) print ( \"Installed TA-Lib pip package\" ) With this, the first time you press Run , the dependency should install. Any subsequent run should already work without having to install.","title":"Troubleshooting"},{"location":"platform/troubleshooting/troubleshooting/#troubleshooting","text":"This section contains solutions, fixes, hints and tips to help you solve the most common issues encountered when using Quix.","title":"Troubleshooting"},{"location":"platform/troubleshooting/troubleshooting/#data-is-not-being-received-into-a-topic","text":"Ensure the Topic Name or Id is correct in Topics option of Quix Portal. You can check the data in / out rates on the Topics tab. If you want to see the data in the Data Catalogue please make sure you are persisting the data to the Topic otherwise it may appear that there is no data. If you are using a consumer group, check that no other services are using the same group. If you run your code locally and deployed somewhere and they are both using the same consumer group one of them may consume all of the data.","title":"Data is not being received into a Topic"},{"location":"platform/troubleshooting/troubleshooting/#topic-authentication-error","text":"If you see errors like these in your service or job logs then you may have used the wrong credentials or it could be that you have specified the wrong Topic Id. Authentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-256 Exception receiving package from Kafka 3/3 brokers are down Broker: Topic authorization failed Check very carefully each of the details. The following must be correct: TopicId or TopicName Sdk Token These can all be found in Topics option of Quix Portal.","title":"Topic Authentication Error"},{"location":"platform/troubleshooting/troubleshooting/#broker-transport-failure","text":"If you have deployed a service or job and the logs mention broker transport failure then check the workspace name and password in the SecurityOptions. Also check the broker address list. You should have these by default: kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093","title":"Broker Transport Failure"},{"location":"platform/troubleshooting/troubleshooting/#401-error","text":"When attempting to access the web API\u2019s you may encounter a 401 error. Check that the bearer token is correct and has not expired. If necessary generate a new bearer token. Example of the error received when trying to connect to the Streaming Reader API with an expired bearer token signalrcore.hub.errors.UnAuthorizedHubError The API\u2019s that require a valid bearer token are: Portal API https://portal-api.platform.quix.ai/swagger/index.html Streaming Writer API https://writer-[YOUR_ORGANISATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/index.html Telemetry Query API https://telemetry-query-[YOUR_ORGANISATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/swagger/index.html","title":"401 Error"},{"location":"platform/troubleshooting/troubleshooting/#error-handling-in-the-sdk-callbacks","text":"Errors generated in the SDK callback can be swallowed or hard to read. To prevent this and make it easier to determine the root cause you should use a traceback Begin by importing traceback import traceback Then, inside the SDK callback where you might have an issue place code similar to this: def read_stream ( new_stream : StreamReader ): def on_parameter_data_handler ( data : ParameterData ): try : data . timestamps [ 19191919 ] # this does not exist except Exception : print ( traceback . format_exc ()) new_stream . parameters . create_buffer () . on_read += on_parameter_data_handler input_topic . on_stream_received += read_stream Notice that the try clause is within the handler and the except clause prints a formatted exception (below) Traceback ( most recent call last ): File \"main.py\" , line 20 , in on_parameter_data_handler data . timestamps [ 19191919 ] File \"/usr/local/lib/python3.8/dist-packages/quixstreaming/models/netlist.py\" , line 22 , in __getitem__ item = self . __wrapped [ key ] IndexError : list index out of range","title":"Error Handling in the SDK callbacks"},{"location":"platform/troubleshooting/troubleshooting/#service-keeps-failing-and-restarting","text":"If your service continually fails and restarts you will not be able to view the logs. Redeploy your service as a job instead. This will allow you to inspect the logs and get a better idea about what is happening.","title":"Service keeps failing and restarting"},{"location":"platform/troubleshooting/troubleshooting/#possible-dns-propagation-errors","text":"There are currently 2 scenarios in which you might encounter an issue caused by DNS propagation. 1. Data catalogue has been deployed but DNS entries have not fully propagated. In this scenario you might see a banner when accessing the data catalogue. 2. A dashboard or other publicly visible deployment is not yet accessible, again due to DNS propagation. Tip In these scenarios simply wait while the DNS records propagate. It can take up to 10 minutes for DNS to records to propagate fully.","title":"Possible DNS Propagation Errors"},{"location":"platform/troubleshooting/troubleshooting/#python-version","text":"If you get strange errors when trying to compile your Python code locally please check that you are using Python version 3.8 For example you may encounter a ModuleNotFoundError ModuleNotFoundError : No module named 'quixstreaming' For information on how to setup your IDE for working with Quix please check out this section on the SDK documentation.","title":"Python Version"},{"location":"platform/troubleshooting/troubleshooting/#jupyter-notebooks","text":"If you are having trouble with Jupyter Notebooks or another consumer of Quix data try using aggregation to reduce the number of records returned. For more info on aggregation check out this short video .","title":"Jupyter Notebooks"},{"location":"platform/troubleshooting/troubleshooting/#process-killed-or-out-of-memory","text":"If your deployment\u2019s logs report \"Killed\" or \"Out of memory\" then you may need to increase the amount of memory assgned to the deployment. You may experience this: At build time if you want to load large third party packages into your code At runtime if you are storing large datasets in memory.","title":"Process Killed or Out of memory"},{"location":"platform/troubleshooting/troubleshooting/#missing-dependency-in-online-ide","text":"Currently the online IDE does not use the same docker image as the one used for deployment due to time it would take to build it and make it available to you. (Likely feature for future however) Because of this you might have some OS level dependencies that you need to install from within your python code to be able to make use of the Run feature in the IDE. The section below should give you guidance how to achieve this. In your main.py (or similar) file, add as the first line: import preinstall . Now create the file preinstall.py and add content based on example below: TA-Lib This script will check if TA-Lib is already installed (like from docker deployment). If not then installs it. import os import sys ta_lib_pip_details = os . system ( \"python3 -m pip show TA-Lib\" ) if ta_lib_pip_details == 0 : print ( \"TA-Lib already installed\" ) else : if os . system ( \"apt-get update\" ) != 0 : print ( \"Failed apt-get update\" ) sys . exit ( 1 ) if os . popen ( \"if [ -e ta-lib-0.4.0-src.tar.gz ]; then echo \\\" ok \\\" ; else echo \\\" nok \\\" ; fi\" ) . read () . strip () == \"ok\" : print ( \"TA-Lib already downloaded\" ) else : print ( \"Downloading ta-lib\" ) if os . system ( \"apt-get install curl -y\" ) != 0 : print ( \"Failed apt-get install curl -y\" ) sys . exit ( 1 ) if os . system ( \"curl https://jztkft.dl.sourceforge.net/project/ta-lib/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz -O\" ) != 0 : print ( \"Failed to download ta-lib\" ) sys . exit ( 1 ) zipmdsum = os . popen ( \"md5sum ta-lib-0.4.0-src.tar.gz | cut -d ' ' -f 1\" ) . read () . strip () if zipmdsum == \"308e53b9644213fc29262f36b9d3d9b9\" : print ( \"TA-Lib validated\" ) else : print ( \"TA-Lib has incorrect hash value, can't trust it. Found hash: '\" + str ( zipmdsum ) + \"'\" ) sys . exit ( 1 ) if os . system ( \"tar -xzf ta-lib-0.4.0-src.tar.gz\" ) != 0 : print ( \"Failed to extract TA-Lib zip\" ) sys . exit ( 1 ) if os . system ( \"apt-get install build-essential -y\" ) != 0 : print ( \"Failed apt-get install build-essential -y\" ) sys . exit ( 1 ) os . chdir ( os . path . abspath ( \".\" ) + \"/ta-lib\" ) if os . system ( \"./configure --prefix=/usr\" ) != 0 : print ( \"Failed to configure TA-Lib for build\" ) sys . exit ( 1 ) if os . system ( \"make\" ) != 0 : print ( \"Failed to make TA-Lib\" ) sys . exit ( 1 ) if os . system ( \"make install\" ) != 0 : print ( \"Failed to make install TA-Lib\" ) sys . exit ( 1 ) print ( \"Installed dependencies for TA-Lib pip package\" ) if os . system ( \"python3 -m pip install TA-Lib\" ) != 0 : print ( \"Failed to pip install TA-Lib\" ) sys . exit ( 1 ) print ( \"Installed TA-Lib pip package\" ) With this, the first time you press Run , the dependency should install. Any subsequent run should already work without having to install.","title":"Missing Dependency in online IDE"},{"location":"platform/tutorials/ImageProcessing/","text":"Image Processing We show you how to build a video processing pipeline using the Transport for London (TfL) traffic cameras, known as Jam Cams, and a YOLO v3 model. We provide a fully functional UI to show you where the recognized objects are located around London. Watch It If you'd rather watch our live stream, where we run through this tutorial, see it here: Get Started To get started make sure you have a Quix account: signup for a completely free account at https://quix.io/ You\u2019ll also need a free TfL account which you can register for here: https://api-portal.tfl.gov.uk/ A rough guide to finding your TfL API key is as follows: Register for an account. Login and click the \"Products\" menu item. You should have 1 product to choose from \"500 Requests per min.\" Click \"500 Requests per min.\" Enter a name for your subscription into the box, e.g. QuixFeed, and click \"Register.\" You can now find your API Keys in the profile page. If you need any assistance while following the tutorial, we\u2019re here to help in The Stream , our free Slack community. Library Most of the code you need has already been written for you and it\u2019s located in our library. We\u2019ll be referring to the library often so make sure you know where it is: the left hand side menu bar. Pipeline Services There are 5 stages to the processing pipeline you are about to build. Video Feeds TfL Camera feed or \u201cJam Cams\u201d Webcam image capture Pre Processing Frame extraction Decoding Object Detection Detect objects within images Stream Merge Merge images from all of the individual TfL cameras into one stream Web UI A simple UI showing: Images with identified objects Map with count of objects at each cameras location Built It Now you know which components will be needed in the image processing pipeline, let\u2019s create and deploy each service. 1. Video Feeds Follow these steps to deploy the traffic camera feed service. Navigate to the Library and locate \u201cTfL Camera Feed\u201d. Click \u201cSetup & deploy\u201d Paste your TfL API Key into the appropriate input Click \u201cDeploy\u201d Deploying will start the service in our pre-provisioned infrastructure. This service will stream data from the TfL cameras to the \u201ctfl-cameras\u201d topic. Once deployed: Stop the service You will restart it later, but for now it can be stopped. INFO: At this point you should be looking at your pipeline view and have one service deployed. When it has started the arrow pointing out of the service will be green. This indicates that data is flowing out of the service into a topic. Now, we need to deploy something to consume the data streaming into that topic. Follow these steps to deploy the webcam service. Navigate to the Library and locate \u201cWebcam image capture service\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service will stream data from your webcam to the \u201cimage-base64\u201d topic. Click the service tile Click the \u201cPublic URL\u201d This opens the deployed website which uses your webcam to stream images to Quix Note Your browser may prompt you to allow access to your webcam. 2. Pre Processing Follow these steps to deploy the frame extraction service. Navigate to the Library and locate \u201cTfL traffic camera frame grabber\u201d. Click \u201cSetup & deploy\u201d Click Deploy This service receives data from the \u201ctfl-cameras\u201d topic and streams data to the \u201cimage-raw\u201d topic. Follow these steps to deploy the decoder service. Navigate to the Library and locate \u201cBase64 Decoder\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service receives data from the \u201cimage-base64\u201d topic and streams data to the \u201cimage-raw\u201d topic. 3. Computer Vision Follow these steps to deploy the object detection service. Navigate to the Library and locate \u201cComputer Vision object detection\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service receives data from the \u201cimage-raw\u201d topic and streams data to the \u201cimage-processed\u201d topic. 4. Stream Merge Follow these steps to deploy the object detection service. Navigate to the Library and locate \u201cStream merge\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service receives data from the \u201cimage-processed\u201d topic and streams data to the \u201cimage-processed-merged\u201d topic. 5. Web UI Follow these steps to deploy the object detection service. Navigate to the Library and locate \u201cTFL image processing UI\u201d. Click \u201cEdit code\u201d Click \u201cSave as project\u201d The code for this Angular UI is now saved to your workspace Find and select the environment-variables.service.ts file. You need to provide the \u201cToken\u201d that this website will need to communicate with Quix. Click the user avatar icon in the top right corner of the Quix portal Click Tokens Generate a token Give it a name and expiry date Copy the token Click back on your browser and relocate the environment-variables.service.ts file. Paste the token into the double quotes as indicated in the file Token : string = \"your token here\" ; Tag the code and deploy the UI Click the +tag button at the top of the code file Enter v1 and press enter Click Deploy near the top right corner Select v1 under the \u201cVersion Tag\u201d This is the tag you created in step 2 Click \u201cService\u201d in \u201cDeployment Settings\u201d This ensures the service runs continuously Click the toggle in \u201cPublic Access\u201d This enables access from anywhere on the internet Click \u201cDeploy\u201d The UI receives data from the \u201cimage-processed-merged\u201d topic Once deployed, click the service tile Click the \u201cPublic URL\u201d This is the user interface for the demo. This screenshot shows the last image processed from one of the TfL traffic cameras as well as the map with a count of all the cars seen so far and where they were seen. Next Steps You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, samples and examples. Now you can build your own connectors and apps and contribute by going to our Github here , forking our library repo and submitting your code, updates and ideas. What will you build, let us know. We\u2019d love to feature your project or use case in our newsletter. If you need any assistance, we\u2019re here to help in The Stream , our free Slack community. That\u2019s all folks! We hope you enjoyed this tutorial on how to deploy a real time image processing pipeline in just a few minutes. If you have any questions or feedback please contact us on The Stream. Thank you and goodbye!","title":"Image Processing"},{"location":"platform/tutorials/ImageProcessing/#image-processing","text":"We show you how to build a video processing pipeline using the Transport for London (TfL) traffic cameras, known as Jam Cams, and a YOLO v3 model. We provide a fully functional UI to show you where the recognized objects are located around London.","title":"Image Processing"},{"location":"platform/tutorials/ImageProcessing/#watch-it","text":"If you'd rather watch our live stream, where we run through this tutorial, see it here:","title":"Watch It"},{"location":"platform/tutorials/ImageProcessing/#get-started","text":"To get started make sure you have a Quix account: signup for a completely free account at https://quix.io/ You\u2019ll also need a free TfL account which you can register for here: https://api-portal.tfl.gov.uk/ A rough guide to finding your TfL API key is as follows: Register for an account. Login and click the \"Products\" menu item. You should have 1 product to choose from \"500 Requests per min.\" Click \"500 Requests per min.\" Enter a name for your subscription into the box, e.g. QuixFeed, and click \"Register.\" You can now find your API Keys in the profile page. If you need any assistance while following the tutorial, we\u2019re here to help in The Stream , our free Slack community.","title":"Get Started"},{"location":"platform/tutorials/ImageProcessing/#library","text":"Most of the code you need has already been written for you and it\u2019s located in our library. We\u2019ll be referring to the library often so make sure you know where it is: the left hand side menu bar.","title":"Library"},{"location":"platform/tutorials/ImageProcessing/#pipeline-services","text":"There are 5 stages to the processing pipeline you are about to build.","title":"Pipeline Services"},{"location":"platform/tutorials/ImageProcessing/#video-feeds","text":"TfL Camera feed or \u201cJam Cams\u201d Webcam image capture","title":"Video Feeds"},{"location":"platform/tutorials/ImageProcessing/#pre-processing","text":"Frame extraction Decoding","title":"Pre Processing"},{"location":"platform/tutorials/ImageProcessing/#object-detection","text":"Detect objects within images","title":"Object Detection"},{"location":"platform/tutorials/ImageProcessing/#stream-merge","text":"Merge images from all of the individual TfL cameras into one stream","title":"Stream Merge"},{"location":"platform/tutorials/ImageProcessing/#web-ui","text":"A simple UI showing: Images with identified objects Map with count of objects at each cameras location","title":"Web UI"},{"location":"platform/tutorials/ImageProcessing/#built-it","text":"Now you know which components will be needed in the image processing pipeline, let\u2019s create and deploy each service.","title":"Built It"},{"location":"platform/tutorials/ImageProcessing/#1-video-feeds","text":"Follow these steps to deploy the traffic camera feed service. Navigate to the Library and locate \u201cTfL Camera Feed\u201d. Click \u201cSetup & deploy\u201d Paste your TfL API Key into the appropriate input Click \u201cDeploy\u201d Deploying will start the service in our pre-provisioned infrastructure. This service will stream data from the TfL cameras to the \u201ctfl-cameras\u201d topic. Once deployed: Stop the service You will restart it later, but for now it can be stopped. INFO: At this point you should be looking at your pipeline view and have one service deployed. When it has started the arrow pointing out of the service will be green. This indicates that data is flowing out of the service into a topic. Now, we need to deploy something to consume the data streaming into that topic. Follow these steps to deploy the webcam service. Navigate to the Library and locate \u201cWebcam image capture service\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service will stream data from your webcam to the \u201cimage-base64\u201d topic. Click the service tile Click the \u201cPublic URL\u201d This opens the deployed website which uses your webcam to stream images to Quix Note Your browser may prompt you to allow access to your webcam.","title":"1. Video Feeds"},{"location":"platform/tutorials/ImageProcessing/#2-pre-processing","text":"Follow these steps to deploy the frame extraction service. Navigate to the Library and locate \u201cTfL traffic camera frame grabber\u201d. Click \u201cSetup & deploy\u201d Click Deploy This service receives data from the \u201ctfl-cameras\u201d topic and streams data to the \u201cimage-raw\u201d topic. Follow these steps to deploy the decoder service. Navigate to the Library and locate \u201cBase64 Decoder\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service receives data from the \u201cimage-base64\u201d topic and streams data to the \u201cimage-raw\u201d topic.","title":"2. Pre Processing"},{"location":"platform/tutorials/ImageProcessing/#3-computer-vision","text":"Follow these steps to deploy the object detection service. Navigate to the Library and locate \u201cComputer Vision object detection\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service receives data from the \u201cimage-raw\u201d topic and streams data to the \u201cimage-processed\u201d topic.","title":"3. Computer Vision"},{"location":"platform/tutorials/ImageProcessing/#4-stream-merge","text":"Follow these steps to deploy the object detection service. Navigate to the Library and locate \u201cStream merge\u201d. Click \u201cSetup & deploy\u201d Click \u201cDeploy\u201d This service receives data from the \u201cimage-processed\u201d topic and streams data to the \u201cimage-processed-merged\u201d topic.","title":"4. Stream Merge"},{"location":"platform/tutorials/ImageProcessing/#5-web-ui","text":"Follow these steps to deploy the object detection service. Navigate to the Library and locate \u201cTFL image processing UI\u201d. Click \u201cEdit code\u201d Click \u201cSave as project\u201d The code for this Angular UI is now saved to your workspace Find and select the environment-variables.service.ts file. You need to provide the \u201cToken\u201d that this website will need to communicate with Quix. Click the user avatar icon in the top right corner of the Quix portal Click Tokens Generate a token Give it a name and expiry date Copy the token Click back on your browser and relocate the environment-variables.service.ts file. Paste the token into the double quotes as indicated in the file Token : string = \"your token here\" ; Tag the code and deploy the UI Click the +tag button at the top of the code file Enter v1 and press enter Click Deploy near the top right corner Select v1 under the \u201cVersion Tag\u201d This is the tag you created in step 2 Click \u201cService\u201d in \u201cDeployment Settings\u201d This ensures the service runs continuously Click the toggle in \u201cPublic Access\u201d This enables access from anywhere on the internet Click \u201cDeploy\u201d The UI receives data from the \u201cimage-processed-merged\u201d topic Once deployed, click the service tile Click the \u201cPublic URL\u201d This is the user interface for the demo. This screenshot shows the last image processed from one of the TfL traffic cameras as well as the map with a count of all the cars seen so far and where they were seen.","title":"5. Web UI"},{"location":"platform/tutorials/ImageProcessing/#next-steps","text":"You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, samples and examples. Now you can build your own connectors and apps and contribute by going to our Github here , forking our library repo and submitting your code, updates and ideas. What will you build, let us know. We\u2019d love to feature your project or use case in our newsletter. If you need any assistance, we\u2019re here to help in The Stream , our free Slack community.","title":"Next Steps"},{"location":"platform/tutorials/ImageProcessing/#thats-all-folks","text":"We hope you enjoyed this tutorial on how to deploy a real time image processing pipeline in just a few minutes. If you have any questions or feedback please contact us on The Stream. Thank you and goodbye!","title":"That\u2019s all folks!"},{"location":"platform/tutorials/RSSProcessingPipeline/","text":"RSS Processing RSS processing pipeline This tutorial explains how to build a pipeline that gathers and processes data from an RSS feed and alerts users when specific criteria are met. It\u2019s a companion to a live coding session on YouTube, where we sourced data from a StackOverflow RSS feed for use in a Python service. This tutorial has three parts Sourcing data Processing data Sending alerts What you need A free Quix account . It comes with enough credits to create this project. A Slack account with access to create a webhook. ( This guide can help you with this step.) Sourcing data 1. Get the \u201cRSS Data Source\u201d connector In your Quix account, go to the library and search for \u201cRSS Data Source.\u201d (Hint: you can watch Steve prepare this code in the video tutorial if you\u2019re like to learn more about it.) Click \u201cSetup & deploy\u201d on the \u201cRSS Data Source\u201d library item. (The card has a blue line across its top that indicates it\u2019s a source connector.) 2. Configure the connector In the configuration panel, keep the default name and output topic. Enter the following URL into the rss_url field: https://stackoverflow.com/feeds/tag/python Click \u201cDeploy\u201d and wait a few seconds for the pre-built connector to be deployed to your workspace. You will then begin to receive data from the RSS feed. The data then goes into the configured output topic. Don\u2019t worry, you won\u2019t lose data. It\u2019s cached in the topic until another deployment starts reading it. Processing data You can do anything you want in the processing phase of a pipeline. You might want to merge several input streams or make decisions on your data. In this tutorial, you\u2019ll filter and augment data so that only questions with certain tags get delivered to you. 1. Get the \u201cRSS Data Filtering\u201d connector Return to the library tab in Quix and search for \u201cRSS Data Filtering.\u201d Click \u201cSetup & deploy\u201d on the card. If you created a new workspace for this project, the fields automatically populate. If you\u2019re using the workspace for other projects, you may need to specify the input topic as \u201crss-data.\u201d You might also want to customize the tag_filter. It is automatically populated with a wide range of tags related to Python. This works well for this demo, because you\u2019ll see a large return of interesting posts. But you can decrease or add tags. 2. Deploy \u201cRSS Data Filtering\u201d connector Click \u201cDeploy\u201d on the \u201cRSS Data Filtering\u201d connector. Once deployed, the connector will begin processing the data that\u2019s been building up in the rss-data topic. Have a look in the logs by clicking the Data Filtering Model tile (pink outlined) on the workspace home page. The transformation stage is now complete. Your project is now sending the filtered and enhanced data to the output topic. Sending alerts Last in our pipeline is the destination for our RSS data. This demo uses a Slack channel as its destination. 1. Get the \u201cSlack Notification\u201d connector Return to the Quix library and search for the \u201cSlack Notification.\u201d Click \u201cPreview code.\u201d You\u2019re going to modify the standard code before deploying this connector. Click \u201cNext\u201d on the dialog box. Ensure \u201cfiltered-rss-data\u201d is selected as the input topic and provide a Slack \u201cwebhook_url.\u201d Note If you have your own slack, head over to the Slack API pages and create a webhook following their guide \u201cGetting started with Incoming Webhooks.\u201d If you don\u2019t have your own Slack or don\u2019t have the account privileges to create the webhook, you can choose another destination from the library, such as Twilio. Warning: Use a dev or demo or unimportant Slack channel while you\u2019re developing this. Trust me. 2. Modify and deploy the \u201cSlack Notification\u201d connector Enter your webhook into the webhook_url field. Click \u201cSave as project.\u201d This will save the code to your workspace, which is a GitLab repository. Once saved, you\u2019ll see the code again. The quix_function.py file should be open. This is what you\u2019ll alter. The default code dumps everything in the parameter data and event data to the Slack channel. It\u2019ll do to get you up and going, but we want something more refined. \ud83d\ude09 Go to our GitHub library of tutorial code here . The code picks out several field values from the parameter data and combines them to form the desired Slack alert. Copy the code and paste it over the quix_function.py file in your project in the Quix portal. Save it by clicking \u201cCTRL+S\u201d or \u201cCommand + S\u201d or click the tick in the top right. Then deploy by clicking the \u201cDeploy\u201d button in the top right. On the dialogue, change the deployment type to \u201cService\u201d and click \u201cDeploy\u201d. Congratulations You have deployed all three stages of the pipeline and should be receiving thousands of Slack messages. You might be thinking that Quix has led you on the path to destroying your Slack server. But don\u2019t worry \u2014 the pipeline is processing all the cached messages and will stop soon, honest. If this were a production pipeline, you\u2019d be very happy you haven\u2019t lost all those precious live messages. Help If you run into trouble with the tutorial, want to chat with us about your project or anything else associated with streaming processing you can join our public Slack community called The Stream .","title":"RSS Processing"},{"location":"platform/tutorials/RSSProcessingPipeline/#rss-processing","text":"","title":"RSS Processing"},{"location":"platform/tutorials/RSSProcessingPipeline/#rss-processing-pipeline","text":"This tutorial explains how to build a pipeline that gathers and processes data from an RSS feed and alerts users when specific criteria are met. It\u2019s a companion to a live coding session on YouTube, where we sourced data from a StackOverflow RSS feed for use in a Python service. This tutorial has three parts Sourcing data Processing data Sending alerts What you need A free Quix account . It comes with enough credits to create this project. A Slack account with access to create a webhook. ( This guide can help you with this step.)","title":"RSS processing pipeline"},{"location":"platform/tutorials/RSSProcessingPipeline/#sourcing-data","text":"","title":"Sourcing data"},{"location":"platform/tutorials/RSSProcessingPipeline/#1-get-the-rss-data-source-connector","text":"In your Quix account, go to the library and search for \u201cRSS Data Source.\u201d (Hint: you can watch Steve prepare this code in the video tutorial if you\u2019re like to learn more about it.) Click \u201cSetup & deploy\u201d on the \u201cRSS Data Source\u201d library item. (The card has a blue line across its top that indicates it\u2019s a source connector.)","title":"1. Get the \u201cRSS Data Source\u201d connector"},{"location":"platform/tutorials/RSSProcessingPipeline/#2-configure-the-connector","text":"In the configuration panel, keep the default name and output topic. Enter the following URL into the rss_url field: https://stackoverflow.com/feeds/tag/python Click \u201cDeploy\u201d and wait a few seconds for the pre-built connector to be deployed to your workspace. You will then begin to receive data from the RSS feed. The data then goes into the configured output topic. Don\u2019t worry, you won\u2019t lose data. It\u2019s cached in the topic until another deployment starts reading it.","title":"2. Configure the connector"},{"location":"platform/tutorials/RSSProcessingPipeline/#processing-data","text":"You can do anything you want in the processing phase of a pipeline. You might want to merge several input streams or make decisions on your data. In this tutorial, you\u2019ll filter and augment data so that only questions with certain tags get delivered to you.","title":"Processing data"},{"location":"platform/tutorials/RSSProcessingPipeline/#1-get-the-rss-data-filtering-connector","text":"Return to the library tab in Quix and search for \u201cRSS Data Filtering.\u201d Click \u201cSetup & deploy\u201d on the card. If you created a new workspace for this project, the fields automatically populate. If you\u2019re using the workspace for other projects, you may need to specify the input topic as \u201crss-data.\u201d You might also want to customize the tag_filter. It is automatically populated with a wide range of tags related to Python. This works well for this demo, because you\u2019ll see a large return of interesting posts. But you can decrease or add tags.","title":"1. Get the \u201cRSS Data Filtering\u201d connector"},{"location":"platform/tutorials/RSSProcessingPipeline/#2-deploy-rss-data-filtering-connector","text":"Click \u201cDeploy\u201d on the \u201cRSS Data Filtering\u201d connector. Once deployed, the connector will begin processing the data that\u2019s been building up in the rss-data topic. Have a look in the logs by clicking the Data Filtering Model tile (pink outlined) on the workspace home page. The transformation stage is now complete. Your project is now sending the filtered and enhanced data to the output topic.","title":"2. Deploy \u201cRSS Data Filtering\u201d connector"},{"location":"platform/tutorials/RSSProcessingPipeline/#sending-alerts","text":"Last in our pipeline is the destination for our RSS data. This demo uses a Slack channel as its destination.","title":"Sending alerts"},{"location":"platform/tutorials/RSSProcessingPipeline/#1-get-the-slack-notification-connector","text":"Return to the Quix library and search for the \u201cSlack Notification.\u201d Click \u201cPreview code.\u201d You\u2019re going to modify the standard code before deploying this connector. Click \u201cNext\u201d on the dialog box. Ensure \u201cfiltered-rss-data\u201d is selected as the input topic and provide a Slack \u201cwebhook_url.\u201d Note If you have your own slack, head over to the Slack API pages and create a webhook following their guide \u201cGetting started with Incoming Webhooks.\u201d If you don\u2019t have your own Slack or don\u2019t have the account privileges to create the webhook, you can choose another destination from the library, such as Twilio. Warning: Use a dev or demo or unimportant Slack channel while you\u2019re developing this. Trust me.","title":"1. Get the \u201cSlack Notification\u201d connector"},{"location":"platform/tutorials/RSSProcessingPipeline/#2-modify-and-deploy-the-slack-notification-connector","text":"Enter your webhook into the webhook_url field. Click \u201cSave as project.\u201d This will save the code to your workspace, which is a GitLab repository. Once saved, you\u2019ll see the code again. The quix_function.py file should be open. This is what you\u2019ll alter. The default code dumps everything in the parameter data and event data to the Slack channel. It\u2019ll do to get you up and going, but we want something more refined. \ud83d\ude09 Go to our GitHub library of tutorial code here . The code picks out several field values from the parameter data and combines them to form the desired Slack alert. Copy the code and paste it over the quix_function.py file in your project in the Quix portal. Save it by clicking \u201cCTRL+S\u201d or \u201cCommand + S\u201d or click the tick in the top right. Then deploy by clicking the \u201cDeploy\u201d button in the top right. On the dialogue, change the deployment type to \u201cService\u201d and click \u201cDeploy\u201d.","title":"2. Modify and deploy the \u201cSlack Notification\u201d connector"},{"location":"platform/tutorials/RSSProcessingPipeline/#congratulations","text":"You have deployed all three stages of the pipeline and should be receiving thousands of Slack messages. You might be thinking that Quix has led you on the path to destroying your Slack server. But don\u2019t worry \u2014 the pipeline is processing all the cached messages and will stop soon, honest. If this were a production pipeline, you\u2019d be very happy you haven\u2019t lost all those precious live messages.","title":"Congratulations"},{"location":"platform/tutorials/RSSProcessingPipeline/#help","text":"If you run into trouble with the tutorial, want to chat with us about your project or anything else associated with streaming processing you can join our public Slack community called The Stream .","title":"Help"},{"location":"platform/tutorials/currency-alerting/","text":"Currency Alerting Aim Quix allows you to create complex and efficient infrastructure in a quick and simple way. To show you how, this tutorial will guide you through the steps to build a real time streaming pipeline that sends alerts to your phone when the Bitcoin price reaches certain threshold. Note This guide will take you through the steps to start working with Twilio (developer platform for communications) and the coinAPI (platform that provides data APIs to cryptocurrency) using Quix. By the end you will have: Deployed a currency exchange rate capture connector Deployed Twilio connector to send alerts to your mobile phone Prerequisites We assume that all you have is a Quix account that you haven\u2019t started using yet. Tip If you don\u2019t have a Quix account yet, go here and create one. Overview This walkthrough covers the following steps: Create third party accounts: Twilio and CoinApi Deploy connectors from the Quix Library for: the Coin API data source the Twilio sink or output Create free third party accounts Create a free CoinApi account Note CoinAPI is a platform that provides data APIs for many financial instruments including cryptocurrency. We will be using it to get live Bitcoin (BTC) data. Let\u2019s create a free coinAPI account: Go to https://www.coinapi.io/ . Click the \"Get a free API key\" button and complete the dialog to create a free account. You will receive your API key on your provided email. Keep it safe for later . Create a free Twilio account Note Twilio is a developer platform for communications. We can use Twilio, for instance, to send SMS messages. Let\u2019s create a free Twilio account: Go to https://www.twilio.com/ . Click the \"Sign Up\" button and complete the dialog. Do the email and text message verifications. Then, we will need to setup the Twilio account and create a messaging service. It\u2019s not too hard but there are a couple of sticky points, so we\u2019ll take you through it. Step 3 video-instructions Tip Complete step 3 following our short 2 min video here . Remember to gather your IDS and keep them safe for later: the Messaging Service SID in the Properties section and the Account SID and Auth Token in the Dashboard. Step 3 written-instructions On your first login, tell Twilio: Which product to use: SMS What to build: Alerts & Notifications How to build it: With no code at all What is your goal: Build something myself Click \"Get Started with Twilio\". Once in the Dashboard menu, get a phone number for sending messages: Click \"Get Trial Number\" You will be assigned a unique phone number That\u2019s the number Twilio is going to be sending SMSs from (and you can receive SMSs there too) In the menu on the left hand side expand \"Messaging\". Click Services section Click the \"Create Messaging Service\" button Name: Something like \"QuixAlerting\" Use Case: Notify my users Click \"Create Messaging Service\" On the next page you\u2019ll assign the \"Sender\", i.e. your new phone number: Click \"Add Senders\" button In the dialog: check that Sender Type is on \"Phone Number\" and click \"Continue\" Select or Tick the phone number (it is the one created for you earlier) Click \"Add Phone Numbers\" Note If you are in the Twilio wizard the next step is to \"Setup Integration\". You don\u2019t need to do this so just click the \"Skip\" button towards the bottom of the screen. You\u2019ll do the integration in Quix. Finally, gather your ID\u2019s and keep them safe for later : Find the Messaging Service SID in the \"Messaging\" then \"Services\" menus on the left hand side. The SID is listed next to the \"QuixAlerting\" messaging service Find the Account SID and Auth Token in the \"Account\" menu under \"API keys & tokens\" Quix Library Now let\u2019s find and deploy the 2 connectors needed to make this project work. Coin API We\u2019ll start with the Coin API. So head over to the Quix Library and search for \"Coin API\" and click \"Setup and Deploy\" on the Coin API tile. The connector needs some values for it to work, some of these have default values which you can change now or later if you want to. The only thing you have to provide is the API key you got when you signed up to Coin API. Once you have entered your Coin API key just click Deploy. Note This version of the Coin API connector is rate limited so that your free trial account doesn\u2019t exceed the quotas imposed by Coin API. It will fetch data every 15 minutes. You can unlock this limit by saving the code to your workspace and removing the limit. (Let us know if you want help with this) Logs Once the connector has deployed it will start automatically and you\u2019ll be redirected to the workspace home page. You can click on the Coin API card where you will see the logs. If you see an error you might have to wait a few minutes for your API key to propagate to all of the Coin API servers. Twilio Head back to the Quix Library and search for \"Twilio\" and click \"Setup and Deploy\" on the Twilio Sink tile. Just like the Coin API connector, this one also needs some values for it to work. Use the guidance in the connectors setup guide to fill in the required details. Ensure the input topic is \"coin-data\". \"Numbers\" should be the command separated list of phone numbers to send SMS alerts to. The account_sid, auth_token and messaging_service_sid can be populated using the values you saved while setting up your Twilio account. message_limit can be left at 2 or changed to suit your needs. Click Deploy and wait for the pre-built connector to be depolyed to your workspace. Logs Once deployed you will again be redirected to the workspace home page. You can click on the Twilio Sink tile and view the logs if you wish. Note The logs may display a message similar to \"skipped due to message limit reached\". It\u2019s just an informational display. Not an error and is due to the per minute message limit we configured for the Twilio connector in the previous stage. Congratulations At this point you should be receiving messages to your mobile phone. Congratulations you\u2019ve done it! Recap - What did you just do! [x] You created a realtime, always on solution on the Quix serverless compute environment. All for Free! [x] You deployed two connectors. One to read data from another platform and one to make decisions based on that data and send notifications via SMS. In real time. [x] You gained some experience of navigating the Quix portal and how to deploy connectors. All without doing any coding! What\u2019s Next What else can you use Quix for? You can stream any kind of data into the Quix platform, from other apps, websites and services. From hardware, machinery or wearables or from anything you can think of that outputs data. You can then process that data in any imaginable way. See what you can do with it and please share your experiences with us. In fact, share it with your colleagues, friends and family. Blog, tweet and post about it and tell anyone who will listen about the possibilities. Tip If you run into trouble please reach out to us. We\u2019ll be more than happy to help. We hang out at The Stream . Come and say hi.","title":"Currency Alerting"},{"location":"platform/tutorials/currency-alerting/#currency-alerting","text":"","title":"Currency Alerting"},{"location":"platform/tutorials/currency-alerting/#aim","text":"Quix allows you to create complex and efficient infrastructure in a quick and simple way. To show you how, this tutorial will guide you through the steps to build a real time streaming pipeline that sends alerts to your phone when the Bitcoin price reaches certain threshold. Note This guide will take you through the steps to start working with Twilio (developer platform for communications) and the coinAPI (platform that provides data APIs to cryptocurrency) using Quix. By the end you will have: Deployed a currency exchange rate capture connector Deployed Twilio connector to send alerts to your mobile phone","title":"Aim"},{"location":"platform/tutorials/currency-alerting/#prerequisites","text":"We assume that all you have is a Quix account that you haven\u2019t started using yet. Tip If you don\u2019t have a Quix account yet, go here and create one.","title":"Prerequisites"},{"location":"platform/tutorials/currency-alerting/#overview","text":"This walkthrough covers the following steps: Create third party accounts: Twilio and CoinApi Deploy connectors from the Quix Library for: the Coin API data source the Twilio sink or output","title":"Overview"},{"location":"platform/tutorials/currency-alerting/#create-free-third-party-accounts","text":"","title":"Create free third party accounts"},{"location":"platform/tutorials/currency-alerting/#create-a-free-coinapi-account","text":"Note CoinAPI is a platform that provides data APIs for many financial instruments including cryptocurrency. We will be using it to get live Bitcoin (BTC) data. Let\u2019s create a free coinAPI account: Go to https://www.coinapi.io/ . Click the \"Get a free API key\" button and complete the dialog to create a free account. You will receive your API key on your provided email. Keep it safe for later .","title":"Create a free CoinApi account"},{"location":"platform/tutorials/currency-alerting/#create-a-free-twilio-account","text":"Note Twilio is a developer platform for communications. We can use Twilio, for instance, to send SMS messages. Let\u2019s create a free Twilio account: Go to https://www.twilio.com/ . Click the \"Sign Up\" button and complete the dialog. Do the email and text message verifications. Then, we will need to setup the Twilio account and create a messaging service. It\u2019s not too hard but there are a couple of sticky points, so we\u2019ll take you through it. Step 3 video-instructions Tip Complete step 3 following our short 2 min video here . Remember to gather your IDS and keep them safe for later: the Messaging Service SID in the Properties section and the Account SID and Auth Token in the Dashboard. Step 3 written-instructions On your first login, tell Twilio: Which product to use: SMS What to build: Alerts & Notifications How to build it: With no code at all What is your goal: Build something myself Click \"Get Started with Twilio\". Once in the Dashboard menu, get a phone number for sending messages: Click \"Get Trial Number\" You will be assigned a unique phone number That\u2019s the number Twilio is going to be sending SMSs from (and you can receive SMSs there too) In the menu on the left hand side expand \"Messaging\". Click Services section Click the \"Create Messaging Service\" button Name: Something like \"QuixAlerting\" Use Case: Notify my users Click \"Create Messaging Service\" On the next page you\u2019ll assign the \"Sender\", i.e. your new phone number: Click \"Add Senders\" button In the dialog: check that Sender Type is on \"Phone Number\" and click \"Continue\" Select or Tick the phone number (it is the one created for you earlier) Click \"Add Phone Numbers\" Note If you are in the Twilio wizard the next step is to \"Setup Integration\". You don\u2019t need to do this so just click the \"Skip\" button towards the bottom of the screen. You\u2019ll do the integration in Quix. Finally, gather your ID\u2019s and keep them safe for later : Find the Messaging Service SID in the \"Messaging\" then \"Services\" menus on the left hand side. The SID is listed next to the \"QuixAlerting\" messaging service Find the Account SID and Auth Token in the \"Account\" menu under \"API keys & tokens\"","title":"Create a free Twilio account"},{"location":"platform/tutorials/currency-alerting/#quix-library","text":"Now let\u2019s find and deploy the 2 connectors needed to make this project work.","title":"Quix Library"},{"location":"platform/tutorials/currency-alerting/#coin-api","text":"We\u2019ll start with the Coin API. So head over to the Quix Library and search for \"Coin API\" and click \"Setup and Deploy\" on the Coin API tile. The connector needs some values for it to work, some of these have default values which you can change now or later if you want to. The only thing you have to provide is the API key you got when you signed up to Coin API. Once you have entered your Coin API key just click Deploy. Note This version of the Coin API connector is rate limited so that your free trial account doesn\u2019t exceed the quotas imposed by Coin API. It will fetch data every 15 minutes. You can unlock this limit by saving the code to your workspace and removing the limit. (Let us know if you want help with this)","title":"Coin API"},{"location":"platform/tutorials/currency-alerting/#logs","text":"Once the connector has deployed it will start automatically and you\u2019ll be redirected to the workspace home page. You can click on the Coin API card where you will see the logs. If you see an error you might have to wait a few minutes for your API key to propagate to all of the Coin API servers.","title":"Logs"},{"location":"platform/tutorials/currency-alerting/#twilio","text":"Head back to the Quix Library and search for \"Twilio\" and click \"Setup and Deploy\" on the Twilio Sink tile. Just like the Coin API connector, this one also needs some values for it to work. Use the guidance in the connectors setup guide to fill in the required details. Ensure the input topic is \"coin-data\". \"Numbers\" should be the command separated list of phone numbers to send SMS alerts to. The account_sid, auth_token and messaging_service_sid can be populated using the values you saved while setting up your Twilio account. message_limit can be left at 2 or changed to suit your needs. Click Deploy and wait for the pre-built connector to be depolyed to your workspace.","title":"Twilio"},{"location":"platform/tutorials/currency-alerting/#logs_1","text":"Once deployed you will again be redirected to the workspace home page. You can click on the Twilio Sink tile and view the logs if you wish. Note The logs may display a message similar to \"skipped due to message limit reached\". It\u2019s just an informational display. Not an error and is due to the per minute message limit we configured for the Twilio connector in the previous stage.","title":"Logs"},{"location":"platform/tutorials/currency-alerting/#congratulations","text":"At this point you should be receiving messages to your mobile phone. Congratulations you\u2019ve done it!","title":"Congratulations"},{"location":"platform/tutorials/currency-alerting/#recap-what-did-you-just-do","text":"[x] You created a realtime, always on solution on the Quix serverless compute environment. All for Free! [x] You deployed two connectors. One to read data from another platform and one to make decisions based on that data and send notifications via SMS. In real time. [x] You gained some experience of navigating the Quix portal and how to deploy connectors. All without doing any coding!","title":"Recap - What did you just do!"},{"location":"platform/tutorials/currency-alerting/#whats-next","text":"What else can you use Quix for? You can stream any kind of data into the Quix platform, from other apps, websites and services. From hardware, machinery or wearables or from anything you can think of that outputs data. You can then process that data in any imaginable way. See what you can do with it and please share your experiences with us. In fact, share it with your colleagues, friends and family. Blog, tweet and post about it and tell anyone who will listen about the possibilities. Tip If you run into trouble please reach out to us. We\u2019ll be more than happy to help. We hang out at The Stream . Come and say hi.","title":"What\u2019s Next"},{"location":"platform/tutorials/nocode-sentiment-analysis/","text":"No code sentiment analysis This tutorial shows how to build a data processing pipeline without code. You\u2019ll analyze tweets that contain information about Bitcoin and stream both raw and transformed data into Snowflake , a storage platform, using the Twitter, HuggingFace\\</superscript> and Snowflake connectors. I\u2019ve made a video of this tutorial if you prefer watching to reading. What you need for this tutorial Free Quix account Snowflake account Twitter developer account (You can follow this tutorial to set up a developer account ) Step one: create a database Sign in to your Snowflake account to create the Snowflake database which will receive your data. Call this \"demodata\" and click \"Create.\" Step two: get your data In Quix, click into the library and search for the Twitter source connector. Click \"Add new.\" This adds the source to your pipeline and brings you back to the library. Fill in the necessary fields: Name: Twitter Data - Source Output: twitter-data Twitter bearer token: paste your Twitter Dev token here Twitter_search_paramaters: (#BTC OR btc #btc OR BTC) Tip Use search parameters to obtain Tweets on a subject that interests you! e.g. (#dolphins OR dolphins) Click \"Deploy\" Step three: transformation for sentiment analysis Click the \"Add transformation\" button In the library, search for \"HuggingFace\" Click \"Set up and deploy\" on the HuggingFace connector Choose \"Twitter data\" as the input topic The output field should be set to \"hugging-face-output\" by default, leave this as it is or enter this if it\u2019s not pre-populated. Leave all other values with their defaults Click \"Deploy\" Note Find out more about the hugging face model and the other models you could use at huggingface.co Step five: delivery to your Snowflake database Click the \"Add destination\" button on the home screen Search the library for the Snowflake connector Click \"Set up and deploy\" on the connector Fill in the necessary fields: Choose the hugging-face-output output topic The \"Broker__TopicName\" field should be set to \"hugging-face-output\". This means it will receive the data being output by the sentiment analysis model. To fill in the Snowflake locator and region (these are similar to a unique ID for your Snowflake instance), navigate to your Snowflake account. Copy the locator and region from the URL and paste them into the corresponding fields in the connector setup in Quix. Lastly, input your username and password. Click \"Deploy\" on the Snowflake connector. If the credentials and connection details are correct, you\u2019ll see the \"Connected\" status in the log and will be redirected to your workspace. Congratulations! You built a no-code pipeline that filters and collects data from Twitter, transforms it with a HuggingFace model and delivers it to a Snowflake database. You can now go back over to Snowflake and find the \"Databases\" menu. Expand the \"demodata\" database and then find the tables under \"public\". Tip If you need help with this tutorial or want to chat about anything related to Quix or stream processing in general please come and say hi on The Stream , our slack community.","title":"No code sentiment analysis"},{"location":"platform/tutorials/nocode-sentiment-analysis/#no-code-sentiment-analysis","text":"This tutorial shows how to build a data processing pipeline without code. You\u2019ll analyze tweets that contain information about Bitcoin and stream both raw and transformed data into Snowflake , a storage platform, using the Twitter, HuggingFace\\</superscript> and Snowflake connectors. I\u2019ve made a video of this tutorial if you prefer watching to reading.","title":"No code sentiment analysis"},{"location":"platform/tutorials/nocode-sentiment-analysis/#what-you-need-for-this-tutorial","text":"Free Quix account Snowflake account Twitter developer account (You can follow this tutorial to set up a developer account )","title":"What you need for this tutorial"},{"location":"platform/tutorials/nocode-sentiment-analysis/#step-one-create-a-database","text":"Sign in to your Snowflake account to create the Snowflake database which will receive your data. Call this \"demodata\" and click \"Create.\"","title":"Step one: create a database"},{"location":"platform/tutorials/nocode-sentiment-analysis/#step-two-get-your-data","text":"In Quix, click into the library and search for the Twitter source connector. Click \"Add new.\" This adds the source to your pipeline and brings you back to the library. Fill in the necessary fields: Name: Twitter Data - Source Output: twitter-data Twitter bearer token: paste your Twitter Dev token here Twitter_search_paramaters: (#BTC OR btc #btc OR BTC) Tip Use search parameters to obtain Tweets on a subject that interests you! e.g. (#dolphins OR dolphins) Click \"Deploy\"","title":"Step two: get your data"},{"location":"platform/tutorials/nocode-sentiment-analysis/#step-three-transformation-for-sentiment-analysis","text":"Click the \"Add transformation\" button In the library, search for \"HuggingFace\" Click \"Set up and deploy\" on the HuggingFace connector Choose \"Twitter data\" as the input topic The output field should be set to \"hugging-face-output\" by default, leave this as it is or enter this if it\u2019s not pre-populated. Leave all other values with their defaults Click \"Deploy\" Note Find out more about the hugging face model and the other models you could use at huggingface.co","title":"Step three: transformation for sentiment analysis"},{"location":"platform/tutorials/nocode-sentiment-analysis/#step-five-delivery-to-your-snowflake-database","text":"Click the \"Add destination\" button on the home screen Search the library for the Snowflake connector Click \"Set up and deploy\" on the connector Fill in the necessary fields: Choose the hugging-face-output output topic The \"Broker__TopicName\" field should be set to \"hugging-face-output\". This means it will receive the data being output by the sentiment analysis model. To fill in the Snowflake locator and region (these are similar to a unique ID for your Snowflake instance), navigate to your Snowflake account. Copy the locator and region from the URL and paste them into the corresponding fields in the connector setup in Quix. Lastly, input your username and password. Click \"Deploy\" on the Snowflake connector. If the credentials and connection details are correct, you\u2019ll see the \"Connected\" status in the log and will be redirected to your workspace. Congratulations! You built a no-code pipeline that filters and collects data from Twitter, transforms it with a HuggingFace model and delivers it to a Snowflake database. You can now go back over to Snowflake and find the \"Databases\" menu. Expand the \"demodata\" database and then find the tables under \"public\". Tip If you need help with this tutorial or want to chat about anything related to Quix or stream processing in general please come and say hi on The Stream , our slack community.","title":"Step five: delivery to your Snowflake database"},{"location":"platform/tutorials/quick-start/","text":"Quick Start This quick start gets you up and running with Quix. It shows how to set up a pipeline that ingests real-time Formula 1 data and sends an alert to your phone whenever a race car passes 300 kilometers per hour. It uses one source, one transformation and one destination from the Quix library along with a Twilio integration. You\u2019ll learn how to create a topic and how to connect, customize and write sources, transformations and destinations to set up a stream of real-time data. In the end, you\u2019ll be more comfortable using the platform and able to apply it in your own projects. When you\u2019re done with this quick start guide, your pipeline should look something like this: Requirements Free Quix account GitHub account Twilio account Installation No installations are required for cloud deployment In this guide Set up a Quix workspace Add a data source to your project Set up data transformation Deliver your data with Twilio Run the pipeline 1. Set up a Quix workspace Sign up for Quix . The free account provides enough resources to build a single streaming data project. You\u2019ll arrive in a workspace when you sign up. 2. Add a data source to your project from the Quix library The data sources in the Quix library are out-of-the-box code samples that you can plug and play or customize. It\u2019s an open source library, so you can also contribute your own when you\u2019re ready! To add a data source to your pipeline, click Add data source . This takes you to the library, which you can also access via GitHub . You\u2019ll use Formula 1 Data for this example. This source includes the speed, acceleration, brake usage and other detailed data from a real F1 car formatted into real-time data. Find the Formula 1 Data card in the Quix library and click Set up connector . Using the prepared data saves the time of importing or building data sets. Setting up a connector will simultaneously create a topic ( topics are channels that carry real-time data, one per data source) and bring the Formula 1 data stream into the topic. You can use the topic name that automatically populates the field or re-name it. Click Connect . Now the source is connected. You can go into the source and check the data in the live preview visualization. The data source includes more data types than needed, so you can click the tick boxes next to Speed and RPM under Select parameters or events . 3. Set up a data transformation using the no-code method You want your pipeline to look at the data and see when a vehicle goes faster than 300 kilometers per hour. This requires a threshold alert. Click Add transform service to open the library. Find the Threshold transformation and click Setup . This transformation connector generates an alert when a certain numeric threshold is crossed. The Project name, Input, Output, ParameterName and ThresholdValue will auto populate. Change ThresholdValue to 300 to receive an alert whenever the car passes the 300 milometers per hour point. Save by clicking Save as a project . Click Deploy . This brings up a dialog box with options to change variables, network and state. For this project, you can choose Service and leave the remaining configuration as is. Click Deploy . A service is any application code that continuously runs in a serverless environment. 4. Deliver your data using the Twilio destination Now that the data is ready to go, you need to tell it where to go. Click Add new next to \u201cDestinations\u201d on the workplace screen. This is where Twilio comes in to send speed alerts to your phone. Click Setup . Choose your threshold-alert-output as the input. You can find the numbers , account_sid , auth_token', and messaging_service_sid in your free Twilio account . Message_limit refers to the total number of messages you\u2019ll receive. It\u2019s set at two by default, as some message charges may apply. Click Save as project once you have added the requested Twilio information. 5. Run the pipeline Here\u2019s the fun part! The three dots (one in the upper-right corner of each card) indicate that everything works correctly. All you need to do is click Deploy to start receiving notifications on your phone whenever the car you\u2019re tracking goes faster than 300 kilometers per hour. Conclusion You\u2019ve successfully built a data pipeline that transforms one stream of data, which is just the beginning! Speak with a technical expert about how to set up and use Quix in your own projects. Additional resources The Stream community on Slack Stream processing glossary Developer docs main navigation","title":"Quick Start"},{"location":"platform/tutorials/quick-start/#quick-start","text":"This quick start gets you up and running with Quix. It shows how to set up a pipeline that ingests real-time Formula 1 data and sends an alert to your phone whenever a race car passes 300 kilometers per hour. It uses one source, one transformation and one destination from the Quix library along with a Twilio integration. You\u2019ll learn how to create a topic and how to connect, customize and write sources, transformations and destinations to set up a stream of real-time data. In the end, you\u2019ll be more comfortable using the platform and able to apply it in your own projects. When you\u2019re done with this quick start guide, your pipeline should look something like this: Requirements Free Quix account GitHub account Twilio account Installation No installations are required for cloud deployment In this guide Set up a Quix workspace Add a data source to your project Set up data transformation Deliver your data with Twilio Run the pipeline","title":"Quick Start"},{"location":"platform/tutorials/quick-start/#1-set-up-a-quix-workspace","text":"Sign up for Quix . The free account provides enough resources to build a single streaming data project. You\u2019ll arrive in a workspace when you sign up.","title":"1. Set up a Quix workspace"},{"location":"platform/tutorials/quick-start/#2-add-a-data-source-to-your-project-from-the-quix-library","text":"The data sources in the Quix library are out-of-the-box code samples that you can plug and play or customize. It\u2019s an open source library, so you can also contribute your own when you\u2019re ready! To add a data source to your pipeline, click Add data source . This takes you to the library, which you can also access via GitHub . You\u2019ll use Formula 1 Data for this example. This source includes the speed, acceleration, brake usage and other detailed data from a real F1 car formatted into real-time data. Find the Formula 1 Data card in the Quix library and click Set up connector . Using the prepared data saves the time of importing or building data sets. Setting up a connector will simultaneously create a topic ( topics are channels that carry real-time data, one per data source) and bring the Formula 1 data stream into the topic. You can use the topic name that automatically populates the field or re-name it. Click Connect . Now the source is connected. You can go into the source and check the data in the live preview visualization. The data source includes more data types than needed, so you can click the tick boxes next to Speed and RPM under Select parameters or events .","title":"2. Add a data source to your project from the Quix library"},{"location":"platform/tutorials/quick-start/#3-set-up-a-data-transformation-using-the-no-code-method","text":"You want your pipeline to look at the data and see when a vehicle goes faster than 300 kilometers per hour. This requires a threshold alert. Click Add transform service to open the library. Find the Threshold transformation and click Setup . This transformation connector generates an alert when a certain numeric threshold is crossed. The Project name, Input, Output, ParameterName and ThresholdValue will auto populate. Change ThresholdValue to 300 to receive an alert whenever the car passes the 300 milometers per hour point. Save by clicking Save as a project . Click Deploy . This brings up a dialog box with options to change variables, network and state. For this project, you can choose Service and leave the remaining configuration as is. Click Deploy . A service is any application code that continuously runs in a serverless environment.","title":"3. Set up a data transformation using the no-code method"},{"location":"platform/tutorials/quick-start/#4-deliver-your-data-using-the-twilio-destination","text":"Now that the data is ready to go, you need to tell it where to go. Click Add new next to \u201cDestinations\u201d on the workplace screen. This is where Twilio comes in to send speed alerts to your phone. Click Setup . Choose your threshold-alert-output as the input. You can find the numbers , account_sid , auth_token', and messaging_service_sid in your free Twilio account . Message_limit refers to the total number of messages you\u2019ll receive. It\u2019s set at two by default, as some message charges may apply. Click Save as project once you have added the requested Twilio information.","title":"4. Deliver your data using the Twilio destination"},{"location":"platform/tutorials/quick-start/#5-run-the-pipeline","text":"Here\u2019s the fun part! The three dots (one in the upper-right corner of each card) indicate that everything works correctly. All you need to do is click Deploy to start receiving notifications on your phone whenever the car you\u2019re tracking goes faster than 300 kilometers per hour. Conclusion You\u2019ve successfully built a data pipeline that transforms one stream of data, which is just the beginning! Speak with a technical expert about how to set up and use Quix in your own projects. Additional resources The Stream community on Slack Stream processing glossary Developer docs main navigation","title":"5. Run the pipeline"},{"location":"platform/tutorials/sentimentAnalysis/","text":"Sentiment analysis Build a real time sentiment analysis pipeline. Use the power and ease of Quix to process high volume tweets or user sourced conversation messages. Get Started To get started make sure you have a Quix account, signup for a completely free account at https://quix.io You\u2019ll also need a Twitter developer account. A rough guide to getting this is as follows: Register for an account here https://developer.twitter.com/en/apply-for-access Fill out the forms, there are quite a few! Wait for Twitter to provide you with your account. Once you have your account locate the \u201cKeys and Tokens\u201d page Save your bearer token for later Use one of the many guides on YouTube or try this one if you need some help creating your Twitter developer account and filling in all those forms. If you need any assistance, we\u2019re here to help in The Stream , our free Slack community. Library Most of the code you need has already been written for you and it\u2019s located in our library. We\u2019ll be referring to the library often so make sure you know where it is. Pipeline Services There are 4 stages to the pipeline you will build in this tutorial. 1. Twitter Data Source Our Twitter connector, pre-built to save you time. 2. Twitter to chat Convert Twitter text to chat messages, so they can be used alongside manually entered chat messages. 3. Sentiment analysis Do the real work of analyzing chat and tweet sentiment. 4. Web UI A web UI allowing you to enter and see messages as well as see the sentiment changes in real time. Build It This guide will show you how to deploy each stage of the processing pipeline, starting with the Web UI. 1. Web UI Follow these steps to deploy the Web UI. Navigate to the Library and locate \u201cSentiment Demo UI\u201d. Click \u201cEdit code\u201d Click \u201cSave as project\u201d The code for this Angular UI is now saved to your workspace Tag the code and deploy the UI Click the +tag button at the top of any code file Enter v1 and press enter Click Deploy near the top right corner In the deployment dialog, select v1 under the \u201cVersion Tag\u201d This is the tag you just created Click \u201cService\u201d in \u201cDeployment Settings\u201d This ensures the service runs continuously Click the toggle in \u201cPublic Access\u201d This enables access from anywhere on the internet Click \u201cDeploy\u201d The UI will stream data from the \u201csentiment\u201d and \u201cmessages\u201d topics as well as send messages to the \u201cmessages\u201d topic. Once deployed, click the service tile Click the \u201cPublic URL\u201d Info This is the user interface for the demo. This screenshot shows the view you\u2019ll see after creating a \u201croom\u201d to chat in. 2. Sentiment analysis Follow these steps to deploy the sentiment analysis stage. Navigate to the Library and locate \u201cSentiment analysis\u201d transformation. Click \u201cEdit code\u201d Click \u201cSave as project\u201d The code for this transformation is now saved to your workspace Locate main.py Locate the line of code that creates the output stream output_stream = output_topic . create_stream ( input_stream . stream_id ) Append \"-output\" to the stream id. This will ensure the Web UI is able to locate the sentiment being output by this transformation service. output_stream = output_topic . create_stream ( input_stream . stream_id + \"-output\" ) Tag the code and deploy the service Click the +tag button at the top of any code file Enter v1 and press enter Click Deploy near the top right corner In the deployment dialog, select v1 under the \u201cVersion Tag\u201d This is the tag you just created Click \u201cService\u201d in \u201cDeployment Settings\u201d This ensures the service runs continuously Click \u201cDeploy\u201d This service receives data from the \u201cmessages\u201d topic and streams data out to the \u201csentiment\u201d topic. . You can now go to the public URL of the Web UI project you deployed in step one. . Enter values for \u201cRoom\u201d and \u201cName\u201d . Click connect. You can now enter \u201cchat\u201d messages. Info The messages will be passed to the sentiment analysis service you deployed in step two. The sentiment is returned to the Web UI and displayed both in the chart and next to the comment in the chat window by colorizing the chat user\u2019s name. 3. Tweet to chat conversion Follow these steps to deploy the tweet-to-chat conversion stage. Navigate to the Library and apply the following filters Languages = Python Pipeline Stage = Transformation Type = Basic templates Select \u201cEmpty template- Transformation\u201d. Click \u201cEdit code\u201d Change the name to \u201ctweet-to-chat\u201d Change the input to \u201ctwitter-data\u201d by either selecting it or typing it. Ensure the output is set to \u201cmessages\u201d Click \u201cSave as project\u201d The code for this transformation is now saved to your workspace Locate main.py Locate the line of code that creates the output stream output_stream = output_topic . create_stream ( input_stream . stream_id ) Change this line to get or create a stream called \u201ctweets\u201d output_stream = output_topic . get_or_create_stream ( \"tweets\" ) Now locate quix_function.py Alter the on_pandas_frame_handler to match the code below def on_pandas_frame_handler ( self , df : pd . DataFrame ): for index , row in df . iterrows (): text = row [ \"text\" ] self . output_stream . events . add_timestamp_nanoseconds ( row . time ) \\ . add_tag ( \"name\" , \"Twitter\" ) \\ . add_value ( \"chat-message\" , text ) \\ . write () Info This will take \u201ctext\u201d from incoming tweets and stream them to the output topics tweets stream as event values with a key of \u201cchat-message\u201d which the other stages of the pipeline will recognize. Now tag the code and deploy the service with these steps: Click the +tag button at the top of any code file Enter v1 and press enter Click Deploy near the top right corner In the deployment dialog, select v1 under the \u201cVersion Tag\u201d This is the tag you just created . Click \u201cService\u201d in \u201cDeployment Settings\u201d . This ensures the service runs continuously . Click \u201cDeploy\u201d This service receives data from the \u201ctwitter-data\u201d topic and streams data out to the \u201cmessages\u201d topic. Success You now have a service that is ready to receive tweets and pass them onto the sentiment processing stage. 4. Twitter Data Source Follow these steps to deploy the Web UI. Navigate to the Library and locate \u201cTwitter Data - Source\u201d. Click \u201cSetup & deploy\u201d Enter your Twitter bearer token into the appropriate field Click \u201cDeploy\u201d This service receives data from Twitter and streams it to the \u201ctwitter-data\u201d topic. NOTE: that the default Twitter search criteria is looking for Bitcoin tweets, it\u2019s a high traffic subject and great for the demo. Feel free to change this once you\u2019ve got the demo working. . Locate the Web UI you deployed in step one . Navigate to the lobby . Enter \u201ctweets\u201d for the chatroom and provide your name in the \u201cname\u201d field . Click connect You will see Bitcoin tweets arriving in the chat along with the calculated average sentiment in a chart. Success Your pipeline is now complete, you can send and view chat messages, receive tweets and analyze the sentiment of all of the messages. Share the QR code with colleagues and friends to talk about anything you like while Quix analyzes the sentiment in the room in real time. Next Steps You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, samples and examples. Now you can build your own connectors and apps and contribute by going to our Github here , forking our library repo and submitting your code, updates and ideas. What will you build, let us know. We\u2019d love to feature your project or use case in our newsletter. If you need any assistance, we\u2019re here to help in The Stream , our free Slack community. Goodbye, for now.. We hope you enjoyed this tutorial on how to Build a real time chat and tweet sentiment analysis pipeline. If you have any questions or feedback please contact us on The Stream. Thank you and goodbye for now!","title":"Sentiment analysis"},{"location":"platform/tutorials/sentimentAnalysis/#sentiment-analysis","text":"Build a real time sentiment analysis pipeline. Use the power and ease of Quix to process high volume tweets or user sourced conversation messages.","title":"Sentiment analysis"},{"location":"platform/tutorials/sentimentAnalysis/#get-started","text":"To get started make sure you have a Quix account, signup for a completely free account at https://quix.io You\u2019ll also need a Twitter developer account. A rough guide to getting this is as follows: Register for an account here https://developer.twitter.com/en/apply-for-access Fill out the forms, there are quite a few! Wait for Twitter to provide you with your account. Once you have your account locate the \u201cKeys and Tokens\u201d page Save your bearer token for later Use one of the many guides on YouTube or try this one if you need some help creating your Twitter developer account and filling in all those forms. If you need any assistance, we\u2019re here to help in The Stream , our free Slack community.","title":"Get Started"},{"location":"platform/tutorials/sentimentAnalysis/#library","text":"Most of the code you need has already been written for you and it\u2019s located in our library. We\u2019ll be referring to the library often so make sure you know where it is.","title":"Library"},{"location":"platform/tutorials/sentimentAnalysis/#pipeline-services","text":"There are 4 stages to the pipeline you will build in this tutorial.","title":"Pipeline Services"},{"location":"platform/tutorials/sentimentAnalysis/#1-twitter-data-source","text":"Our Twitter connector, pre-built to save you time.","title":"1. Twitter Data Source"},{"location":"platform/tutorials/sentimentAnalysis/#2-twitter-to-chat","text":"Convert Twitter text to chat messages, so they can be used alongside manually entered chat messages.","title":"2. Twitter to chat"},{"location":"platform/tutorials/sentimentAnalysis/#3-sentiment-analysis","text":"Do the real work of analyzing chat and tweet sentiment.","title":"3. Sentiment analysis"},{"location":"platform/tutorials/sentimentAnalysis/#4-web-ui","text":"A web UI allowing you to enter and see messages as well as see the sentiment changes in real time.","title":"4. Web UI"},{"location":"platform/tutorials/sentimentAnalysis/#build-it","text":"This guide will show you how to deploy each stage of the processing pipeline, starting with the Web UI.","title":"Build It"},{"location":"platform/tutorials/sentimentAnalysis/#1-web-ui","text":"Follow these steps to deploy the Web UI. Navigate to the Library and locate \u201cSentiment Demo UI\u201d. Click \u201cEdit code\u201d Click \u201cSave as project\u201d The code for this Angular UI is now saved to your workspace","title":"1. Web UI"},{"location":"platform/tutorials/sentimentAnalysis/#tag-the-code-and-deploy-the-ui","text":"Click the +tag button at the top of any code file Enter v1 and press enter Click Deploy near the top right corner In the deployment dialog, select v1 under the \u201cVersion Tag\u201d This is the tag you just created Click \u201cService\u201d in \u201cDeployment Settings\u201d This ensures the service runs continuously Click the toggle in \u201cPublic Access\u201d This enables access from anywhere on the internet Click \u201cDeploy\u201d The UI will stream data from the \u201csentiment\u201d and \u201cmessages\u201d topics as well as send messages to the \u201cmessages\u201d topic. Once deployed, click the service tile Click the \u201cPublic URL\u201d Info This is the user interface for the demo. This screenshot shows the view you\u2019ll see after creating a \u201croom\u201d to chat in.","title":"Tag the code and deploy the UI"},{"location":"platform/tutorials/sentimentAnalysis/#2-sentiment-analysis","text":"Follow these steps to deploy the sentiment analysis stage. Navigate to the Library and locate \u201cSentiment analysis\u201d transformation. Click \u201cEdit code\u201d Click \u201cSave as project\u201d The code for this transformation is now saved to your workspace Locate main.py Locate the line of code that creates the output stream output_stream = output_topic . create_stream ( input_stream . stream_id ) Append \"-output\" to the stream id. This will ensure the Web UI is able to locate the sentiment being output by this transformation service. output_stream = output_topic . create_stream ( input_stream . stream_id + \"-output\" )","title":"2. Sentiment analysis"},{"location":"platform/tutorials/sentimentAnalysis/#tag-the-code-and-deploy-the-service","text":"Click the +tag button at the top of any code file Enter v1 and press enter Click Deploy near the top right corner In the deployment dialog, select v1 under the \u201cVersion Tag\u201d This is the tag you just created Click \u201cService\u201d in \u201cDeployment Settings\u201d This ensures the service runs continuously Click \u201cDeploy\u201d This service receives data from the \u201cmessages\u201d topic and streams data out to the \u201csentiment\u201d topic. . You can now go to the public URL of the Web UI project you deployed in step one. . Enter values for \u201cRoom\u201d and \u201cName\u201d . Click connect. You can now enter \u201cchat\u201d messages. Info The messages will be passed to the sentiment analysis service you deployed in step two. The sentiment is returned to the Web UI and displayed both in the chart and next to the comment in the chat window by colorizing the chat user\u2019s name.","title":"Tag the code and deploy the service"},{"location":"platform/tutorials/sentimentAnalysis/#3-tweet-to-chat-conversion","text":"Follow these steps to deploy the tweet-to-chat conversion stage. Navigate to the Library and apply the following filters Languages = Python Pipeline Stage = Transformation Type = Basic templates Select \u201cEmpty template- Transformation\u201d. Click \u201cEdit code\u201d Change the name to \u201ctweet-to-chat\u201d Change the input to \u201ctwitter-data\u201d by either selecting it or typing it. Ensure the output is set to \u201cmessages\u201d Click \u201cSave as project\u201d The code for this transformation is now saved to your workspace Locate main.py Locate the line of code that creates the output stream output_stream = output_topic . create_stream ( input_stream . stream_id ) Change this line to get or create a stream called \u201ctweets\u201d output_stream = output_topic . get_or_create_stream ( \"tweets\" ) Now locate quix_function.py Alter the on_pandas_frame_handler to match the code below def on_pandas_frame_handler ( self , df : pd . DataFrame ): for index , row in df . iterrows (): text = row [ \"text\" ] self . output_stream . events . add_timestamp_nanoseconds ( row . time ) \\ . add_tag ( \"name\" , \"Twitter\" ) \\ . add_value ( \"chat-message\" , text ) \\ . write () Info This will take \u201ctext\u201d from incoming tweets and stream them to the output topics tweets stream as event values with a key of \u201cchat-message\u201d which the other stages of the pipeline will recognize.","title":"3. Tweet to chat conversion"},{"location":"platform/tutorials/sentimentAnalysis/#now-tag-the-code-and-deploy-the-service-with-these-steps","text":"Click the +tag button at the top of any code file Enter v1 and press enter Click Deploy near the top right corner In the deployment dialog, select v1 under the \u201cVersion Tag\u201d This is the tag you just created . Click \u201cService\u201d in \u201cDeployment Settings\u201d . This ensures the service runs continuously . Click \u201cDeploy\u201d This service receives data from the \u201ctwitter-data\u201d topic and streams data out to the \u201cmessages\u201d topic. Success You now have a service that is ready to receive tweets and pass them onto the sentiment processing stage.","title":"Now tag the code and deploy the service with these steps:"},{"location":"platform/tutorials/sentimentAnalysis/#4-twitter-data-source","text":"Follow these steps to deploy the Web UI. Navigate to the Library and locate \u201cTwitter Data - Source\u201d. Click \u201cSetup & deploy\u201d Enter your Twitter bearer token into the appropriate field Click \u201cDeploy\u201d This service receives data from Twitter and streams it to the \u201ctwitter-data\u201d topic. NOTE: that the default Twitter search criteria is looking for Bitcoin tweets, it\u2019s a high traffic subject and great for the demo. Feel free to change this once you\u2019ve got the demo working. . Locate the Web UI you deployed in step one . Navigate to the lobby . Enter \u201ctweets\u201d for the chatroom and provide your name in the \u201cname\u201d field . Click connect You will see Bitcoin tweets arriving in the chat along with the calculated average sentiment in a chart. Success Your pipeline is now complete, you can send and view chat messages, receive tweets and analyze the sentiment of all of the messages. Share the QR code with colleagues and friends to talk about anything you like while Quix analyzes the sentiment in the room in real time.","title":"4. Twitter Data Source"},{"location":"platform/tutorials/sentimentAnalysis/#next-steps","text":"You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, samples and examples. Now you can build your own connectors and apps and contribute by going to our Github here , forking our library repo and submitting your code, updates and ideas. What will you build, let us know. We\u2019d love to feature your project or use case in our newsletter. If you need any assistance, we\u2019re here to help in The Stream , our free Slack community.","title":"Next Steps"},{"location":"platform/tutorials/sentimentAnalysis/#goodbye-for-now","text":"We hope you enjoyed this tutorial on how to Build a real time chat and tweet sentiment analysis pipeline. If you have any questions or feedback please contact us on The Stream. Thank you and goodbye for now!","title":"Goodbye, for now.."},{"location":"platform/tutorials/telemetry-data/","text":"Telemetry data Stream and visualize real-time telemetry data with an Android app and Streamlit Learn how to build an end-to-end telemetry data pipeline for IoT applications. Use our Android companion app to stream sensor data from your phone and visualize it in a Streamlit app. Get Started To get started make sure you have a Quix account, signup for a completely free account at https://quix.io You will need an Android mobile phone for this tutorial (we're working on the Apple app). If you need any assistance, we're here to help in The Stream , our free Slack community. Library Most of the code you'll need has already been written. It lives in our library, which is accessible from inside the Quix portal or directly via our open source Github repo. We'll be referring to the library often so make sure you know where it is. Components Android App - Our companion app for collecting real-time sensor data from your phone. It'spre-built and published to the Play store to save you time. You can also access the source code in our Github repo . Streamlit App - See your location on a map and other activity metrics. QR Settings Share - A lightweight UI and API for sharing settings with the Android app. Build It This guide will show you how to deploy each of the components, starting with QR Settings Share. 1. QR Settings Share Follow these steps to deploy the QR Settings Share. Navigate to the Library and locate \"QR Settings Share\" Click \"Setup & deploy\" Click \"Deploy\" Info The service will be deployed to your workspace Open the UI with these steps Once deployed, click the service tile Click the \"Public URL\" Append the following querystring to the url in the address bar ?topic=phone-data&notificationsTopic=notifications-topic These are needed by the Android app and will be passed to it via the QR code you'll generate in a moment You can now enter a username and device name into the relevant inputs These can be anything! But sensible values will help understand which token belongs to which user Click OK You can now click Generate Token Several things happened when you clicked the button. A token was generated in Quix with an expiry time in around 1 month. The token was published to an API and a link to the token was generated. The QR shown on screen is a short-lived link to the longer lasting token. Info Scanning the token with the Android App will allow the app to receive its configuration information without you having to enter any long token strings by hand. 2. Android App It's time to install the Android app! Go to the Google Play store and search for \"Quix Tracker App\" ensuring it's the one published by us. [Screenshot of Play store coming soon] Install the app. Tap the menu hamburger Tap settings Tap SCAN QR CODE Generate a new QR code in the QR Settings Share website you deployed in step 1 Scan the QR code with your device The Token, Workspace and topics should now be configured Tap the menu hamburger once again Tap Dashboard Tap START at the bottom of the screen Your device is now streaming to the topic called 'phone-data' in your workspace These parameters are being streamed from your phone: Accuracy Altitude BatteryLevel BatteryPowerSource BatteryState EnergySaverStatus gForceX gForceY gForceZ Heading Latitude LogInfo Longitude Speed Tip Leave the app running until you've completed the next step. 3. Streamlit app Explore the data inside Quix and then deploy a frontend app. Within Quix: Click Data Explorer on the left hand menu Select the \"phone-data\" topic You should have at least one stream, select it Select as many parameters as you want. The gForce ones are great for this tutorial You can see the gForce from your device displayed on the waveform in real time Deploy an app: Click Library on the left hand menu Search for Streamlit Dashboard Click Preview code Click Edit code Ensure phone-data is selected for the input field Click Save as project The code for the dashboard is now saved to your workspace. You can now edit it to ensure it works with the data coming from the device Locate the following line in streamlit_file.py st . line_chart ( local_df [[ \"datetime\" , 'Speed' , 'EngineRPM' ]] . set_index ( \"datetime\" )) Replace 'Speed' with 'gForceX' Replace 'EngineRPM' with 'gForceY' Remove the following code block with fig_col2 : st . markdown ( \"### Chart 2 Title\" ) st . line_chart ( local_df [[ \"datetime\" , 'Gear' , 'Brake' ]] . set_index ( \"datetime\" )) Click Run near the top right corner of the code editor The dashboard will run right here in the development environment Click the icon that appears next to the project name (circled here in green) Once the dashboard has loaded you will see sensor data from your device in real time Below this is also the raw data showing all the values from the device Close the dashboard The dashboard you were just using was deployed to a temporary URL in the development area You need to deploy it to its permanent home! Click the Deploy button in the top right corner. Click Service in the Deployment Settings Click the toggle button under Public Access Click Deploy You will be redirected to the home page Once the dashboard has been built and deployed click the deployment tile Click the Public URL You are now looking at your dashboard in its permanent home Be aware that there is no security on the dashboard Next Steps You've just connected your mobile device to Quix and deployed a Streamlit app to view the data in real time. Now try showing some different parameters in the app, or build a transformation in Quix to make better use of the raw data. What will you build? Let us know. We'd love to feature your project or use case in our newsletter. If you need any assistance, we're here to help in The Stream , our free Slack community. Ciao for now! We hope you enjoyed this tutorial. If you have any questions or feedback please contact us on The Stream. Thank you.!","title":"Telemetry data"},{"location":"platform/tutorials/telemetry-data/#telemetry-data","text":"","title":"Telemetry data"},{"location":"platform/tutorials/telemetry-data/#stream-and-visualize-real-time-telemetry-data-with-an-android-app-and-streamlit","text":"Learn how to build an end-to-end telemetry data pipeline for IoT applications. Use our Android companion app to stream sensor data from your phone and visualize it in a Streamlit app.","title":"Stream and visualize real-time telemetry data with an Android app and Streamlit"},{"location":"platform/tutorials/telemetry-data/#get-started","text":"To get started make sure you have a Quix account, signup for a completely free account at https://quix.io You will need an Android mobile phone for this tutorial (we're working on the Apple app). If you need any assistance, we're here to help in The Stream , our free Slack community.","title":"Get Started"},{"location":"platform/tutorials/telemetry-data/#library","text":"Most of the code you'll need has already been written. It lives in our library, which is accessible from inside the Quix portal or directly via our open source Github repo. We'll be referring to the library often so make sure you know where it is.","title":"Library"},{"location":"platform/tutorials/telemetry-data/#components","text":"Android App - Our companion app for collecting real-time sensor data from your phone. It'spre-built and published to the Play store to save you time. You can also access the source code in our Github repo . Streamlit App - See your location on a map and other activity metrics. QR Settings Share - A lightweight UI and API for sharing settings with the Android app.","title":"Components"},{"location":"platform/tutorials/telemetry-data/#build-it","text":"This guide will show you how to deploy each of the components, starting with QR Settings Share.","title":"Build It"},{"location":"platform/tutorials/telemetry-data/#1-qr-settings-share","text":"Follow these steps to deploy the QR Settings Share. Navigate to the Library and locate \"QR Settings Share\" Click \"Setup & deploy\" Click \"Deploy\" Info The service will be deployed to your workspace Open the UI with these steps Once deployed, click the service tile Click the \"Public URL\" Append the following querystring to the url in the address bar ?topic=phone-data&notificationsTopic=notifications-topic These are needed by the Android app and will be passed to it via the QR code you'll generate in a moment You can now enter a username and device name into the relevant inputs These can be anything! But sensible values will help understand which token belongs to which user Click OK You can now click Generate Token Several things happened when you clicked the button. A token was generated in Quix with an expiry time in around 1 month. The token was published to an API and a link to the token was generated. The QR shown on screen is a short-lived link to the longer lasting token. Info Scanning the token with the Android App will allow the app to receive its configuration information without you having to enter any long token strings by hand.","title":"1. QR Settings Share"},{"location":"platform/tutorials/telemetry-data/#2-android-app","text":"It's time to install the Android app! Go to the Google Play store and search for \"Quix Tracker App\" ensuring it's the one published by us. [Screenshot of Play store coming soon] Install the app. Tap the menu hamburger Tap settings Tap SCAN QR CODE Generate a new QR code in the QR Settings Share website you deployed in step 1 Scan the QR code with your device The Token, Workspace and topics should now be configured Tap the menu hamburger once again Tap Dashboard Tap START at the bottom of the screen Your device is now streaming to the topic called 'phone-data' in your workspace These parameters are being streamed from your phone: Accuracy Altitude BatteryLevel BatteryPowerSource BatteryState EnergySaverStatus gForceX gForceY gForceZ Heading Latitude LogInfo Longitude Speed Tip Leave the app running until you've completed the next step.","title":"2. Android App"},{"location":"platform/tutorials/telemetry-data/#3-streamlit-app","text":"Explore the data inside Quix and then deploy a frontend app. Within Quix: Click Data Explorer on the left hand menu Select the \"phone-data\" topic You should have at least one stream, select it Select as many parameters as you want. The gForce ones are great for this tutorial You can see the gForce from your device displayed on the waveform in real time Deploy an app: Click Library on the left hand menu Search for Streamlit Dashboard Click Preview code Click Edit code Ensure phone-data is selected for the input field Click Save as project The code for the dashboard is now saved to your workspace. You can now edit it to ensure it works with the data coming from the device Locate the following line in streamlit_file.py st . line_chart ( local_df [[ \"datetime\" , 'Speed' , 'EngineRPM' ]] . set_index ( \"datetime\" )) Replace 'Speed' with 'gForceX' Replace 'EngineRPM' with 'gForceY' Remove the following code block with fig_col2 : st . markdown ( \"### Chart 2 Title\" ) st . line_chart ( local_df [[ \"datetime\" , 'Gear' , 'Brake' ]] . set_index ( \"datetime\" )) Click Run near the top right corner of the code editor The dashboard will run right here in the development environment Click the icon that appears next to the project name (circled here in green) Once the dashboard has loaded you will see sensor data from your device in real time Below this is also the raw data showing all the values from the device Close the dashboard The dashboard you were just using was deployed to a temporary URL in the development area You need to deploy it to its permanent home! Click the Deploy button in the top right corner. Click Service in the Deployment Settings Click the toggle button under Public Access Click Deploy You will be redirected to the home page Once the dashboard has been built and deployed click the deployment tile Click the Public URL You are now looking at your dashboard in its permanent home Be aware that there is no security on the dashboard","title":"3. Streamlit app"},{"location":"platform/tutorials/telemetry-data/#next-steps","text":"You've just connected your mobile device to Quix and deployed a Streamlit app to view the data in real time. Now try showing some different parameters in the app, or build a transformation in Quix to make better use of the raw data. What will you build? Let us know. We'd love to feature your project or use case in our newsletter. If you need any assistance, we're here to help in The Stream , our free Slack community.","title":"Next Steps"},{"location":"platform/tutorials/telemetry-data/#ciao-for-now","text":"We hope you enjoyed this tutorial. If you have any questions or feedback please contact us on The Stream. Thank you.!","title":"Ciao for now!"},{"location":"sdk/connect/","text":"Connect to Quix The Quix SDK comes with a streaming client that enables you to connect to Quix easily, to read data from Quix, and to write data to Quix. The streaming client manages the connections between your application and Quix and makes sure that the data is delivered reliably to and from your application. Using QuixStreamingClient Starting with 0.4.0, we\u2019re offering QuixStreamingClient, which handles the cumbersome part of setting up your streaming credentials using the Quix Api. When you\u2019re running the app in the online IDE or as a deployment , all you have to do is the following: Initialize the Client Python C# client = QuixStreamingClient () var client = new Quix . Sdk . Streaming . QuixStreamingClient (); If you wish to run the same code locally, you\u2019ll have to provide an OAuth2.0 bearer token. We have created a purpose made token for this, called SDK token . Once you have the token you will have to provide it as an argument to QuixStreamingClient or set Quix__Sdk__Token environment variable. Initialize the Client with an SDK Token Python C# client = QuixStreamingClient ( 'your_token' ) var client = new Quix . Sdk . Streaming . QuixStreamingClient ( \"your_token\" ); Using the streaming client is another way to talk with a broker. It is a Kafka specific client implementation that requires some explicit configuration but allows you to connect to any Kafka cluster even outside Quix platform. It involves the following steps: Obtain a client certificate and credentials (security context) for your application. Create a streaming client. A security context consists of a client certificate, username, and password. Quix generates these automatically for you when you create a project using the templates provided on the Quix Portal. If necessary, you can download these credentials separately from Topics option on Quix Portal. The following code shows you how to set up the SecurityOptions for your connection and how to create a StreamingClient instance to start Reading and Writing real-time time series data with Quix: Set Up the Security Options for Your Connection Python C# Javascript security = SecurityOptions ( CERTIFICATES_FOLDER , QUIX_USER , QUIX_PASSWORD ) client = StreamingClient ( 'kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093' , security ) var security = new SecurityOptions ( CERTIFICATES_FOLDER , QUIX_USER , QUIX_PASSWORD ); var client = new Quix . Sdk . Streaming . StreamingClient ( \"kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093\" , security ); Quix web APIs are secured with OAuth2.0 bearer scheme. Therefore, all HTTP requests to Quix must contain a valid bearer token. You can generate a personal access token (PAT) for use as a bearer token from the portal by following the following steps. Navigate to your profile by clicking on your avatar and selecting \"Profile\" from the drop-down menu. Select \"Personal Access Tokens\" tab on the profile page. Click on \"Generate Token\" button to open a dialog to set a name and an expiry date for the PAT and click on \"Create\" to generate the PAT. Tip For your convenience, when you create a new project on the Quix platform, the credentials are generated and set in the code for you. However, it is good practice to move them out of the code to a more secure location like environment variables or a keystore, depending on your development platform. When you deploy your application to Quix, you can store them on Quix as environment variables.","title":"Connect to Quix"},{"location":"sdk/connect/#connect-to-quix","text":"The Quix SDK comes with a streaming client that enables you to connect to Quix easily, to read data from Quix, and to write data to Quix. The streaming client manages the connections between your application and Quix and makes sure that the data is delivered reliably to and from your application.","title":"Connect to Quix"},{"location":"sdk/connect/#using-quixstreamingclient","text":"Starting with 0.4.0, we\u2019re offering QuixStreamingClient, which handles the cumbersome part of setting up your streaming credentials using the Quix Api. When you\u2019re running the app in the online IDE or as a deployment , all you have to do is the following:","title":"Using QuixStreamingClient"},{"location":"sdk/connect/#initialize-the-client","text":"Python C# client = QuixStreamingClient () var client = new Quix . Sdk . Streaming . QuixStreamingClient (); If you wish to run the same code locally, you\u2019ll have to provide an OAuth2.0 bearer token. We have created a purpose made token for this, called SDK token . Once you have the token you will have to provide it as an argument to QuixStreamingClient or set Quix__Sdk__Token environment variable.","title":"Initialize the Client"},{"location":"sdk/connect/#initialize-the-client-with-an-sdk-token","text":"Python C# client = QuixStreamingClient ( 'your_token' ) var client = new Quix . Sdk . Streaming . QuixStreamingClient ( \"your_token\" ); Using the streaming client is another way to talk with a broker. It is a Kafka specific client implementation that requires some explicit configuration but allows you to connect to any Kafka cluster even outside Quix platform. It involves the following steps: Obtain a client certificate and credentials (security context) for your application. Create a streaming client. A security context consists of a client certificate, username, and password. Quix generates these automatically for you when you create a project using the templates provided on the Quix Portal. If necessary, you can download these credentials separately from Topics option on Quix Portal. The following code shows you how to set up the SecurityOptions for your connection and how to create a StreamingClient instance to start Reading and Writing real-time time series data with Quix:","title":"Initialize the Client with an SDK Token"},{"location":"sdk/connect/#set-up-the-security-options-for-your-connection","text":"Python C# Javascript security = SecurityOptions ( CERTIFICATES_FOLDER , QUIX_USER , QUIX_PASSWORD ) client = StreamingClient ( 'kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093' , security ) var security = new SecurityOptions ( CERTIFICATES_FOLDER , QUIX_USER , QUIX_PASSWORD ); var client = new Quix . Sdk . Streaming . StreamingClient ( \"kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093\" , security ); Quix web APIs are secured with OAuth2.0 bearer scheme. Therefore, all HTTP requests to Quix must contain a valid bearer token. You can generate a personal access token (PAT) for use as a bearer token from the portal by following the following steps. Navigate to your profile by clicking on your avatar and selecting \"Profile\" from the drop-down menu. Select \"Personal Access Tokens\" tab on the profile page. Click on \"Generate Token\" button to open a dialog to set a name and an expiry date for the PAT and click on \"Create\" to generate the PAT. Tip For your convenience, when you create a new project on the Quix platform, the credentials are generated and set in the code for you. However, it is good practice to move them out of the code to a more secure location like environment variables or a keystore, depending on your development platform. When you deploy your application to Quix, you can store them on Quix as environment variables.","title":"Set Up the Security Options for Your Connection"},{"location":"sdk/docker-setup/","text":"Set up docker environment Docker is an alternative to the classic environment setup for local development ( see Set up your local IDE ) Note Docker knowledge is not required to develop or deploy applications with Quix. This guide is only for people that prefer using local development environments using Docker. It enables you to install and run your project in isolation from the rest of the system which provides the advantage of removing system dependencies and conflicts. Isolation is achieved by creating a lightweight container (Docker Container) which behaves like a virtual machine (in our case Ubuntu Linux). Install Prerequisities In order to use the Quix SDK in Docker you need to have installed these prerequisities. Docker (tested on version 20.10.17) Docker Compose (tested on version 1.29.2) Install Docker ( step 1 ) To install the Docker on your environment you need to follow this guide here ( https://docs.docker.com/get-docker/ ) . Note On Windows we tested this setup using the WSL2 backend. Install docker-compose ( step 2 ) We\u2019ll be using the docker-compose tool which is designed for easy configuration of local Docker setups. To install docker-compose, please follow the guide here ( https://docs.docker.com/compose/install/ ) . Note If you are on Windows then you can skip this step because the Docker installation package from the step 1 already contains the docker-compose tool. Start with downloading a couple of things Download the project then navigate to your solution\u2019s root folder. Navigate to quix-library and download /docker/ folder content. Build and run project Open a command line within your new docker folder. You can start the build by running the following command. docker-compose run --rm server Note On the first run the compile script may take while (around 10 minutes) to build all the project dependencies. The subsequent builds will be much faster. Using the docker file provided, you should now be in a running server, which has all requirements installed for Quix. As the running image is nothing more than the quixpythonbaseimage with your code folder mounted at /app , in order to get your application working, you\u2019ll need to install your python requirements. You can do this using the following, executed in the /app folder python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ Use the resulting environment as you would your own machine, such as run your python application by executing python3 main.py Note As /apps folder is a mounted directory, any file or folder change in the container will be synced to your original folder in your machine and vice-versa. Note As your environment variables will greatly depend on what your application needs, make sure to update docker/.env as needed. By default all values are placeholder and this might be something you need to configure or add to before the application can correctly run. Several of these environment values could be considered \"secrets\", therefore be mindful of what you end up committing to your repository. Additional documentation To get the additional information on Docker and Docker compose commands please follow up with the documentation: Docker documentation ( https://docs.docker.com/reference/ ) Docker compose documentation ( https://docs.docker.com/compose/ )","title":"Set up docker environment"},{"location":"sdk/docker-setup/#set-up-docker-environment","text":"Docker is an alternative to the classic environment setup for local development ( see Set up your local IDE ) Note Docker knowledge is not required to develop or deploy applications with Quix. This guide is only for people that prefer using local development environments using Docker. It enables you to install and run your project in isolation from the rest of the system which provides the advantage of removing system dependencies and conflicts. Isolation is achieved by creating a lightweight container (Docker Container) which behaves like a virtual machine (in our case Ubuntu Linux).","title":"Set up docker environment"},{"location":"sdk/docker-setup/#install-prerequisities","text":"In order to use the Quix SDK in Docker you need to have installed these prerequisities. Docker (tested on version 20.10.17) Docker Compose (tested on version 1.29.2)","title":"Install Prerequisities"},{"location":"sdk/docker-setup/#install-docker-step-1","text":"To install the Docker on your environment you need to follow this guide here ( https://docs.docker.com/get-docker/ ) . Note On Windows we tested this setup using the WSL2 backend.","title":"Install Docker ( step 1 )"},{"location":"sdk/docker-setup/#install-docker-compose-step-2","text":"We\u2019ll be using the docker-compose tool which is designed for easy configuration of local Docker setups. To install docker-compose, please follow the guide here ( https://docs.docker.com/compose/install/ ) . Note If you are on Windows then you can skip this step because the Docker installation package from the step 1 already contains the docker-compose tool.","title":"Install docker-compose ( step 2 )"},{"location":"sdk/docker-setup/#start-with-downloading-a-couple-of-things","text":"Download the project then navigate to your solution\u2019s root folder. Navigate to quix-library and download /docker/ folder content.","title":"Start with downloading a couple of things"},{"location":"sdk/docker-setup/#build-and-run-project","text":"Open a command line within your new docker folder. You can start the build by running the following command. docker-compose run --rm server Note On the first run the compile script may take while (around 10 minutes) to build all the project dependencies. The subsequent builds will be much faster. Using the docker file provided, you should now be in a running server, which has all requirements installed for Quix. As the running image is nothing more than the quixpythonbaseimage with your code folder mounted at /app , in order to get your application working, you\u2019ll need to install your python requirements. You can do this using the following, executed in the /app folder python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ Use the resulting environment as you would your own machine, such as run your python application by executing python3 main.py Note As /apps folder is a mounted directory, any file or folder change in the container will be synced to your original folder in your machine and vice-versa. Note As your environment variables will greatly depend on what your application needs, make sure to update docker/.env as needed. By default all values are placeholder and this might be something you need to configure or add to before the application can correctly run. Several of these environment values could be considered \"secrets\", therefore be mindful of what you end up committing to your repository.","title":"Build and run project"},{"location":"sdk/docker-setup/#additional-documentation","text":"To get the additional information on Docker and Docker compose commands please follow up with the documentation: Docker documentation ( https://docs.docker.com/reference/ ) Docker compose documentation ( https://docs.docker.com/compose/ )","title":"Additional documentation"},{"location":"sdk/introduction/","text":"Introduction The Quix SDK makes it quick and easy to develop streaming applications. It\u2019s designed to be used for high performance telemetry services where you need to process high volumes of data in a nanosecond response time. The SDK is available for Python and C#. Using the Quix SDK, you can: Write time-series data to a Kafka Topic Read time-series data from a Kafka Topic Process data by reading it from one Topic and writing the results to another one. To support these operations, the SDK provides several useful features out of the box, and solves all the common problems you should face when developing real-time streaming applications: Streaming context The Quix SDK handles stream contexts for you, so all the data from one data source is bundled in the same scope. This allows you to attach metadata to streams. The SDK simplifies the processing of streams by providing callbacks on the reading side. When processing stream data, you can identify data from different streams more easily than with the key-value approach used by other technologies. Refer to the Streaming context section of this documentation for more information. In-memory data processing The Quix SDK is designed to make in-memory data processing extremely efficient. We use high-performance SDK features in conjunction with the message broker capabilities to achieve maximum throughput with the very minimum latency. Refer to the In-memory data processing section of this documentation for more information. Built-in buffers If you\u2019re sending data at high frequency, processing each message can be costly. The SDK provides a built-in buffers features for reading and writing to give you absolute freedom in balancing between latency and cost. Refer to the Built-in buffers section of this documentation for more information. Support for data frames In many use cases, multiple parameters are emitted at the same time, so they share one timestamp. Handling this data independently is wasteful. The SDK uses a rows system, and can work with Pandas DataFrames natively. Each row has a timestamp and user-defined tags as indexes. Refer to the Support for Data Frames section of this documentation for more information. Message splitting The SDK automatically handles large messages on the producer side, splitting them up if required. You no longer need to worry about Kafka message limits. On the consumer side, those messages are automatically merged back. Refer to the Message splitting section of this documentation for more information. Message compression The Quix SDK automatically compresses your messages, reducing them by an average factor of 10 times. You save money via added efficiency. The SDK also sends parameter values as the delta between timestamps, converting strings to flags, and in general reduces payload size for each message. This happens before compression is applied, so the final compression ratio is even higher. Refer to the Message compression section of this documentation for more information. Data serialization and de-serialization The Quix SDK automatically serializes data from native types in your language. You can work with familiar data types, such as Pandas DataFrames , without worrying about conversion. Serialization can be painful, especially if it is done with performance in mind. We serialize native types using our codecs so you don\u2019t have to worry about that. Refer to the Data serialization section of this documentation for more information. Multiple data types The SDK allows you to attach any type of data to your timestamps, like Numbers, Strings or even raw Binary data. This gives the SDK the ability to adapt to any streaming application use case. Refer to the Multiple data types section of this documentation for more information. Message Broker configuration including authentication and authorization Quix handles Kafka configuration efficiently and reliably. Our templates come with pre-configured certificates and connection settings. Many configuration settings are needed to use Kafka at its best, and the ideal configuration takes time! We take care of this in the SDK so you don\u2019t have to. Refer to the Broker configuration section of this documentation for more information. Checkpointing The SDK allows you to do manual checkpointing when you read data from a Topic. This provides the ability to inform the Message Broker that you have already processed messages up to one point, usually called a checkpoint . This is a very important concept when you are developing high performance streaming applications. Refer to the Checkpointing section of this documentation for more information. Horizontal scaling The Quix SDK provides horizontal scale out of the box via the streaming context feature. This means a data scientist or data engineer does not have to implement parallel processing themselves. You can scale the processing models, from one replica to many and back to one, and use the callback system inside the SDK to ensure that your data load is always shared between your model replicas. Refer to the Horizontal scaling section of this documentation for more information. Integrations The SDK offers integrations out of the box, including data persistence and historic or real-time APIs with other systems. That means you don\u2019t have to implement them by yourself. Refer to the Integrations section of this documentation for more information. Portability The Quix SDK is an abstraction layer over a concrete broker technology. You\u2019re not locked into a specific broker and can innovate over time. Refer to the Portability section of this documentation for more information. Raw messages The Quix SDK uses an internal protocol which is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides. However, in some cases, you simply do not have the ability to run the Quix SDK on both sides. To cater for these cases we added the ability to both write and read the raw, unformatted, messages as byte array. This is giving you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ).","title":"Introduction"},{"location":"sdk/introduction/#introduction","text":"The Quix SDK makes it quick and easy to develop streaming applications. It\u2019s designed to be used for high performance telemetry services where you need to process high volumes of data in a nanosecond response time. The SDK is available for Python and C#. Using the Quix SDK, you can: Write time-series data to a Kafka Topic Read time-series data from a Kafka Topic Process data by reading it from one Topic and writing the results to another one. To support these operations, the SDK provides several useful features out of the box, and solves all the common problems you should face when developing real-time streaming applications:","title":"Introduction"},{"location":"sdk/introduction/#streaming-context","text":"The Quix SDK handles stream contexts for you, so all the data from one data source is bundled in the same scope. This allows you to attach metadata to streams. The SDK simplifies the processing of streams by providing callbacks on the reading side. When processing stream data, you can identify data from different streams more easily than with the key-value approach used by other technologies. Refer to the Streaming context section of this documentation for more information.","title":"Streaming context"},{"location":"sdk/introduction/#in-memory-data-processing","text":"The Quix SDK is designed to make in-memory data processing extremely efficient. We use high-performance SDK features in conjunction with the message broker capabilities to achieve maximum throughput with the very minimum latency. Refer to the In-memory data processing section of this documentation for more information.","title":"In-memory data processing"},{"location":"sdk/introduction/#built-in-buffers","text":"If you\u2019re sending data at high frequency, processing each message can be costly. The SDK provides a built-in buffers features for reading and writing to give you absolute freedom in balancing between latency and cost. Refer to the Built-in buffers section of this documentation for more information.","title":"Built-in buffers"},{"location":"sdk/introduction/#support-for-data-frames","text":"In many use cases, multiple parameters are emitted at the same time, so they share one timestamp. Handling this data independently is wasteful. The SDK uses a rows system, and can work with Pandas DataFrames natively. Each row has a timestamp and user-defined tags as indexes. Refer to the Support for Data Frames section of this documentation for more information.","title":"Support for data frames"},{"location":"sdk/introduction/#message-splitting","text":"The SDK automatically handles large messages on the producer side, splitting them up if required. You no longer need to worry about Kafka message limits. On the consumer side, those messages are automatically merged back. Refer to the Message splitting section of this documentation for more information.","title":"Message splitting"},{"location":"sdk/introduction/#message-compression","text":"The Quix SDK automatically compresses your messages, reducing them by an average factor of 10 times. You save money via added efficiency. The SDK also sends parameter values as the delta between timestamps, converting strings to flags, and in general reduces payload size for each message. This happens before compression is applied, so the final compression ratio is even higher. Refer to the Message compression section of this documentation for more information.","title":"Message compression"},{"location":"sdk/introduction/#data-serialization-and-de-serialization","text":"The Quix SDK automatically serializes data from native types in your language. You can work with familiar data types, such as Pandas DataFrames , without worrying about conversion. Serialization can be painful, especially if it is done with performance in mind. We serialize native types using our codecs so you don\u2019t have to worry about that. Refer to the Data serialization section of this documentation for more information.","title":"Data serialization and de-serialization"},{"location":"sdk/introduction/#multiple-data-types","text":"The SDK allows you to attach any type of data to your timestamps, like Numbers, Strings or even raw Binary data. This gives the SDK the ability to adapt to any streaming application use case. Refer to the Multiple data types section of this documentation for more information.","title":"Multiple data types"},{"location":"sdk/introduction/#message-broker-configuration-including-authentication-and-authorization","text":"Quix handles Kafka configuration efficiently and reliably. Our templates come with pre-configured certificates and connection settings. Many configuration settings are needed to use Kafka at its best, and the ideal configuration takes time! We take care of this in the SDK so you don\u2019t have to. Refer to the Broker configuration section of this documentation for more information.","title":"Message Broker configuration including authentication and authorization"},{"location":"sdk/introduction/#checkpointing","text":"The SDK allows you to do manual checkpointing when you read data from a Topic. This provides the ability to inform the Message Broker that you have already processed messages up to one point, usually called a checkpoint . This is a very important concept when you are developing high performance streaming applications. Refer to the Checkpointing section of this documentation for more information.","title":"Checkpointing"},{"location":"sdk/introduction/#horizontal-scaling","text":"The Quix SDK provides horizontal scale out of the box via the streaming context feature. This means a data scientist or data engineer does not have to implement parallel processing themselves. You can scale the processing models, from one replica to many and back to one, and use the callback system inside the SDK to ensure that your data load is always shared between your model replicas. Refer to the Horizontal scaling section of this documentation for more information.","title":"Horizontal scaling"},{"location":"sdk/introduction/#integrations","text":"The SDK offers integrations out of the box, including data persistence and historic or real-time APIs with other systems. That means you don\u2019t have to implement them by yourself. Refer to the Integrations section of this documentation for more information.","title":"Integrations"},{"location":"sdk/introduction/#portability","text":"The Quix SDK is an abstraction layer over a concrete broker technology. You\u2019re not locked into a specific broker and can innovate over time. Refer to the Portability section of this documentation for more information.","title":"Portability"},{"location":"sdk/introduction/#raw-messages","text":"The Quix SDK uses an internal protocol which is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides. However, in some cases, you simply do not have the ability to run the Quix SDK on both sides. To cater for these cases we added the ability to both write and read the raw, unformatted, messages as byte array. This is giving you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ).","title":"Raw messages"},{"location":"sdk/kafka/","text":"Kafka and Quix SDK The Quix SDK helps you to leverage Kafka\u2019s powerful features with ease. Why this is important Kafka is a powerful but complex technology to master. Using the Quix SDK, you can leverage the power of Kafka without worrying about mastering it. There are just a couple of important concepts to grasp, the rest is handled in the background by the SDK. Concepts Topic replica Each topic can be set to replicate over the Kafka cluster for increased resilience, so a failure of a node will not cause downtime of your processing pipeline. For example, if you set replica to 2, every message you send to the topic will be replicated twice in the cluster. Tip If you set replication to two, data streamed to the cluster is billed twice. Topic retention Each topic has temporary storage. Every message sent to the topic will live in Kafka for a configured amount of time or size. That means that a consumer can join the topic later and still consume messages. If your processing pipeline has downtime, no data is lost. Topic partitions Each Kafka topic is created with a number of partitions. You can add more partitions later, but you can\u2019t remove them. Each partition is an independent queue that preserves the order of messages. The Quix SDK restricts all messages inside one stream to the same single partition. That means that inside one stream, a consumer can rely on the order of messages. Partitions are spread across your Kafka cluster, over different Kafka nodes, for improved performance. Redistribution of load Streams are redistributed over available partitions. With an increasing number of streams, each partition will end up with approximately the same number of streams. Warning The number of partitions sets the limit for how many parallel instances of one model can process the topic. For example: A topic with three partitions can be processed with up to 3 instances of a model. The fourth instance will remain idle. Consumer group The Consumer group is a concept of how to horizontally scale topic processing. Each consumer group has an ID, which you set when opening a connection to the topic: output_topic = client . open_input_topic ( \" {topic} \" , \"{your-consumer-group-id}\" ) If you deploy this model with a replica set to 3, your model will be deployed in three instances as members of one consumer group. This group will share partitions between each other and therefore share the load. If you increase the number, some partitions will get reassigned to new instances of the model. If you decrease the number, partitions left by leaving instances get reassigned to remaining processing instances in the consumer group. Checkpointing We can think of Kafka temporary storage as a processing queue for each partition. Consumer groups read from this queue and regularly commit offsets to track which messages were already processed. By default, this is done by the Quix SDK automatically, but you can override that by manually committing an offset when you are done processing a set of rows. input_topic = client . open_input_topic ( 'Telemetry' , commit_settings = CommitMode . Manual ) input_topic . commit () The consumer group is playing an important role here as offset commits are associated with the consumer group ID. That means that if you connect to the same topic with a different consumer group ID, the model will start reading from the start of the Kafka queue. Tip If you want to consume data from the topic locally for debugging purposes, and the model is deployed in the Quix serverless environment at the same time, make sure that you change consumer group ID to prevent clashing with the cloud deployment. Note When you open a topic you can also choose where to start reading data from. Either read all the data from the start or only read the new data as it arrives. Read more here Data Grouping Topics group data streams from a single type of source. The golden rule for maximum performance is to always maintain one schema per topic. For example: For connected car data you could create individual topics to group data from different systems like the engine, transmission, electronics, chassis, infotainment systems. For games you might create individual topics to separate player, game and machine data. For consumer apps you could create a topic each source i.e one for your IOS app, one for your Android app, and one for your web app. For live ML pipelines you\u2019ll want to create a topic for each stage of the pipeline ie raw-data-topic \u2192 cleaning model \u2192 clean-data-topic \u2192 ML model \u2192 results topic Data Governance Topics are key to good data governance. Use them to organise your data by: Group data streams by type or source. Use separate topics for raw, clean or processed data. Create prototyping topics to publish results of models in development. Scale Topics automatically scale. We have designed the underlying infrastructure to automatically stream any amount of data from any number of sources. With Quix you can connect one source - like a car, wearable or web app - to do R\\&D, then scale your solution to millions of cars, wearables or apps in production, all on the same topic. Security Our topics are secured with industry standard SSL data encryption and SASL ACL authorisation and authentication. You can safely send data over public networks and trust that it is encrypted in-flight.","title":"Kafka and Quix SDK"},{"location":"sdk/kafka/#kafka-and-quix-sdk","text":"The Quix SDK helps you to leverage Kafka\u2019s powerful features with ease.","title":"Kafka and Quix SDK"},{"location":"sdk/kafka/#why-this-is-important","text":"Kafka is a powerful but complex technology to master. Using the Quix SDK, you can leverage the power of Kafka without worrying about mastering it. There are just a couple of important concepts to grasp, the rest is handled in the background by the SDK.","title":"Why this is important"},{"location":"sdk/kafka/#concepts","text":"","title":"Concepts"},{"location":"sdk/kafka/#topic-replica","text":"Each topic can be set to replicate over the Kafka cluster for increased resilience, so a failure of a node will not cause downtime of your processing pipeline. For example, if you set replica to 2, every message you send to the topic will be replicated twice in the cluster. Tip If you set replication to two, data streamed to the cluster is billed twice.","title":"Topic replica"},{"location":"sdk/kafka/#topic-retention","text":"Each topic has temporary storage. Every message sent to the topic will live in Kafka for a configured amount of time or size. That means that a consumer can join the topic later and still consume messages. If your processing pipeline has downtime, no data is lost.","title":"Topic retention"},{"location":"sdk/kafka/#topic-partitions","text":"Each Kafka topic is created with a number of partitions. You can add more partitions later, but you can\u2019t remove them. Each partition is an independent queue that preserves the order of messages. The Quix SDK restricts all messages inside one stream to the same single partition. That means that inside one stream, a consumer can rely on the order of messages. Partitions are spread across your Kafka cluster, over different Kafka nodes, for improved performance.","title":"Topic partitions"},{"location":"sdk/kafka/#redistribution-of-load","text":"Streams are redistributed over available partitions. With an increasing number of streams, each partition will end up with approximately the same number of streams. Warning The number of partitions sets the limit for how many parallel instances of one model can process the topic. For example: A topic with three partitions can be processed with up to 3 instances of a model. The fourth instance will remain idle.","title":"Redistribution of load"},{"location":"sdk/kafka/#consumer-group","text":"The Consumer group is a concept of how to horizontally scale topic processing. Each consumer group has an ID, which you set when opening a connection to the topic: output_topic = client . open_input_topic ( \" {topic} \" , \"{your-consumer-group-id}\" ) If you deploy this model with a replica set to 3, your model will be deployed in three instances as members of one consumer group. This group will share partitions between each other and therefore share the load. If you increase the number, some partitions will get reassigned to new instances of the model. If you decrease the number, partitions left by leaving instances get reassigned to remaining processing instances in the consumer group.","title":"Consumer group"},{"location":"sdk/kafka/#checkpointing","text":"We can think of Kafka temporary storage as a processing queue for each partition. Consumer groups read from this queue and regularly commit offsets to track which messages were already processed. By default, this is done by the Quix SDK automatically, but you can override that by manually committing an offset when you are done processing a set of rows. input_topic = client . open_input_topic ( 'Telemetry' , commit_settings = CommitMode . Manual ) input_topic . commit () The consumer group is playing an important role here as offset commits are associated with the consumer group ID. That means that if you connect to the same topic with a different consumer group ID, the model will start reading from the start of the Kafka queue. Tip If you want to consume data from the topic locally for debugging purposes, and the model is deployed in the Quix serverless environment at the same time, make sure that you change consumer group ID to prevent clashing with the cloud deployment. Note When you open a topic you can also choose where to start reading data from. Either read all the data from the start or only read the new data as it arrives. Read more here","title":"Checkpointing"},{"location":"sdk/kafka/#data-grouping","text":"Topics group data streams from a single type of source. The golden rule for maximum performance is to always maintain one schema per topic. For example: For connected car data you could create individual topics to group data from different systems like the engine, transmission, electronics, chassis, infotainment systems. For games you might create individual topics to separate player, game and machine data. For consumer apps you could create a topic each source i.e one for your IOS app, one for your Android app, and one for your web app. For live ML pipelines you\u2019ll want to create a topic for each stage of the pipeline ie raw-data-topic \u2192 cleaning model \u2192 clean-data-topic \u2192 ML model \u2192 results topic","title":"Data Grouping"},{"location":"sdk/kafka/#data-governance","text":"Topics are key to good data governance. Use them to organise your data by: Group data streams by type or source. Use separate topics for raw, clean or processed data. Create prototyping topics to publish results of models in development.","title":"Data Governance"},{"location":"sdk/kafka/#scale","text":"Topics automatically scale. We have designed the underlying infrastructure to automatically stream any amount of data from any number of sources. With Quix you can connect one source - like a car, wearable or web app - to do R\\&D, then scale your solution to millions of cars, wearables or apps in production, all on the same topic.","title":"Scale"},{"location":"sdk/kafka/#security","text":"Our topics are secured with industry standard SSL data encryption and SASL ACL authorisation and authentication. You can safely send data over public networks and trust that it is encrypted in-flight.","title":"Security"},{"location":"sdk/process/","text":"Processing data The Quix SDK is specifically designed to make real time data processing very easy. We provide high-performance technology, inherited from F1, in a way that anybody with basic development skills can understand and use it very quickly. Tip The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples. Other streaming platforms are tied to a narrow set of functions, queries, or syntax to set up a data processor or Model. This limited approach is often only suitable for some use cases, and tends to have a steep learning curve. The feature limitations and time investment required form a barrier to entry for inexperienced developers and Data scientists alike. Our approach is simpler and far more powerful than other streaming solutions. So much so that we can\u2019t show you any SDK related functions here because you literally don\u2019t need them if you use the Quix SDK. With the Quix SDK, you are not tied to complicated Functions, Lambdas, Maps or Query libraries to be able to deploy and process data in real time. You just need to know how to read and write data with the SDK \u2014 that\u2019s it, the rest is up to you and your imagination. Let\u2019s see some examples of how to read and write data in a Data processor using the Quix SDK. We just read data from the message broker, process it, and write it back to the stream. Python - Data Frame Python - Plain C# # Callback triggered for each new data frame def on_parameter_data_handler ( data : ParameterData ): df = data . to_panda_frame () # Input data frame output_df = pd . DataFrame () output_df [ \"time\" ] = df [ \"time\" ] output_df [ \"TAG__LapNumber\" ] = df [ \"TAG__LapNumber\" ] # If braking force applied is more than 50%, we mark HardBraking with True output_df [ \"HardBraking\" ] = df . apply ( lambda row : \"True\" if row . Brake > 0.5 else \"False\" , axis = 1 ) stream_output . parameters . write ( output_df ) # Send data back to the stream # Callback triggered for each new data frame def on_parameter_data_handler ( data : ParameterData ): for row in data . timestamps : # If braking force applied is more than 50%, we mark HardBraking with True hard_braking = row . parameters [ \"Brake\" ] . numeric_value > 0.5 stream_output . parameters \\ . add_timestamp ( row . timestamp ) \\ . add_tag ( \"LapNumber\" , row . tags [ \"LapNumber\" ]) \\ . add_value ( \"HardBraking\" , hard_braking ) \\ . write () buffer . OnRead += ( data ) => { var outputData = new ParameterData (); // We calculate mean value for each second of data to effectively down-sample source topic to 1Hz. outputData . AddTimestamp ( data . Timestamps . First (). Timestamp ) . AddValue ( \"ParameterA 10Hz\" , data . Timestamps . Average ( s => s . Parameters [ \"ParameterA\" ]. NumericValue . GetValueOrDefault ())) . AddValue ( \"ParameterA source frequency\" , data . Timestamps . Count ); // Send data back to the stream streamOutput . Parameters . Write ( outputData ); }; So, because you are not tied to any narrow processing architecture, you can use any methods, classes or libraries that you are already familiar with to implement your model or data processor. Check the complete code example in GitHub for further information.","title":"Processing data"},{"location":"sdk/process/#processing-data","text":"The Quix SDK is specifically designed to make real time data processing very easy. We provide high-performance technology, inherited from F1, in a way that anybody with basic development skills can understand and use it very quickly. Tip The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples. Other streaming platforms are tied to a narrow set of functions, queries, or syntax to set up a data processor or Model. This limited approach is often only suitable for some use cases, and tends to have a steep learning curve. The feature limitations and time investment required form a barrier to entry for inexperienced developers and Data scientists alike. Our approach is simpler and far more powerful than other streaming solutions. So much so that we can\u2019t show you any SDK related functions here because you literally don\u2019t need them if you use the Quix SDK. With the Quix SDK, you are not tied to complicated Functions, Lambdas, Maps or Query libraries to be able to deploy and process data in real time. You just need to know how to read and write data with the SDK \u2014 that\u2019s it, the rest is up to you and your imagination. Let\u2019s see some examples of how to read and write data in a Data processor using the Quix SDK. We just read data from the message broker, process it, and write it back to the stream. Python - Data Frame Python - Plain C# # Callback triggered for each new data frame def on_parameter_data_handler ( data : ParameterData ): df = data . to_panda_frame () # Input data frame output_df = pd . DataFrame () output_df [ \"time\" ] = df [ \"time\" ] output_df [ \"TAG__LapNumber\" ] = df [ \"TAG__LapNumber\" ] # If braking force applied is more than 50%, we mark HardBraking with True output_df [ \"HardBraking\" ] = df . apply ( lambda row : \"True\" if row . Brake > 0.5 else \"False\" , axis = 1 ) stream_output . parameters . write ( output_df ) # Send data back to the stream # Callback triggered for each new data frame def on_parameter_data_handler ( data : ParameterData ): for row in data . timestamps : # If braking force applied is more than 50%, we mark HardBraking with True hard_braking = row . parameters [ \"Brake\" ] . numeric_value > 0.5 stream_output . parameters \\ . add_timestamp ( row . timestamp ) \\ . add_tag ( \"LapNumber\" , row . tags [ \"LapNumber\" ]) \\ . add_value ( \"HardBraking\" , hard_braking ) \\ . write () buffer . OnRead += ( data ) => { var outputData = new ParameterData (); // We calculate mean value for each second of data to effectively down-sample source topic to 1Hz. outputData . AddTimestamp ( data . Timestamps . First (). Timestamp ) . AddValue ( \"ParameterA 10Hz\" , data . Timestamps . Average ( s => s . Parameters [ \"ParameterA\" ]. NumericValue . GetValueOrDefault ())) . AddValue ( \"ParameterA source frequency\" , data . Timestamps . Count ); // Send data back to the stream streamOutput . Parameters . Write ( outputData ); }; So, because you are not tied to any narrow processing architecture, you can use any methods, classes or libraries that you are already familiar with to implement your model or data processor. Check the complete code example in GitHub for further information.","title":"Processing data"},{"location":"sdk/python-setup/","text":"Set up local environment The Quix Portal offers you an online IDE , ready to use without any additional setup to develop or deploy your applications. However, you might sometimes want to work with your application in a local IDE. In such cases, Python development needs some setup before you can use our SDK. In this section are detailed instructions of how to set up your Python environment on Linux, Mac or Windows. Note This guide assumes you have already cloned a project of your own or download a library sample provided within the platform. In addition some stages may require files from GitHub . Warning Make sure you\u2019re using Python version 3.7>=, \\<3.9 Install Dependencies To get started, install the SDK dependencies. MacOS Intel MacOS M1 Windows Linux You can install dependencies via a script or manually. Install dependencies via script Download the script named quix-dependency-installer-mac.sh from GitHub , which installs all necessary requirements. Download the project then run the script by copy-pasting the following into a terminal from the project\u2019s top directory. chmod +x ./quix-dependency-installer-mac.sh && ./quix-dependency-installer-mac.sh Install dependencies manually Install and configure PythonNet dependencies Install the Brew package manager (from brew.sh ). To add brew to your path: echo \"export PATH=/usr/local/bin: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Install Mono Version brew install mono echo \"export PATH=/Library/Frameworks/Mono.framework/Versions/Current/Commands: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" echo \"export PKG_CONFIG_PATH=/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig: $PKG_CONFIG_PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Additional things to install: brew install pkg-config python3 -m pip install wheel python3 -m pip install pycparser You can install dependencies via a script or manually. Install dependencies via script Download the script named quix-dependency-installer-mac.sh from GitHub , which installs all necessary requirements. Download the project then run the script by copy-pasting the following into a terminal from the project\u2019s top directory. chmod +x ./quix-dependency-installer-mac.sh && ./quix-dependency-installer-mac.sh Install dependencies manually Install and configure PythonNet dependencies Install the Brew package manager (from brew.sh ). To add brew to your path: echo \"export PATH=/opt/homebrew/bin: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Install rosetta sudo softwareupdate --install-rosetta --agree-to-license Install Mono Version curl https://download.mono-project.com/archive/6.12.0/macos-10-universal/MonoFramework-MDK-6.12.0.122.macos10.xamarin.universal.pkg -O monoPkgPath = ./MonoFramework-MDK-6.12.0.122.macos10.xamarin.universal.pkg sudo installer -pkg $monoPkgPath -target / echo \"export PATH=/Library/Frameworks/Mono.framework/Versions/Current/Commands: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" echo \"export PKG_CONFIG_PATH=/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig: $PKG_CONFIG_PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Additional things to install: brew install pkg-config python3 -m pip install wheel python3 -m pip install pycparser Install the latest .Net Core runtime ( https://dotnet.microsoft.com/download/dotnet-core/current/runtime ) Note Because of the variability between linux distributions we highly recommend using a Docker environment ( see Docker setup ) in case you experience any issues during the installation. Install and configure PythonNet dependencies - Ubuntu 20.04 Install Mono version 6.10.0 sudo apt install gnupg ca-certificates sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF echo \"deb https://download.mono-project.com/repo/ubuntu stable-focal/snapshots/6.10.0 main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list && apt-get update sudo apt update sudo apt install mono-devel For other linux distributions, please see the Mono current release page and older Releases page Additional things to install: sudo apt install python3-pip clang libglib2.0-0 build-essential librdkafka-dev python3 -m pip install pycparser In some distributions librdkafka-dev will not be readily available and guide ( https://github.com/edenhill/librdkafka ) must be followed. For ubuntu 20.04, the following steps are readily curated for you: sudo apt-get install -y wget software-properties-common wget -qO - https://packages.confluent.io/deb/7.2/archive.key | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://packages.confluent.io/deb/7.2 stable main\" sudo add-apt-repository \"deb https://packages.confluent.io/clients/deb $( lsb_release -cs ) main\" sudo apt-get update sudo apt install librdkafka-dev -y Create New Python Environment It\u2019s good practice to use a python virtual environment and it is doubly true when using the Quix streaming library. The library currently relies on some dll redirecting, which is achieved by adding a file to your python environment. This is done automatically, but if you have other python application(s)/package(s) that also rely on similar redirection then a virtual environment is advised. MacOS Windows Linux To create a new virtual environment, execute the following in a terminal at your desired location (such as the root folder of the downloaded sample): python3 -m pip install virtualenv python3 -m virtualenv env --python = python3.8.7 chmod +x ./env/bin/activate source ./env/bin/activate You will know you succeeded in activating the environment if your terminal line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally. To create a new virtual environment, execute the following in a terminal at your desired location (such as the root folder of the downloaded sample): pip install virtualenv python -m virtualenv env --python = python3.8 \"env/Scripts/activate\" You will know you succeeded in activating the environment if your command line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally. Note You might need to use a new command line after installing Python, because PATH isn\u2019t refreshed for existing command lines when something is installed. To create a new virtual environment, execute the following in a terminal at your desired location: python3 -m pip install virtualenv python3 -m virtualenv env --python = python3.8 chmod +x ./env/bin/activate source ./env/bin/activate You will know you succeeded in activating the environment if your terminal line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally. Install code requirements MacOS Windows Linux In the same terminal you activated the virtual environment, navigate to the folder where requirements.txt (in the sample you downloaded) is located and execute the following: python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ In the same console you activated the virtual enviornment, navigate to the folder where requirements.txt (in the sample you downloaded) is located and execute the following: pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ In the same terminal you activated the virtual environment, navigate to the folder where requirements.txt (in the sample you downloaded) is located and execute the following: python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ Your environment should be ready to run the code! MacOS Windows Linux python3 main.py python main.py python3 main.py","title":"Set up local environment"},{"location":"sdk/python-setup/#set-up-local-environment","text":"The Quix Portal offers you an online IDE , ready to use without any additional setup to develop or deploy your applications. However, you might sometimes want to work with your application in a local IDE. In such cases, Python development needs some setup before you can use our SDK. In this section are detailed instructions of how to set up your Python environment on Linux, Mac or Windows. Note This guide assumes you have already cloned a project of your own or download a library sample provided within the platform. In addition some stages may require files from GitHub . Warning Make sure you\u2019re using Python version 3.7>=, \\<3.9","title":"Set up local environment"},{"location":"sdk/python-setup/#install-dependencies","text":"To get started, install the SDK dependencies. MacOS Intel MacOS M1 Windows Linux You can install dependencies via a script or manually. Install dependencies via script Download the script named quix-dependency-installer-mac.sh from GitHub , which installs all necessary requirements. Download the project then run the script by copy-pasting the following into a terminal from the project\u2019s top directory. chmod +x ./quix-dependency-installer-mac.sh && ./quix-dependency-installer-mac.sh Install dependencies manually Install and configure PythonNet dependencies Install the Brew package manager (from brew.sh ). To add brew to your path: echo \"export PATH=/usr/local/bin: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Install Mono Version brew install mono echo \"export PATH=/Library/Frameworks/Mono.framework/Versions/Current/Commands: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" echo \"export PKG_CONFIG_PATH=/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig: $PKG_CONFIG_PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Additional things to install: brew install pkg-config python3 -m pip install wheel python3 -m pip install pycparser You can install dependencies via a script or manually. Install dependencies via script Download the script named quix-dependency-installer-mac.sh from GitHub , which installs all necessary requirements. Download the project then run the script by copy-pasting the following into a terminal from the project\u2019s top directory. chmod +x ./quix-dependency-installer-mac.sh && ./quix-dependency-installer-mac.sh Install dependencies manually Install and configure PythonNet dependencies Install the Brew package manager (from brew.sh ). To add brew to your path: echo \"export PATH=/opt/homebrew/bin: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Install rosetta sudo softwareupdate --install-rosetta --agree-to-license Install Mono Version curl https://download.mono-project.com/archive/6.12.0/macos-10-universal/MonoFramework-MDK-6.12.0.122.macos10.xamarin.universal.pkg -O monoPkgPath = ./MonoFramework-MDK-6.12.0.122.macos10.xamarin.universal.pkg sudo installer -pkg $monoPkgPath -target / echo \"export PATH=/Library/Frameworks/Mono.framework/Versions/Current/Commands: $PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" echo \"export PKG_CONFIG_PATH=/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig: $PKG_CONFIG_PATH \" >> ~/.bash_profile && source ~/.bash_profile && echo \"Worked\" || echo \"Failed\" Additional things to install: brew install pkg-config python3 -m pip install wheel python3 -m pip install pycparser Install the latest .Net Core runtime ( https://dotnet.microsoft.com/download/dotnet-core/current/runtime ) Note Because of the variability between linux distributions we highly recommend using a Docker environment ( see Docker setup ) in case you experience any issues during the installation. Install and configure PythonNet dependencies - Ubuntu 20.04 Install Mono version 6.10.0 sudo apt install gnupg ca-certificates sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF echo \"deb https://download.mono-project.com/repo/ubuntu stable-focal/snapshots/6.10.0 main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list && apt-get update sudo apt update sudo apt install mono-devel For other linux distributions, please see the Mono current release page and older Releases page Additional things to install: sudo apt install python3-pip clang libglib2.0-0 build-essential librdkafka-dev python3 -m pip install pycparser In some distributions librdkafka-dev will not be readily available and guide ( https://github.com/edenhill/librdkafka ) must be followed. For ubuntu 20.04, the following steps are readily curated for you: sudo apt-get install -y wget software-properties-common wget -qO - https://packages.confluent.io/deb/7.2/archive.key | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://packages.confluent.io/deb/7.2 stable main\" sudo add-apt-repository \"deb https://packages.confluent.io/clients/deb $( lsb_release -cs ) main\" sudo apt-get update sudo apt install librdkafka-dev -y","title":"Install Dependencies"},{"location":"sdk/python-setup/#create-new-python-environment","text":"It\u2019s good practice to use a python virtual environment and it is doubly true when using the Quix streaming library. The library currently relies on some dll redirecting, which is achieved by adding a file to your python environment. This is done automatically, but if you have other python application(s)/package(s) that also rely on similar redirection then a virtual environment is advised. MacOS Windows Linux To create a new virtual environment, execute the following in a terminal at your desired location (such as the root folder of the downloaded sample): python3 -m pip install virtualenv python3 -m virtualenv env --python = python3.8.7 chmod +x ./env/bin/activate source ./env/bin/activate You will know you succeeded in activating the environment if your terminal line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally. To create a new virtual environment, execute the following in a terminal at your desired location (such as the root folder of the downloaded sample): pip install virtualenv python -m virtualenv env --python = python3.8 \"env/Scripts/activate\" You will know you succeeded in activating the environment if your command line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally. Note You might need to use a new command line after installing Python, because PATH isn\u2019t refreshed for existing command lines when something is installed. To create a new virtual environment, execute the following in a terminal at your desired location: python3 -m pip install virtualenv python3 -m virtualenv env --python = python3.8 chmod +x ./env/bin/activate source ./env/bin/activate You will know you succeeded in activating the environment if your terminal line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally.","title":"Create New Python Environment"},{"location":"sdk/python-setup/#install-code-requirements","text":"MacOS Windows Linux In the same terminal you activated the virtual environment, navigate to the folder where requirements.txt (in the sample you downloaded) is located and execute the following: python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ In the same console you activated the virtual enviornment, navigate to the folder where requirements.txt (in the sample you downloaded) is located and execute the following: pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ In the same terminal you activated the virtual environment, navigate to the folder where requirements.txt (in the sample you downloaded) is located and execute the following: python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/ Your environment should be ready to run the code! MacOS Windows Linux python3 main.py python main.py python3 main.py","title":"Install code requirements"},{"location":"sdk/read/","text":"Reading data The Quix SDK allows you to read data in real time from the existing streams of your Topics. All the necessary code to read data from your Quix Workspace is auto-generated when you create a project using the existing templates. In this section, we explain more in-depth how to read data using the Quix SDK. Tip The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples. Connect to Quix In order to start reading data from Quix you need an instance of the Quix client, QuixStreamingClient . This is the central point where you interact with the main SDK operations. You can create an instance of QuixStreamingClient using the proper constructor of the SDK. Python C# client = QuixStreamingClient () var client = new Quix . Sdk . Streaming . QuixStreamingClient (); You can find more advanced information on how to connect to Quix in the Connect to Quix section. Open a topic for reading Topics are the default environment for input/output operations on Quix. In order to access that topic for reading you need an instance of InputTopic . This instance allow you to read all the incoming streams on the specified Topic. You can create an instance of InputTopic using the client\u2019s open_input_topic method, passing the TOPIC_ID or the TOPIC_NAME as a parameter. Python C# input_topic = client . open_input_topic ( TOPIC_ID ) var inputTopic = client . OpenInputTopic ( TOPIC_ID ); Consumer group The Consumer group is a concept used when you want to scale horizontally . Each consumer group is identified using an ID, which you set optionally when opening a connection to the topic for reading: Python C# input_topic = client . open_input_topic ( \" {topic} \" , \"{your-consumer-group-id}\" ) var inputTopic = client . OpenInputTopic ( \"{topic}\" , \"{your-consumer-group-id}\" ); When you want to enable horizontal scalability , all the replicas of your process should use the same ConsumerId . This is how the message broker knows that all the replicas of your process want to share the load of the incoming streams between replicas. Each replica will receive only a subset of the streams incoming to the Input Topic. Warning If you want to consume data from the topic locally for debugging purposes, and the model is also deployed in the Quix serverless environment, make sure that you change the consumer group ID to prevent clashing with the cloud deployment. If the clash happens, only one instance will be able to read data of the Stream at a time, and you will probably notice that your code is not receiving data at some point, either locally or in the cloud environment. Reading Streams Python C# Once you have the InputTopic instance you can start reading streams. For each stream received to the specified topic, InputTopic will execute the event on_stream_received . You can attach a callback to this event to execute code that reacts when you receive a new Stream. For example, the following code prints the StreamId for each newStream received on that Topic: def read_stream ( new_stream : StreamReader ): print ( \"New stream read:\" + new_stream . stream_id ) input_topic . on_stream_received += read_stream input_topic . start_reading () Once you have the InputTopic instance you can start reading streams. For each stream received to the specified topic, InputTopic will execute the event OnStreamReceived . You can attach a callback to this event to execute code that reacts when you receive a new Stream. For example the following code prints the StreamId for each newStream received on that Topic: inputTopic . OnStreamReceived += ( s , newStream ) => { Console . WriteLine ( $\"New stream read: {newStream.StreamId}\" ); }; inputTopic . StartReading (); Tip The StartReading method indicates to the SDK the moment to start reading streams and data from your Topic. This should normally happen after you\u2019ve registered callbacks for all the events you want to listen to. Reading Parameter Data You can read real-time data from Streams using the on_read event of the StreamReader instance received in the previous callback when you receive a new stream in your Topic. For instance, in the following example we read and print the first timestamp and value of the parameter ParameterA received in the ParameterData packet: Python C# def on_stream_received_handler ( new_stream : StreamReader ): def on_parameter_data_handler ( data : ParameterData ): timestamp = data . timestamps [ 0 ] . timestamp num_value = data . timestamps [ 0 ] . parameters [ 'ParameterA' ] . numeric_value print ( \"ParameterA - \" + str ( timestamp ) + \": \" + str ( num_value )) new_stream . on_read += on_parameter_data_handler input_topic . on_stream_received += on_stream_received_handler input_topic . start_reading () inputTopic . OnStreamReceived += ( s , streamReader ) => { streamReader . Parameters . OnRead += parameterData => { var timestamp = parameterData . Timestamps [ 0 ]. Timestamp ; var numValue = parameterData . Timestamps [ 0 ]. Parameters [ \"ParameterA\" ]. NumericValue ; Console . WriteLine ( $\"ParameterA - {timestamp}: {numValue}\" ); }; }; inputTopic . StartReading (); We use ParameterData packages to read data from the stream. This class handles reading and writing of time series data. The Quix SDK provides multiple helpers for reading and writing data using ParameterData . Tip If you\u2019re using Python you can convert ParameterData to a Pandas DataFrames or read them directly from the SDK. Refer to Using Data Frames for more information. Parameter Data format ParameterData is the formal class in the SDK which represents a time series data packet in memory. ParameterData consists of a list of Timestamps with their corresponding Parameter Names and Values for each timestamp. You should imagine a Parameter Data as a table where the Timestamp is the first column of that table and where the Parameters are the columns for the Values of that table. Timestamp Speed Gear 1 120 3 2 123 3 3 125 3 6 110 2 An example of ParameterData You can use the timestamps property of a ParameterData instance to access each row of that table, and the parameters property to access the values of that timestamp. The Quix SDK supports Numeric, String, and Binary values and you should use the proper property depending of the value type of your Parameter: Python C# numeric_value : Returns the Numeric value of the Parameter, represented as a float type. string_value : Returns the String value of the Parameter, represented as a string type. binary_value : Returns the Binary value of the Parameter, represented as a bytearray type. NumericValue : Returns the Numeric value of the Parameter, represented as a double type. StringValue : Returns the String value of the Parameter, represented as a string type. BinaryValue : Returns the Binary value of the Parameter, represented as an array of byte . This is a simple example showing how to read Speed values of the ParameterData used in the previous example: Python C# for ts in data . timestamps : timestamp = ts . timestamp_nanoseconds numValue = ts . parameters [ 'Speed' ] . numeric_value print ( \"Speed - \" + str ( timestamp ) \": \" + str ( numValue )) foreach ( var timestamp in data . Timestamps ) { var timestamp = timestamp . TimestampNanoseconds ; var numValue = timestamp . Parameters [ \"Speed\" ]. NumericValue ; Console . WriteLine ( $\"Speed - {timestamp}: {numValue}\" ); } output: Speed - 1: 120 Speed - 2: 123 Speed - 3: 125 Speed - 6: 110 Buffer The Quix SDK provides you with a programmable buffer which you can tailor to your needs. Using buffers to read data enhances the throughput of your application. This helps you to develop Models with a high performance throughput. You can use the buffer property embedded in the Parameters property of your stream , or create a separate instance of that buffer using the create_buffer method: Python C# buffer = newStream . parameters . create_buffer () var buffer = newStream . Parameters . CreateBuffer (); You can configure a buffer\u2019s input requirements using built-in properties. For example, the following configuration means that the Buffer will release a packet when the time span between first and last timestamp inside the buffer reaches 100 milliseconds: Python C# buffer . time_span_in_milliseconds = 100 buffer . TimeSpanInMilliseconds = 100 ; Reading data from that buffer is as simple as using its OnRead event. For each ParameterData packet released from the buffer, the SDK will execute the OnRead event with the parameter data as a given parameter. For example the following code prints the ParameterA value of the first timestamp of each packet released from the buffer: Python C# def on_parameter_data_handler ( data : ParameterData ): timestamp = data . timestamps [ 0 ] . timestamp num_value = data . timestamps [ 0 ] . parameters [ 'ParameterA' ] . numeric_value print ( \"ParameterA - \" + str ( timestamp ) + \": \" + str ( num_value )) buffer . on_read += on_parameter_data_handler buffer . OnRead += ( data ) => { var timestamp = data . Timestamps [ 0 ]. Timestamp ; var numValue = data . Timestamps [ 0 ]. Parameters [ \"ParameterA\" ]. NumericValue ; Console . WriteLine ( $\"ParameterA - {timestamp}: {numValue}\" ); }; You can configure multiple conditions to determine when the Buffer has to release data, if any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer: Python C# buffer.buffer_timeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. buffer.packet_size : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. buffer.time_span_in_nanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. buffer.time_span_in_milliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. buffer.custom_trigger_before_enqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. buffer.custom_trigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. buffer.filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t. Buffer.BufferTimeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. Buffer.PacketSize : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. Buffer.TimeSpanInNanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Buffer.TimeSpanInMilliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. Buffer.CustomTriggerBeforeEnqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. Buffer.CustomTrigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. Buffer.Filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t. Examples This buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timeout period, it will empty the buffer and send the pending data anyway. Python C# stream . parameters . buffer . packet_size = 100 stream . parameters . buffer . buffer_timeout = 1000 stream . Parameters . Buffer . PacketSize = 100 ; stream . Parameters . Buffer . BufferTimeout = 1000 ; This buffer configuration will send data every 100ms window or if critical data arrives. Python C# buffer . time_span_in_milliseconds = 100 buffer . custom_trigger = lambda data : data . timestamps [ 0 ] . tags [ \"is_critical\" ] == 'True' stream . Parameters . Buffer . TimeSpanInMilliseconds = 100 ; stream . Parameters . Buffer . CustomTrigger = data => data . Timestamps [ 0 ]. Tags [\"is_critical\"] == \"True\" ; Using Data Frames If you use the Python version of the SDK you can use Pandas DataFrames for reading and writing ParameterData to Quix. The Pandas DataFrames format is just a representation of ParameterData format, where the Timestamp is mapped to a column named time and the rest of the parameters are mapped as columns named as the ParameterId of the parameter. Tags are mapped as columns with the prefix TAG__ and the TagId of the tag. For example, the following ParameterData : Timestamp CarId (tag) Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 An example of ParameterData Is represented as the following Pandas DataFrame: time TAG__CarId Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 A representation of ParameterData in a Pandas DataFrame One simple way to read data from Quix using Pandas DataFrames is using the event on_read_pandas instead of the common event on_read when reading from a stream , or when reading data from a buffer: def read_stream ( new_stream : StreamReader ): buffer = new_stream . parameters . create_buffer () def on_pandas_frame_handler ( df : pd . DataFrame ): print ( df . to_string ()) buffer . on_read_pandas += on_pandas_frame_handler input_topic . on_stream_received += read_stream input_topic . start_reading () Alternatively, you can always convert a ParameterData to a Pandas DataFrame using the method to_panda_frame : def read_stream ( new_stream : StreamReader ): buffer = new_stream . parameters . create_buffer () def on_parameter_data_handler ( data : ParameterData ): # read from input stream df = data . to_panda_frame () print ( df . to_string ()) buffer . on_read += on_parameter_data_handler input_topic . on_stream_received += read_stream input_topic . start_reading () Tip The conversions from ParameterData to Pandas DataFrames have an intrinsic cost overhead. For high-performance models using Pandas DataFrames, you probably want to use the on_read_pandas event provided by the SDK, which is optimized for doing as few conversions as possible. Reading Events EventData is the formal class in the SDK which represents an Event data packet in memory. EventData is meant to be used for time series data coming from sources that generate data at irregular intervals or without a defined structure. Event Data format EventData consists of a record with a Timestamp, an EventId and an EventValue. You should imagine a list of Event Data instances as a simple table of three columns where the Timestamp is the first column of that table and the EventId and EventValue are the second and third columns. Timestamp EventId EventValue 1 failure23 Gearbox has a failure 2 box-event2 Car has entered to the box 3 motor-off Motor has stopped 6 race-event3 Race has finished An example of a list of EventData Reading events from a stream is as easy as reading parameter data. In this case, the SDK does not use a Buffer because we don\u2019t need high performance throughput, but the way we read Event Data from a Stream is identical. Python C# def on_event_data_handler ( data : EventData ): print ( \"Event read for stream. Event Id: \" + data . Id ) new_stream . events . on_read += on_event_data_handler newStream . Events . OnRead += ( data ) => { Console . WriteLine ( $\"Event read for stream. Event Id: {data.Id}\" ); }; output: Event read for stream. Event Id: failure23 Event read for stream. Event Id: box-event2 Event read for stream. Event Id: motor-off Event read for stream. Event Id: race-event3 Committing / checkpointing It is important to be aware of the commit concept when working with a broker. Committing allows one to mark how far data has been processed, also known as creating a checkpoint. In the event of a restart or rebalance, the client only processes messages from the last commit position. In Kafka this is equivalent to commits for a consumer group . Commits are done for each consumer group, so if you have several consumer groups in use, they do not affect each another when commiting to one of them. Tip Commits are done at a partition level when you use Kafka as a Message Broker, which means that streams that belong to the same partition are committed using the same position. The SDK currently does not expose the option to subscribe to only specific partitions of a topic, but commits will only ever affect partitions that are currently assigned to your client. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of the Quix SDK. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the SDK. Automatic committing By default, the SDK automatically commits messages for which all handlers returned at a regular default interval, which is every 5 seconds or 5,000 messages, whichever happens sooner. However this is subject to change. If you wish to use different automatic commit intervals, use the following code: Python C# from quixstreaming import CommitOptions commit_settings = CommitOptions () commit_settings . commit_every = 100 # note, you can set this to none commit_settings . commit_interval = 500 # note, you can set this to none commit_settings . auto_commit_enabled = True input_topic = client . open_input_topic ( 'yourtopic' , commit_settings = commit_settings ) var inputTopic = client . OpenInputTopic ( topic , consumerGroup , new CommitOptions () { CommitEvery = 100 , CommitInterval = 500 , AutoCommitEnabled = true // optional, defaults to true }); The code above will commit every 100 processed messages or 500 ms, whichever is sooner. Manual committing Some use cases need manual committing to mark completion of work, for example when you wish to batch process data, so the frequency of commit depends on the data. This can be achieved by first enabling manual commit for the topic: Python C# from quixstreaming import CommitMode input_topic = client . open_input_topic ( 'yourtopic' , commit_settings = CommitMode . Manual ) client . OpenInputTopic ( topic , consumerGroup , CommitMode . Manual ); Then, whenever your commit condition fulfils, call: Python C# input_topic . commit () inputTopic . Commit (); The piece of code above will commit anything \u2013 like parameter, event or metadata - read and served to you from the input topic up to this point. Commit callback Whenever a commit occurs, an event is raised to let you know. This event is raised for both manual and automatic commits. You can subscribe to this event using the following code: Python C# def on_committed_handler (): # your code doing something when committed to broker input_topic . on_committed += on_committed_handler inputTopic . OnCommitted += ( sender , args ) => { //... your code \u2026 }; Auto Offset Reset You can control the offset that data is read from by optionally specifying AutoOffsetReset when you open the topic. When setting the AutoOffsetReset you can specify one of three options. Option Description Latest Read only the latest data as it arrives, dont include older data Earliest Read from the beginning, i.e. as much as possible Error Throws exception if no previous offset is found The possible options for AutoOffsetReset Python C# input_topic = client . open_input_topic ( test_topic , auto_offset_reset = AutoOffsetReset . Latest ) or input_topic = client . open_input_topic ( test_topic , auto_offset_reset = AutoOffsetReset . Earliest ) var inputTopic = client . OpenInputTopic ( \"MyTopic\" , autoOffset : AutoOffsetReset . Latest ); or var inputTopic = client . OpenInputTopic ( \"MyTopic\" , autoOffset : AutoOffsetReset . Earliest ); Revocation When working with a broker, you have a certain number of topic streams assigned to your consumer. Over the course of the client\u2019s lifetime, there may be several events causing a stream to be revoked, like another client joining or leaving the consumer group, so your application should be prepared to handle these scenarios in order to avoid data loss and/or avoidable reprocessing of messages. Tip Kafka revokes entire partitions, but the SDK makes it easy to determine which streams are affected by providing two events you can listen to. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of the Quix SDK. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the SDK. Streams revoking One or more streams are about to be revoked from your client, but you have a limited time frame \u2013 according to your broker configuration \u2013 to react to this and even commit to the broker: Python C# def on_revoking_handler (): # your code input_topic . on_revoking += on_revoking_handler inputTopic . OnRevoking += ( sender , args ) => { // ... your code ... }; Streams Revoked One or more streams are revoked from your client. You can no longer commit to these streams, you can only handle the revocation in your client. Python C# from quixstreaming import StreamReader def on_streams_revoked_handler ( readers : [ StreamReader ]): for reader in readers : print ( \"Stream \" + reader . stream_id + \" got revoked\" ) input_topic . on_streams_revoked += on_streams_revoked_handler inputTopic . OnStreamsRevoked += ( sender , revokedStreams ) => { // revoked streams are provided to the handler }; Stream Closure You can detect stream closure with the stream closed callback which receives the StreamEndType, to help determine the closure reason if required. Python C# def on_stream_closed_handler ( end_type : StreamEndType ): print ( \"Stream closed with {} \" . format ( end_type )) new_stream . on_stream_closed += on_stream_closed_handler inputTopic . OnStreamReceived += ( s , streamReader ) => { streamReader . OnStreamClosed += ( reader , type ) => { Console . WriteLine ( \"Stream closed with {0}\" , type ); }; }; The StreamEndType can be one of: StreamEndType Description Closed The stream was closed normally Aborted The stream was aborted by your code for your own reasons Terminated The stream was terminated unexpectedly while data was being written Possible end types Minimal example This is a minimal code example you can use to read data from a topic using the Quix SDK. Python C# from quixstreaming import * from quixstreaming.app import App from quixstreaming.models.parametersbufferconfiguration import ParametersBufferConfiguration import sys import signal import threading # Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument. client = QuixStreamingClient () input_topic = client . open_input_topic ( TOPIC_ID ) # read streams def read_stream ( new_stream : StreamReader ): buffer = new_stream . parameters . create_buffer () def on_parameter_data_handler ( data : ParameterData ): df = data . to_panda_frame () print ( df . to_string ()) buffer . on_read += on_parameter_data_handler # Hook up events before initiating read to avoid losing out on any data input_topic . on_stream_received += read_stream # Hook up to termination signal (for docker image) and CTRL-C print ( \"Listening to streams. Press CTRL-C to exit.\" ) # Handle graceful exit App . run () using System ; using System.Linq ; using System.Threading ; using Quix.Sdk.Streaming ; using Quix.Sdk.Streaming.Configuration ; using Quix.Sdk.Streaming.Models ; namespace ReadHelloWorld { class Program { /// <summary> /// Main will be invoked when you run the application /// </summary> static void Main () { // Create a client which holds generic details for creating input and output topics var client = new Quix . Sdk . Streaming . QuixStreamingClient (); using var inputTopic = client . OpenInputTopic ( TOPIC_ID ); // Hook up events before initiating read to avoid losing out on any data inputTopic . OnStreamReceived += ( s , streamReader ) => { Console . WriteLine ( $\"New stream read: {streamReader.StreamId}\" ); var buffer = streamReader . Parameters . CreateBuffer (); buffer . OnRead += parameterData => { Console . WriteLine ( $\"ParameterA - {parameterData.Timestamps[0].Timestamp}: {parameterData.Timestamps.Average(a => a.Parameters[\" ParameterA \"].NumericValue)}\" ); }; }; Console . WriteLine ( \"Listening for streams\" ); // Hook up to termination signal (for docker image) and CTRL-C and open streams App . Run (); Console . WriteLine ( \"Exiting\" ); } } } Read raw kafka messages The Quix SDK uses the message brokers' internal protocol for data transmission. This protocol is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides. However, in some cases, you simply do not have the ability to run the Quix SDK on both sides and you need to have the ability to connect to the data in different ways. To cater for these cases we added the ability to read the raw, unformatted, messages. Using this feature you have the ability to access the raw, unmodified content of each Kafka message from the topic. The data is a byte array, giving you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ). Python C# inp = client . open_raw_input_topic ( TOPIC_ID ) def on_raw_message ( msg ): #bytearray containing bytes received from kafka data = msg . value #broker metadata as dict meta = msg . metadata inp . on_message_read += on_raw_message inp . start_reading () var inp = client . OpenRawInputTopic ( TOPIC_ID ) inp . OnMessageRead += ( message ) => { var data = ( byte []) message . Value ; }; inp . StartReading ()","title":"Reading data"},{"location":"sdk/read/#reading-data","text":"The Quix SDK allows you to read data in real time from the existing streams of your Topics. All the necessary code to read data from your Quix Workspace is auto-generated when you create a project using the existing templates. In this section, we explain more in-depth how to read data using the Quix SDK. Tip The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples.","title":"Reading data"},{"location":"sdk/read/#connect-to-quix","text":"In order to start reading data from Quix you need an instance of the Quix client, QuixStreamingClient . This is the central point where you interact with the main SDK operations. You can create an instance of QuixStreamingClient using the proper constructor of the SDK. Python C# client = QuixStreamingClient () var client = new Quix . Sdk . Streaming . QuixStreamingClient (); You can find more advanced information on how to connect to Quix in the Connect to Quix section.","title":"Connect to Quix"},{"location":"sdk/read/#open-a-topic-for-reading","text":"Topics are the default environment for input/output operations on Quix. In order to access that topic for reading you need an instance of InputTopic . This instance allow you to read all the incoming streams on the specified Topic. You can create an instance of InputTopic using the client\u2019s open_input_topic method, passing the TOPIC_ID or the TOPIC_NAME as a parameter. Python C# input_topic = client . open_input_topic ( TOPIC_ID ) var inputTopic = client . OpenInputTopic ( TOPIC_ID );","title":"Open a topic for reading"},{"location":"sdk/read/#consumer-group","text":"The Consumer group is a concept used when you want to scale horizontally . Each consumer group is identified using an ID, which you set optionally when opening a connection to the topic for reading: Python C# input_topic = client . open_input_topic ( \" {topic} \" , \"{your-consumer-group-id}\" ) var inputTopic = client . OpenInputTopic ( \"{topic}\" , \"{your-consumer-group-id}\" ); When you want to enable horizontal scalability , all the replicas of your process should use the same ConsumerId . This is how the message broker knows that all the replicas of your process want to share the load of the incoming streams between replicas. Each replica will receive only a subset of the streams incoming to the Input Topic. Warning If you want to consume data from the topic locally for debugging purposes, and the model is also deployed in the Quix serverless environment, make sure that you change the consumer group ID to prevent clashing with the cloud deployment. If the clash happens, only one instance will be able to read data of the Stream at a time, and you will probably notice that your code is not receiving data at some point, either locally or in the cloud environment.","title":"Consumer group"},{"location":"sdk/read/#reading-streams","text":"Python C# Once you have the InputTopic instance you can start reading streams. For each stream received to the specified topic, InputTopic will execute the event on_stream_received . You can attach a callback to this event to execute code that reacts when you receive a new Stream. For example, the following code prints the StreamId for each newStream received on that Topic: def read_stream ( new_stream : StreamReader ): print ( \"New stream read:\" + new_stream . stream_id ) input_topic . on_stream_received += read_stream input_topic . start_reading () Once you have the InputTopic instance you can start reading streams. For each stream received to the specified topic, InputTopic will execute the event OnStreamReceived . You can attach a callback to this event to execute code that reacts when you receive a new Stream. For example the following code prints the StreamId for each newStream received on that Topic: inputTopic . OnStreamReceived += ( s , newStream ) => { Console . WriteLine ( $\"New stream read: {newStream.StreamId}\" ); }; inputTopic . StartReading (); Tip The StartReading method indicates to the SDK the moment to start reading streams and data from your Topic. This should normally happen after you\u2019ve registered callbacks for all the events you want to listen to.","title":"Reading Streams"},{"location":"sdk/read/#reading-parameter-data","text":"You can read real-time data from Streams using the on_read event of the StreamReader instance received in the previous callback when you receive a new stream in your Topic. For instance, in the following example we read and print the first timestamp and value of the parameter ParameterA received in the ParameterData packet: Python C# def on_stream_received_handler ( new_stream : StreamReader ): def on_parameter_data_handler ( data : ParameterData ): timestamp = data . timestamps [ 0 ] . timestamp num_value = data . timestamps [ 0 ] . parameters [ 'ParameterA' ] . numeric_value print ( \"ParameterA - \" + str ( timestamp ) + \": \" + str ( num_value )) new_stream . on_read += on_parameter_data_handler input_topic . on_stream_received += on_stream_received_handler input_topic . start_reading () inputTopic . OnStreamReceived += ( s , streamReader ) => { streamReader . Parameters . OnRead += parameterData => { var timestamp = parameterData . Timestamps [ 0 ]. Timestamp ; var numValue = parameterData . Timestamps [ 0 ]. Parameters [ \"ParameterA\" ]. NumericValue ; Console . WriteLine ( $\"ParameterA - {timestamp}: {numValue}\" ); }; }; inputTopic . StartReading (); We use ParameterData packages to read data from the stream. This class handles reading and writing of time series data. The Quix SDK provides multiple helpers for reading and writing data using ParameterData . Tip If you\u2019re using Python you can convert ParameterData to a Pandas DataFrames or read them directly from the SDK. Refer to Using Data Frames for more information.","title":"Reading Parameter Data"},{"location":"sdk/read/#parameter-data-format","text":"ParameterData is the formal class in the SDK which represents a time series data packet in memory. ParameterData consists of a list of Timestamps with their corresponding Parameter Names and Values for each timestamp. You should imagine a Parameter Data as a table where the Timestamp is the first column of that table and where the Parameters are the columns for the Values of that table. Timestamp Speed Gear 1 120 3 2 123 3 3 125 3 6 110 2 An example of ParameterData You can use the timestamps property of a ParameterData instance to access each row of that table, and the parameters property to access the values of that timestamp. The Quix SDK supports Numeric, String, and Binary values and you should use the proper property depending of the value type of your Parameter: Python C# numeric_value : Returns the Numeric value of the Parameter, represented as a float type. string_value : Returns the String value of the Parameter, represented as a string type. binary_value : Returns the Binary value of the Parameter, represented as a bytearray type. NumericValue : Returns the Numeric value of the Parameter, represented as a double type. StringValue : Returns the String value of the Parameter, represented as a string type. BinaryValue : Returns the Binary value of the Parameter, represented as an array of byte . This is a simple example showing how to read Speed values of the ParameterData used in the previous example: Python C# for ts in data . timestamps : timestamp = ts . timestamp_nanoseconds numValue = ts . parameters [ 'Speed' ] . numeric_value print ( \"Speed - \" + str ( timestamp ) \": \" + str ( numValue )) foreach ( var timestamp in data . Timestamps ) { var timestamp = timestamp . TimestampNanoseconds ; var numValue = timestamp . Parameters [ \"Speed\" ]. NumericValue ; Console . WriteLine ( $\"Speed - {timestamp}: {numValue}\" ); } output: Speed - 1: 120 Speed - 2: 123 Speed - 3: 125 Speed - 6: 110","title":"Parameter Data format"},{"location":"sdk/read/#buffer","text":"The Quix SDK provides you with a programmable buffer which you can tailor to your needs. Using buffers to read data enhances the throughput of your application. This helps you to develop Models with a high performance throughput. You can use the buffer property embedded in the Parameters property of your stream , or create a separate instance of that buffer using the create_buffer method: Python C# buffer = newStream . parameters . create_buffer () var buffer = newStream . Parameters . CreateBuffer (); You can configure a buffer\u2019s input requirements using built-in properties. For example, the following configuration means that the Buffer will release a packet when the time span between first and last timestamp inside the buffer reaches 100 milliseconds: Python C# buffer . time_span_in_milliseconds = 100 buffer . TimeSpanInMilliseconds = 100 ; Reading data from that buffer is as simple as using its OnRead event. For each ParameterData packet released from the buffer, the SDK will execute the OnRead event with the parameter data as a given parameter. For example the following code prints the ParameterA value of the first timestamp of each packet released from the buffer: Python C# def on_parameter_data_handler ( data : ParameterData ): timestamp = data . timestamps [ 0 ] . timestamp num_value = data . timestamps [ 0 ] . parameters [ 'ParameterA' ] . numeric_value print ( \"ParameterA - \" + str ( timestamp ) + \": \" + str ( num_value )) buffer . on_read += on_parameter_data_handler buffer . OnRead += ( data ) => { var timestamp = data . Timestamps [ 0 ]. Timestamp ; var numValue = data . Timestamps [ 0 ]. Parameters [ \"ParameterA\" ]. NumericValue ; Console . WriteLine ( $\"ParameterA - {timestamp}: {numValue}\" ); }; You can configure multiple conditions to determine when the Buffer has to release data, if any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer: Python C# buffer.buffer_timeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. buffer.packet_size : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. buffer.time_span_in_nanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. buffer.time_span_in_milliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. buffer.custom_trigger_before_enqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. buffer.custom_trigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. buffer.filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t. Buffer.BufferTimeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. Buffer.PacketSize : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. Buffer.TimeSpanInNanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Buffer.TimeSpanInMilliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. Buffer.CustomTriggerBeforeEnqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. Buffer.CustomTrigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. Buffer.Filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t.","title":"Buffer"},{"location":"sdk/read/#examples","text":"This buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timeout period, it will empty the buffer and send the pending data anyway. Python C# stream . parameters . buffer . packet_size = 100 stream . parameters . buffer . buffer_timeout = 1000 stream . Parameters . Buffer . PacketSize = 100 ; stream . Parameters . Buffer . BufferTimeout = 1000 ; This buffer configuration will send data every 100ms window or if critical data arrives. Python C# buffer . time_span_in_milliseconds = 100 buffer . custom_trigger = lambda data : data . timestamps [ 0 ] . tags [ \"is_critical\" ] == 'True' stream . Parameters . Buffer . TimeSpanInMilliseconds = 100 ; stream . Parameters . Buffer . CustomTrigger = data => data . Timestamps [ 0 ]. Tags [\"is_critical\"] == \"True\" ;","title":"Examples"},{"location":"sdk/read/#using-data-frames","text":"If you use the Python version of the SDK you can use Pandas DataFrames for reading and writing ParameterData to Quix. The Pandas DataFrames format is just a representation of ParameterData format, where the Timestamp is mapped to a column named time and the rest of the parameters are mapped as columns named as the ParameterId of the parameter. Tags are mapped as columns with the prefix TAG__ and the TagId of the tag. For example, the following ParameterData : Timestamp CarId (tag) Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 An example of ParameterData Is represented as the following Pandas DataFrame: time TAG__CarId Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 A representation of ParameterData in a Pandas DataFrame One simple way to read data from Quix using Pandas DataFrames is using the event on_read_pandas instead of the common event on_read when reading from a stream , or when reading data from a buffer: def read_stream ( new_stream : StreamReader ): buffer = new_stream . parameters . create_buffer () def on_pandas_frame_handler ( df : pd . DataFrame ): print ( df . to_string ()) buffer . on_read_pandas += on_pandas_frame_handler input_topic . on_stream_received += read_stream input_topic . start_reading () Alternatively, you can always convert a ParameterData to a Pandas DataFrame using the method to_panda_frame : def read_stream ( new_stream : StreamReader ): buffer = new_stream . parameters . create_buffer () def on_parameter_data_handler ( data : ParameterData ): # read from input stream df = data . to_panda_frame () print ( df . to_string ()) buffer . on_read += on_parameter_data_handler input_topic . on_stream_received += read_stream input_topic . start_reading () Tip The conversions from ParameterData to Pandas DataFrames have an intrinsic cost overhead. For high-performance models using Pandas DataFrames, you probably want to use the on_read_pandas event provided by the SDK, which is optimized for doing as few conversions as possible.","title":"Using Data Frames"},{"location":"sdk/read/#reading-events","text":"EventData is the formal class in the SDK which represents an Event data packet in memory. EventData is meant to be used for time series data coming from sources that generate data at irregular intervals or without a defined structure.","title":"Reading Events"},{"location":"sdk/read/#event-data-format","text":"EventData consists of a record with a Timestamp, an EventId and an EventValue. You should imagine a list of Event Data instances as a simple table of three columns where the Timestamp is the first column of that table and the EventId and EventValue are the second and third columns. Timestamp EventId EventValue 1 failure23 Gearbox has a failure 2 box-event2 Car has entered to the box 3 motor-off Motor has stopped 6 race-event3 Race has finished An example of a list of EventData Reading events from a stream is as easy as reading parameter data. In this case, the SDK does not use a Buffer because we don\u2019t need high performance throughput, but the way we read Event Data from a Stream is identical. Python C# def on_event_data_handler ( data : EventData ): print ( \"Event read for stream. Event Id: \" + data . Id ) new_stream . events . on_read += on_event_data_handler newStream . Events . OnRead += ( data ) => { Console . WriteLine ( $\"Event read for stream. Event Id: {data.Id}\" ); }; output: Event read for stream. Event Id: failure23 Event read for stream. Event Id: box-event2 Event read for stream. Event Id: motor-off Event read for stream. Event Id: race-event3","title":"Event Data format"},{"location":"sdk/read/#committing-checkpointing","text":"It is important to be aware of the commit concept when working with a broker. Committing allows one to mark how far data has been processed, also known as creating a checkpoint. In the event of a restart or rebalance, the client only processes messages from the last commit position. In Kafka this is equivalent to commits for a consumer group . Commits are done for each consumer group, so if you have several consumer groups in use, they do not affect each another when commiting to one of them. Tip Commits are done at a partition level when you use Kafka as a Message Broker, which means that streams that belong to the same partition are committed using the same position. The SDK currently does not expose the option to subscribe to only specific partitions of a topic, but commits will only ever affect partitions that are currently assigned to your client. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of the Quix SDK. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the SDK.","title":"Committing / checkpointing"},{"location":"sdk/read/#automatic-committing","text":"By default, the SDK automatically commits messages for which all handlers returned at a regular default interval, which is every 5 seconds or 5,000 messages, whichever happens sooner. However this is subject to change. If you wish to use different automatic commit intervals, use the following code: Python C# from quixstreaming import CommitOptions commit_settings = CommitOptions () commit_settings . commit_every = 100 # note, you can set this to none commit_settings . commit_interval = 500 # note, you can set this to none commit_settings . auto_commit_enabled = True input_topic = client . open_input_topic ( 'yourtopic' , commit_settings = commit_settings ) var inputTopic = client . OpenInputTopic ( topic , consumerGroup , new CommitOptions () { CommitEvery = 100 , CommitInterval = 500 , AutoCommitEnabled = true // optional, defaults to true }); The code above will commit every 100 processed messages or 500 ms, whichever is sooner.","title":"Automatic committing"},{"location":"sdk/read/#manual-committing","text":"Some use cases need manual committing to mark completion of work, for example when you wish to batch process data, so the frequency of commit depends on the data. This can be achieved by first enabling manual commit for the topic: Python C# from quixstreaming import CommitMode input_topic = client . open_input_topic ( 'yourtopic' , commit_settings = CommitMode . Manual ) client . OpenInputTopic ( topic , consumerGroup , CommitMode . Manual ); Then, whenever your commit condition fulfils, call: Python C# input_topic . commit () inputTopic . Commit (); The piece of code above will commit anything \u2013 like parameter, event or metadata - read and served to you from the input topic up to this point.","title":"Manual committing"},{"location":"sdk/read/#commit-callback","text":"Whenever a commit occurs, an event is raised to let you know. This event is raised for both manual and automatic commits. You can subscribe to this event using the following code: Python C# def on_committed_handler (): # your code doing something when committed to broker input_topic . on_committed += on_committed_handler inputTopic . OnCommitted += ( sender , args ) => { //... your code \u2026 };","title":"Commit callback"},{"location":"sdk/read/#auto-offset-reset","text":"You can control the offset that data is read from by optionally specifying AutoOffsetReset when you open the topic. When setting the AutoOffsetReset you can specify one of three options. Option Description Latest Read only the latest data as it arrives, dont include older data Earliest Read from the beginning, i.e. as much as possible Error Throws exception if no previous offset is found The possible options for AutoOffsetReset Python C# input_topic = client . open_input_topic ( test_topic , auto_offset_reset = AutoOffsetReset . Latest ) or input_topic = client . open_input_topic ( test_topic , auto_offset_reset = AutoOffsetReset . Earliest ) var inputTopic = client . OpenInputTopic ( \"MyTopic\" , autoOffset : AutoOffsetReset . Latest ); or var inputTopic = client . OpenInputTopic ( \"MyTopic\" , autoOffset : AutoOffsetReset . Earliest );","title":"Auto Offset Reset"},{"location":"sdk/read/#revocation","text":"When working with a broker, you have a certain number of topic streams assigned to your consumer. Over the course of the client\u2019s lifetime, there may be several events causing a stream to be revoked, like another client joining or leaving the consumer group, so your application should be prepared to handle these scenarios in order to avoid data loss and/or avoidable reprocessing of messages. Tip Kafka revokes entire partitions, but the SDK makes it easy to determine which streams are affected by providing two events you can listen to. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of the Quix SDK. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the SDK.","title":"Revocation"},{"location":"sdk/read/#streams-revoking","text":"One or more streams are about to be revoked from your client, but you have a limited time frame \u2013 according to your broker configuration \u2013 to react to this and even commit to the broker: Python C# def on_revoking_handler (): # your code input_topic . on_revoking += on_revoking_handler inputTopic . OnRevoking += ( sender , args ) => { // ... your code ... };","title":"Streams revoking"},{"location":"sdk/read/#streams-revoked","text":"One or more streams are revoked from your client. You can no longer commit to these streams, you can only handle the revocation in your client. Python C# from quixstreaming import StreamReader def on_streams_revoked_handler ( readers : [ StreamReader ]): for reader in readers : print ( \"Stream \" + reader . stream_id + \" got revoked\" ) input_topic . on_streams_revoked += on_streams_revoked_handler inputTopic . OnStreamsRevoked += ( sender , revokedStreams ) => { // revoked streams are provided to the handler };","title":"Streams Revoked"},{"location":"sdk/read/#stream-closure","text":"You can detect stream closure with the stream closed callback which receives the StreamEndType, to help determine the closure reason if required. Python C# def on_stream_closed_handler ( end_type : StreamEndType ): print ( \"Stream closed with {} \" . format ( end_type )) new_stream . on_stream_closed += on_stream_closed_handler inputTopic . OnStreamReceived += ( s , streamReader ) => { streamReader . OnStreamClosed += ( reader , type ) => { Console . WriteLine ( \"Stream closed with {0}\" , type ); }; }; The StreamEndType can be one of: StreamEndType Description Closed The stream was closed normally Aborted The stream was aborted by your code for your own reasons Terminated The stream was terminated unexpectedly while data was being written Possible end types","title":"Stream Closure"},{"location":"sdk/read/#minimal-example","text":"This is a minimal code example you can use to read data from a topic using the Quix SDK. Python C# from quixstreaming import * from quixstreaming.app import App from quixstreaming.models.parametersbufferconfiguration import ParametersBufferConfiguration import sys import signal import threading # Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument. client = QuixStreamingClient () input_topic = client . open_input_topic ( TOPIC_ID ) # read streams def read_stream ( new_stream : StreamReader ): buffer = new_stream . parameters . create_buffer () def on_parameter_data_handler ( data : ParameterData ): df = data . to_panda_frame () print ( df . to_string ()) buffer . on_read += on_parameter_data_handler # Hook up events before initiating read to avoid losing out on any data input_topic . on_stream_received += read_stream # Hook up to termination signal (for docker image) and CTRL-C print ( \"Listening to streams. Press CTRL-C to exit.\" ) # Handle graceful exit App . run () using System ; using System.Linq ; using System.Threading ; using Quix.Sdk.Streaming ; using Quix.Sdk.Streaming.Configuration ; using Quix.Sdk.Streaming.Models ; namespace ReadHelloWorld { class Program { /// <summary> /// Main will be invoked when you run the application /// </summary> static void Main () { // Create a client which holds generic details for creating input and output topics var client = new Quix . Sdk . Streaming . QuixStreamingClient (); using var inputTopic = client . OpenInputTopic ( TOPIC_ID ); // Hook up events before initiating read to avoid losing out on any data inputTopic . OnStreamReceived += ( s , streamReader ) => { Console . WriteLine ( $\"New stream read: {streamReader.StreamId}\" ); var buffer = streamReader . Parameters . CreateBuffer (); buffer . OnRead += parameterData => { Console . WriteLine ( $\"ParameterA - {parameterData.Timestamps[0].Timestamp}: {parameterData.Timestamps.Average(a => a.Parameters[\" ParameterA \"].NumericValue)}\" ); }; }; Console . WriteLine ( \"Listening for streams\" ); // Hook up to termination signal (for docker image) and CTRL-C and open streams App . Run (); Console . WriteLine ( \"Exiting\" ); } } }","title":"Minimal example"},{"location":"sdk/read/#read-raw-kafka-messages","text":"The Quix SDK uses the message brokers' internal protocol for data transmission. This protocol is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides. However, in some cases, you simply do not have the ability to run the Quix SDK on both sides and you need to have the ability to connect to the data in different ways. To cater for these cases we added the ability to read the raw, unformatted, messages. Using this feature you have the ability to access the raw, unmodified content of each Kafka message from the topic. The data is a byte array, giving you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ). Python C# inp = client . open_raw_input_topic ( TOPIC_ID ) def on_raw_message ( msg ): #bytearray containing bytes received from kafka data = msg . value #broker metadata as dict meta = msg . metadata inp . on_message_read += on_raw_message inp . start_reading () var inp = client . OpenRawInputTopic ( TOPIC_ID ) inp . OnMessageRead += ( message ) => { var data = ( byte []) message . Value ; }; inp . StartReading ()","title":"Read raw kafka messages"},{"location":"sdk/state-management/","text":"State management A Quix deployment and its underlying code can be restarted multiple times. This can happen because of either user intervention (manually stopping and starting the deployment) or because of a runtime error in the code. In the second case, where a runtime error is detected, the Quix Serverless Environment automatically detects the problem and restarts the underlying service in an attempt to recover from the fault. Due to the code being run in memory, each time a deployment restarts, internal variables will be reset. For example, if you were to calculate the count of the elements in the stream, this counter would get reset on each restart. The counter would then starts at the default variable not knowing what was the last known value in the state of previous run before program stopped working. The Quix SDK has state management built in to allow values to be used and persisted across restarts of a given deployment. We persist the state to dedicated and private key-value pair storage in your workspace. == Usage To use the SDK\u2019s state management feature create an instance of LocalFileStorage . This is in quixstreaming.state.localfilestorage . Then use the set, get, containsKey and clear methods to manipulate the state as needed. Python C# from quixstreaming.state.localfilestorage import LocalFileStorage storage = LocalFileStorage () #clear storage ( remove all keys ) storage . clear () #storage class supports handling of # `str`, `int`, `float`, `bool`, `bytes`, `bytearray` types. #set value storage . set ( \"KEY1\" , 12.51 ) storage . set ( \"KEY2\" , \"str\" ) storage . set ( \"KEY3\" , True ) storage . set ( \"KEY4\" , False ) #check if the storage contains key storage . containsKey ( \"KEY1\" ) #get value value = storage . get ( \"KEY1\" ) C# supports two ways to call the Storage API. Synchronous Asynchronous ( methods are with Async suffix ) The Synchronous API. During a call to these synchronous methods, the program thread execution is blocked. var storage = new LocalFileStorage (); //clear storage ( remove all keys ) await storage . Clear (); //set value to specific key await storage . Set ( \"KEY1\" , 123 ); //long await storage . Set ( \"KEY2\" , 1.23 ); //double await storage . Set ( \"KEY3\" , \"1.23\" ); //string await storage . Set ( \"KEY4\" , new byte []{ 12 , 53 , 23 }); //binary await storage . Set ( \"KEY5\" , false ); //boolean //check if the key exists await storage . ContainsKey ( \"KEY1\" ); //retrieve value from key await storage . GetLong ( \"KEY1\" ); await storage . GetDouble ( \"KEY2\" ); await storage . GetString ( \"KEY3\" ); await storage . GetBinary ( \"KEY4\" ); await storage . GetBinary ( \"KEY5\" ); //list all keys in the storage await storage . GetAllKeys (); The asynchronous API in which methods do contain Async suffix. These methods use the Task-Based Asynchronous Pattern (TAP) and return Tasks. TAP allows us to use async / await and avoid blocking the main thread on longer-running operations. In this case internal I/O. var storage = new LocalFileStorage (); //clear storage ( remove all keys ) await storage . ClearAsync (); //set value to specific key await storage . SetAsync ( \"KEY1\" , 123 ); //long await storage . SetAsync ( \"KEY2\" , 1.23 ); //double await storage . SetAsync ( \"KEY3\" , \"1.23\" ); //string await storage . SetAsync ( \"KEY4\" , new byte []{ 12 , 53 , 23 }); //binary await storage . SetAsync ( \"KEY5\" , false ); //boolean //check if the key exists await storage . ContainsKeyAsync ( \"KEY1\" ); //retrieve value from key await storage . GetLongAsync ( \"KEY1\" ); await storage . GetDoubleAsync ( \"KEY2\" ); await storage . GetStringAsync ( \"KEY3\" ); await storage . GetBinaryAsync ( \"KEY4\" ); await storage . GetBinaryAsync ( \"KEY5\" ); //list all keys in the storage await storage . GetAllKeysAsync ();","title":"State management"},{"location":"sdk/state-management/#state-management","text":"A Quix deployment and its underlying code can be restarted multiple times. This can happen because of either user intervention (manually stopping and starting the deployment) or because of a runtime error in the code. In the second case, where a runtime error is detected, the Quix Serverless Environment automatically detects the problem and restarts the underlying service in an attempt to recover from the fault. Due to the code being run in memory, each time a deployment restarts, internal variables will be reset. For example, if you were to calculate the count of the elements in the stream, this counter would get reset on each restart. The counter would then starts at the default variable not knowing what was the last known value in the state of previous run before program stopped working. The Quix SDK has state management built in to allow values to be used and persisted across restarts of a given deployment. We persist the state to dedicated and private key-value pair storage in your workspace. == Usage To use the SDK\u2019s state management feature create an instance of LocalFileStorage . This is in quixstreaming.state.localfilestorage . Then use the set, get, containsKey and clear methods to manipulate the state as needed. Python C# from quixstreaming.state.localfilestorage import LocalFileStorage storage = LocalFileStorage () #clear storage ( remove all keys ) storage . clear () #storage class supports handling of # `str`, `int`, `float`, `bool`, `bytes`, `bytearray` types. #set value storage . set ( \"KEY1\" , 12.51 ) storage . set ( \"KEY2\" , \"str\" ) storage . set ( \"KEY3\" , True ) storage . set ( \"KEY4\" , False ) #check if the storage contains key storage . containsKey ( \"KEY1\" ) #get value value = storage . get ( \"KEY1\" ) C# supports two ways to call the Storage API. Synchronous Asynchronous ( methods are with Async suffix ) The Synchronous API. During a call to these synchronous methods, the program thread execution is blocked. var storage = new LocalFileStorage (); //clear storage ( remove all keys ) await storage . Clear (); //set value to specific key await storage . Set ( \"KEY1\" , 123 ); //long await storage . Set ( \"KEY2\" , 1.23 ); //double await storage . Set ( \"KEY3\" , \"1.23\" ); //string await storage . Set ( \"KEY4\" , new byte []{ 12 , 53 , 23 }); //binary await storage . Set ( \"KEY5\" , false ); //boolean //check if the key exists await storage . ContainsKey ( \"KEY1\" ); //retrieve value from key await storage . GetLong ( \"KEY1\" ); await storage . GetDouble ( \"KEY2\" ); await storage . GetString ( \"KEY3\" ); await storage . GetBinary ( \"KEY4\" ); await storage . GetBinary ( \"KEY5\" ); //list all keys in the storage await storage . GetAllKeys (); The asynchronous API in which methods do contain Async suffix. These methods use the Task-Based Asynchronous Pattern (TAP) and return Tasks. TAP allows us to use async / await and avoid blocking the main thread on longer-running operations. In this case internal I/O. var storage = new LocalFileStorage (); //clear storage ( remove all keys ) await storage . ClearAsync (); //set value to specific key await storage . SetAsync ( \"KEY1\" , 123 ); //long await storage . SetAsync ( \"KEY2\" , 1.23 ); //double await storage . SetAsync ( \"KEY3\" , \"1.23\" ); //string await storage . SetAsync ( \"KEY4\" , new byte []{ 12 , 53 , 23 }); //binary await storage . SetAsync ( \"KEY5\" , false ); //boolean //check if the key exists await storage . ContainsKeyAsync ( \"KEY1\" ); //retrieve value from key await storage . GetLongAsync ( \"KEY1\" ); await storage . GetDoubleAsync ( \"KEY2\" ); await storage . GetStringAsync ( \"KEY3\" ); await storage . GetBinaryAsync ( \"KEY4\" ); await storage . GetBinaryAsync ( \"KEY5\" ); //list all keys in the storage await storage . GetAllKeysAsync ();","title":"State management"},{"location":"sdk/write/","text":"Writing data You write data to Quix using streams in your topic. The Quix SDK allows you to create new streams, append data to existing streams, organize streams in folders, and add context data to the streams. All the necessary code to write data to your Quix Workspace is auto-generated when you create a project using the existing templates. In this section, we explain more in-depth how to write data using the Quix SDK. Tip The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples. Connect to Quix In order to start writing data to Quix you need an instance of the Quix client, QuixStreamingClient . This is the central point where you interact with the main SDK operations. You can create an instance of QuixStreamingClient using the proper constructor of the SDK. Python C# client = QuixStreamingClient () var client = new Quix . Sdk . Streaming . QuixStreamingClient (); You can find more advanced information on how to connect to Quix in the Connect to Quix section. Open a topic Topics are the default environment for input/output real-time operations on Quix. In order to access that topic for writing you need an instance of OutputTopic . You can create an instance of OutputTopic using the client\u2019s open_output_topic method, passing the TOPIC_ID or the TOPIC_NAME as a parameter. Python C# output_topic = client . open_output_topic ( TOPIC_ID ) var outputTopic = client . OpenOutputTopic ( TOPIC_ID ); Create / Attach to a Stream Streams are the central context of data in Quix. Streams make easy to manage, discover, and work with your data. They are key to good data governance in your organisation. Also, Streams are vital for parallelizing huge data loads with an infinite number of data sources. You can create as many streams as you want using the create_stream method of your OutputTopic instance. Python C# stream = output_topic . create_stream () var stream = outputTopic . CreateStream (); A stream ID is auto-generated but you can also pass a StreamId to the method to append or update data of an existing stream. Python C# stream = output_topic . create_stream ( \"existing-stream-id\" ) var stream = outputTopic . CreateStream ( \"existing-stream-id\" ); Stream Properties As an option, you can add context to your streams by adding a name, some metadata, or a default location. You can add this metadata to a stream using the Properties options of the generated stream instance. Python C# stream . properties . name = \"Hello World Python stream\" stream . properties . location = \"/test/location\" stream . properties . metadata [ \"meta\" ] = \"is\" stream . properties . metadata [ \"working\" ] = \"well\" stream . Properties . Name = \"Hello World C# stream\" ; stream . Properties . Location = \"/test/location\" ; stream . Properties . Metadata [ \"meta\" ] = \"is\" ; stream . Properties . Metadata [ \"working\" ] = \"well\" ; Stream Name The stream name is the display name of your stream in the platform. If you specify one, Quix will use it instead of the Stream Id to represent your stream inside the platform. For example, the following name: Python C# stream . properties . name = \"Hello World my first stream\" stream . Properties . Name = \"Hello World my first stream\" ; Would result in this visualization in the list of streams of your workspace: Stream Location The stream location property defines a default folder for the stream in the folder structure of your Persisted steams. For example, the following location: Python C# stream . properties . location = \"/Game/Codemasters/F1-2019/ {track} \" stream . Properties . Location = $\"/Game/Codemasters/F1-2019/{track}\" Would result in this hierarchy: Any streams sent without a location property will be located under the \"Root\" level by default. Close a Stream Streams can be left open 24/7 if you aren\u2019t sure when the next data will arrive, but they can and should be closed when you know that you have all the data you need. They will also be closed automatically when your service stops. However, sometimes a stream can be closed for other reasons e.g. if an error occurrs in the writer code or something unexpected happens. These snippets show you how to close a stream and how to specify the StreamEndType. Python C# stream . close () stream . close ( StreamEndType . Closed ) stream . close ( StreamEndType . Aborted ) stream . close ( StreamEndType . Terminated ) stream . Close (); stream . Close ( StreamEndType . Closed ); stream . Close ( StreamEndType . Aborted ); stream . Close ( StreamEndType . Terminated ); The StreamEndType can be one of: StreamEndType Description Closed The stream was closed normally Aborted The stream was aborted by your code for your own reasons Terminated The stream was terminated unexpectedly while data was being written Possible end types Writing Parameter Data You can now start writing data to your stream. ParameterData is the formal class in the SDK which represents a time-series data packet in memory. ParameterData is meant to be used for time-series data coming from sources that generate data at a regular time basis and with a fixed number of Parameters. Tip If your data source generates data at irregular time intervals and you don\u2019t have a defined list of regular Parameters, the EventData format is probably a better fit for your time-series data. Parameter Data format ParameterData is the formal class in the SDK which represents a time series data packet in memory. ParameterData consists of a list of Timestamps with their corresponding Parameter Names and Values for each timestamp. You should imagine a Parameter Data as a table where the Timestamp is the first column of that table and where the Parameters are the columns for the Values of that table. Timestamp Speed Gear 1 120 3 2 123 3 3 125 3 6 110 2 An example of ParameterData Tip The Timestamp column plus the Tags assigned to it work as the index of that table. If you add values for the same Timestamp and Tags combination, only the last Values will be sent to the stream. The Quix SDK provides several helpers to create and send ParameterData packets through the stream. The following code would generate the previous ParameterData and send it to the stream: Python C# data = ParameterData () data . add_timestamp_nanoseconds ( 1 ) \\ . add_value ( \"Speed\" , 120 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 2 ) \\ . add_value ( \"Speed\" , 123 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 3 ) \\ . add_value ( \"Speed\" , 125 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 6 ) \\ . add_value ( \"Speed\" , 110 ) \\ . add_value ( \"Gear\" , 2 ) stream . parameters . write ( data ) var data = new ParameterData (); data . AddTimestampNanoseconds ( 1 ) . AddValue ( \"Speed\" , 120 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 2 ) . AddValue ( \"Speed\" , 123 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 3 ) . AddValue ( \"Speed\" , 125 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 6 ) . AddValue ( \"Speed\" , 110 ) . AddValue ( \"Gear\" , 2 ); stream . Parameters . Write ( data ); Although Quix allows you to send ParameterData to a stream directly, without any buffering, we recommended you use the built-in Buffer feature to achieve high throughput speeds. The following code would send the same ParameterData through a buffer: Python C# stream . parameters . buffer . write ( data ) stream . Parameters . Buffer . Write ( data ); Visit the Buffer section of this documentation to find out more about the built-in Buffer feature. The Quix SDK allows you to attach any type of data \u2014 Numbers, Strings, or raw Binary data \u2014 to your timestamps. The following code will attach one of each to the same timestamp: Python C# data = ParameterData () data . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . add_value ( \"ParameterC\" , bytearray ( \"hello, Quix!\" , 'utf-8' )) # use bytearray to write binary data to a stream. var data = new ParameterData (); data . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . AddValue ( \"ParameterC\" , Encoding . ASCII . GetBytes ( \"Hello Quix!\" )); // Write binary data as a byte array. Timestamps The Quix SDK supports common date and time formats for timestamps when adding data to a stream. The SDK gives you several helper functions to add new timestamps to Buffer , ParamaterData , and EventData instances with several types of date/time formats. These are all the common helper functions: Python C# add_timestamp(datetime: datetime) : Add a new timestamp in datetime format. Default epoch will never be added to this. add_timestamp(time: timedelta) : Add a new timestamp in timedelta format since the default epoch determined in the stream. add_timestamp_milliseconds(milliseconds: int) : Add a new timestamp in milliseconds since the default epoch determined in the stream. add_timestamp_nanoseconds(nanoseconds: int) : Add a new timestamp in nanoseconds since the default epoch determined in the stream. AddTimestamp(DateTime dateTime) : Add a new timestamp in DateTime format. Default Epoch will never be added to this. AddTimestamp(TimeSpan timeSpan) : Add a new timestamp in TimeSpan format since the default Epoch determined in the stream. AddTimestampMilliseconds(long timeMilliseconds) : Add a new timestamp in milliseconds since the default Epoch determined in the stream. AddTimestampNanoseconds(long timeNanoseconds) : Add a new timestamp in nanoseconds since the default Epoch determined in the stream. Epoch There is a stream property called Epoch (set to 0 by default) that is added to every timestamp (except for datetime formats) when it\u2019s added to the stream. You can use any value you like to act as a base, from which point timestamps will be relative to. The following code indicates to the SDK to add the current date/time to each timestamp added to the stream. Python C# stream . epoch = date . today () stream . Epoch = DateTime . Today ; Adding data without using Epoch property: Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . Write (); Or we can add a timestamp 1000ms from the epoch \"Today\" : Python C# stream . epoch = date . today () stream . parameters . buffer \\ . add_timestamp_milliseconds ( 1000 ) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . write () stream . Epoch = DateTime . Today ; stream . Parameters . Buffer . AddTimestampInMilliseconds ( 1000 ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . Write (); Buffer The Quix SDK provides a built in Buffer to help you achieve high performance data streaming without the complexity of managing underlying streaming technologies. Instead, you just have to configure the buffer with your requirements using the property Buffer present in the Parameters property of your stream. For example the following configuration means that the SDK will send a packet when the size of the buffer reaches 100 timestamps: Python C# stream . parameters . buffer . packet_size = 100 stream . Parameters . Buffer . PacketSize = 100 ; Writing a ParameterData to that buffer is as simple as using the Write method of that built-in Buffer , passing the ParameterData to write: Python C# stream . parameters . buffer . write ( data ) stream . Parameters . Buffer . Write ( data ); The Quix SDK also allows you to write data to the buffer without creating a ParameterData instance explicitly. To do so, you can use the same helper methods that are supported by the ParameterData class like add_timestamp , add_value or add_tag . At the end, use the write method to write that timestamp to the buffer. This is an example of how to write data to the buffer without using an explicit ParameterData instance: Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . add_value ( \"ParameterC\" , bytearray ( \"hello, Quix!\" , 'utf-8' )) # use bytearray to write binary data to a stream. . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . AddValue ( \"ParameterC\" , Encoding . ASCII . GetBytes ( \"Hello Quix!\" )) // Write binary data as a byte array. . Write (); You can configure multiple conditions to determine when the Buffer has to release data. If any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer: Python C# buffer.buffer_timeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. buffer.packet_size : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. buffer.time_span_in_nanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. buffer.time_span_in_milliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. buffer.custom_trigger_before_enqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. buffer.custom_trigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. buffer.filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t. Buffer.BufferTimeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. Buffer.PacketSize : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. Buffer.TimeSpanInNanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Buffer.TimeSpanInMilliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. Buffer.CustomTriggerBeforeEnqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. Buffer.CustomTrigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. Buffer.Filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t. Examples This buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timout period, it will flush and empty the buffer anyway. Python C# stream . parameters . buffer . time_span_in_milliseconds = 100 stream . parameters . buffer . buffer_timeout = 1000 stream . Parameters . Buffer . TimeSpanInMilliseconds = 100 ; stream . Parameters . Buffer . BufferTimeout = 1000 ; This buffer configuration will send data every 100ms window or if critical data arrives. Python C# stream . parameters . buffer . time_span_in_milliseconds = 100 stream . parameters . buffer . custom_trigger = lambda data : data . timestamps [ 0 ] . tags [ \"is_critical\" ] == 'True' stream . Parameters . Buffer . TimeSpanInMilliseconds = 100 ; stream . Parameters . Buffer . CustomTrigger = data => data . Timestamps [ 0 ]. Tags [ \"is_critical\" ] == \"True\" ; Parameter Definitions The Quix SDK allows you to define metadata for parameters and events, to describe them. You can define things like human readable names, descriptions, acceptable ranges of values, etc. Quix uses some of this configuration when visualising data on the platform, but you can use also them in your own models, bridges or visualization implementations. Python C# We call this parameter metadata ParameterDefinitions , and all you need to do is to use the add_definition helper function of the stream.parameters property: parameters . add_definition ( parameter_id : str , name : str = None , description : str = None ) We call this parameter metadata ParameterDefinitions , and all you need to do is to use the AddDefinition helper function of the stream.Parameters property: Parameters . AddDefinition ( parameterId : string , name : string = null , description : string = null ) Once you have added a new definition, you can attach some additional configuration to it. This is the whole list of visualization and metadata options you can attach to a ParameterDefinition : Python C# set_range(minimum_value: float, maximum_value: float) : Set the minimum and maximum range of the parameter. set_unit(unit: str) : Set the unit of the parameter. set_format(format: str) : Set the format of the parameter. set_custom_properties(custom_properties: str) : Set the custom properties of the parameter. Example: stream . parameters \\ . add_location ( \"vehicle/ecu\" ) \\ . add_definition ( \"vehicle-speed\" , \"Vehicle speed\" , \"Current vehicle speed measured using wheel sensor\" ) \\ . set_unit ( \"kmh\" ) \\ . set_range ( 0 , 400 ) SetRange(double minimumValue, double maximumValue) : Set the minimum and maximum range of the parameter. SetUnit(string unit) : Set the unit of the parameter. SetFormat(string format) : Set the format of the parameter. SetCustomProperties(string customProperties) : Set the custom properties of the parameter. Example: stream . Parameters . AddLocation ( \"vehicle/ecu\" ) . AddDefinition ( \"vehicle-speed\" , \"Vehicle speed\" , \"Current vehicle speed measured using wheel sensor\" ) . SetUnit ( \"kmh\" ) . SetRange ( 0 , 400 ); The Min and Max range definition sets the Y axis range in the waveform visualisation view. This definition: Python C# . add_definition ( \"Speed\" ) . set_range ( 0 , 400 ) . AddDefinition ( \"Speed\" ). SetRange ( 0 , 400 ) Will set up this view in Visualise: Adding additional Definitions for each parameter allows you to see data with different ranges on the same waveform view: You can also define a Location before adding parameter and event definitions. Locations are used to organize the Parameters and Events in hierarchy groups in the data catalogue. To add a Location you should use the add_location method before adding the definitions you want to include in that group. For example, setting this parameter location: Python C# stream . parameters \\ . add_location ( \"/Player/Motion/Car\" ) \\ . add_definition ( \"Pitch\" ) \\ . add_definition ( \"Roll\" ) \\ . add_definition ( \"Yaw\" ) stream . Parameters . AddLocation ( \"/Player/Motion/Car\" ) . AddDefinition ( \"Pitch\" ) . AddDefinition ( \"Roll\" ) . AddDefinition ( \"Yaw\" ); Will result in this parameter hierarchy in the parameter selection dialogue: Using Data Frames If you use the Python version of the SDK you can use Pandas DataFrames for reading and writing ParameterData to Quix. The Pandas DataFrames format is just a representation of ParameterData format, where the Timestamp is mapped to a column named time and the rest of the parameters are mapped as columns named as the ParameterId of the parameter. Tags are mapped as columns with the prefix TAG__ and the TagId of the tag. For example, the following ParameterData : Timestamp CarId (tag) Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 An example of ParameterData Is represented as the following Pandas DataFrame: time TAG__CarId Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 A representation of ParameterData in a Pandas DataFrame The SDK allows you to write data to Quix using Pandas DataFrames directly. You just need to use the common write methods of the stream.parameters and buffer , passing the Data Frame instead of a ParameterData : df = data . to_panda_frame () stream . parameters . buffer . write ( df ) Alternatively, you can convert a Pandas Data Frame to a ParameterData using the method from_panda_frame : data = ParameterData . from_panda_frame ( df ) stream . parameters . buffer . write ( data ) Tip The conversions from Pandas DataFrames to ParameterData have an intrinsic cost overhead. For high-performance models using Pandas DataFrames, you probably want to directly use Pandas DataFrames methods provided by the SDK that are optimized for doing as few conversions as possible. Writing Events EventData is the formal class in the SDK which represents an Event data packet in memory. EventData is meant to be used for time series data coming from sources that generate data at irregular intervals or without a defined structure. Tip If your data source generates data at regular time intervals, or the information can be organized in a fixed list of Parameters, the ParameterData format is probably a better fit for your time-series data. Writing Events to a stream is identical to writing ParameterData values, although you don\u2019t need to use buffering features because events don\u2019t need high performance throughput. Event Data format EventData consists of a record with a Timestamp, an EventId and an EventValue. You should imagine a list of Event Data instances as a simple table of three columns where the Timestamp is the first column of that table and the EventId and EventValue are the second and third columns. Timestamp EventId EventValue 1 failure23 Gearbox has a failure 2 box-event2 Car has entered to the box 3 motor-off Motor has stopped 6 race-event3 Race has finished An example of a list of EventData The Quix SDK provides several helpers to create and send EventData packets through the stream. The following code would generate the list of EventData shown in the previous example and send it to the stream: Python C# events = [] events . append ( EventData ( \"failure23\" , 1 , \"Gearbox has a failure\" )) events . append ( EventData ( \"box-event2\" , 2 , \"Car has entered to the box\" )) events . append ( EventData ( \"motor-off\" , 3 , \"Motor has stopped\" )) events . append ( EventData ( \"race-event3\" , 6 , \"Race has finished\" )) stream . events . write ( events ) var events = new List < EventData >(); events . Add ( new EventData ( \"failure23\" , 1 , \"Gearbox has a failure\" )); events . Add ( new EventData ( \"box-event2\" , 2 , \"Car has entered to the box\" )); events . Add ( new EventData ( \"motor-off\" , 3 , \"Motor has stopped\" )); events . Add ( new EventData ( \"race-event3\" , 6 , \"Race has finished\" )); stream . Events . Write ( events ) The Quix SDK lets you write Events without creating EventData instances explicitly. To do so, you can use the same helpers present in ParameterData format like add_timestamp , add_value or add_tag . At the end, use the write method to write that timestamp to the stream. This is an example of how to write Events to the stream without using explicit EventData instances: Python C# stream . events \\ . add_timestamp ( 1 ) \\ . add_value ( \"failure23\" , \"Gearbox has a failure\" ) \\ . write () stream . events \\ . add_timestamp ( 2 ) \\ . add_value ( \"box-event2\" , \"Car has entered to the box\" ) \\ . write () stream . events \\ . add_timestamp ( 3 ) \\ . add_value ( \"motor-off\" , \"Motor has stopped\" ) \\ . write () stream . events \\ . add_timestamp ( 6 ) \\ . add_value ( \"race-event3\" , \"Race has finished\" ) \\ . write () stream . Events . AddTimestamp ( 1 ) . AddValue ( \"failure23\" , \"Gearbox has a failure\" ) . Write (); stream . Events . AddTimestamp ( 2 ) . AddValue ( \"box-event2\" , \"Car has entered to the box\" ) . Write (); stream . Events . AddTimestamp ( 3 ) . AddValue ( \"motor-off\" , \"Motor has stopped\" ) . Write (); stream . Events . AddTimestamp ( 6 ) . AddValue ( \"race-event3\" , \"Race has finished\" ) . Write (); Event Definitions As with parameters, you can attach Definitions to each event. This is the whole list of visualization and metadata options we can attach to a EventDefinition : set_level(level: EventLevel) : Set severity level of the event. set_custom_properties(custom_properties: str) : Set the custom properties of the event. For example, the following code defines a human readable name and a Severity level for the EventA : Python C# stream . events \\ . add_definition ( \"EventA\" , \"The Event A\" ) \\ . set_level ( EventLevel . Critical ) stream . Events . AddDefinition ( \"EventA\" , \"The Event A\" ). SetLevel ( EventLevel . Critical ); Tags The Quix SDK allows you to tag data for ParameterData and EventData packets. Using tags alongside parameters and events helps when indexing persisted data in the database. Tags allow you to filter and group data with fast queries. Tags work as a part of the primary key inside ParameterData and EventData, in combination with the default Timestamp key. If you add data values with the same Timestamps, but a different combination of Tags, the timestamp will be treated as a separate row. For example, the following code: Python C# data = ParameterData () data . add_timestamp_nanoseconds ( 1 ) \\ . add_tag ( \"CarId\" , \"car1\" ) \\ . add_value ( \"Speed\" , 120 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 2 ) \\ . add_tag ( \"CarId\" , \"car1\" ) \\ . add_value ( \"Speed\" , 123 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 3 ) \\ . add_tag ( \"CarId\" , \"car1\" ) \\ . add_value ( \"Speed\" , 125 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 1 ) \\ . add_tag ( \"CarId\" , \"car2\" ) \\ . add_value ( \"Speed\" , 95 ) \\ . add_value ( \"Gear\" , 2 ) data . add_timestamp_nanoseconds ( 2 ) \\ . add_tag ( \"CarId\" , \"car2\" ) \\ . add_value ( \"Speed\" , 98 ) \\ . add_value ( \"Gear\" , 2 ) data . add_timestamp_nanoseconds ( 3 ) \\ . add_tag ( \"CarId\" , \"car2\" ) \\ . add_value ( \"Speed\" , 105 ) \\ . add_value ( \"Gear\" , 2 ) var data = new ParameterData (); data . AddTimestampNanoseconds ( 1 ) . AddTag ( \"CarId\" , \"car1\" ) . AddValue ( \"Speed\" , 120 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 2 ) . AddTag ( \"CarId\" , \"car1\" ) . AddValue ( \"Speed\" , 123 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 3 ) . AddTag ( \"CarId\" , \"car1\" ) . AddValue ( \"Speed\" , 125 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 1 ) . AddTag ( \"CarId\" , \"car2\" ) . AddValue ( \"Speed\" , 95 ) . AddValue ( \"Gear\" , 2 ); data . AddTimestampNanoseconds ( 2 ) . AddTag ( \"CarId\" , \"car2\" ) . AddValue ( \"Speed\" , 98 ) . AddValue ( \"Gear\" , 2 ); data . AddTimestampNanoseconds ( 3 ) . AddTag ( \"CarId\" , \"car2\" ) . AddValue ( \"Speed\" , 105 ) . AddValue ( \"Gear\" , 2 ); Will generate the following ParameterData packet: Timestamp CarId Speed Gear 1 car1 120 3 1 car2 95 2 2 car1 123 3 2 car2 98 2 3 car1 125 3 3 car2 105 2 ParameterData with tagged data Warning Tags have to be chosen carefully as excessive cardinality leads to performance degradation in the database. You should use tags only for identifiers and not cardinal values. Good tagging: This will allow you to query the maximum speed for driver identifier \"Peter\". Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_tag ( \"vehicle-plate\" , \"SL96 XCX\" ) \\ . add_tag ( \"driver-id\" , \"Peter\" ) \\ . add_value ( \"Speed\" , 53 ) \\ . add_value ( \"Gear\" , 4 ) \\ . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddTag ( \"vehicle-plate\" , \"SL96 XCX\" ) . AddTag ( \"driver-id\" , \"Peter\" ) . AddValue ( \"Speed\" , 53 ) . AddValue ( \"Gear\" , 4 ) . Write (); Bad tagging: This will lead to excessive cardinality as there will be a massive number of different values for the specified tag, Speed. Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_tag ( \"Speed\" , 53 ) \\ . add_value ( \"Gear\" , 4 ) \\ . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddTag ( \"Speed\" , 53 ) . AddValue ( \"Gear\" , 4 ) . Write (); Minimal example This is a minimal code example you can use to write data to a topic using the Quix SDK. Python C# import time import datetime import math from quixstreaming import * # Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument. client = QuixStreamingClient () output_topic = client . open_output_topic ( TOPIC_ID ) stream = output_topic . create_stream () stream . properties . name = \"Hello World python stream\" for index in range ( 0 , 3000 ): stream . parameters \\ . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , index ) \\ . write () time . sleep ( 0.01 ) print ( \"Closing stream\" ) stream . close () using System ; using System.Threading ; using Quix.Sdk.Streaming.Configuration ; namespace WriteHelloWorld { class Program { /// <summary> /// Main will be invoked when you run the application /// </summary> static void Main () { // Create a client which holds generic details for creating input and output topics var client = new Quix . Sdk . Streaming . QuixStreamingClient (); using var outputTopic = client . OpenOutputTopic ( TOPIC_ID ); var stream = outputTopic . CreateStream (); stream . Properties . Name = \"Hello World stream\" ; Console . WriteLine ( \"Sending values for 30 seconds\" ); for ( var index = 0 ; index < 3000 ; index ++) { stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , index ) . Write (); Thread . Sleep ( 10 ); } Console . WriteLine ( \"Closing stream\" ); stream . Close (); Console . WriteLine ( \"Done!\" ); } } } Write raw kafka messages The Quix SDK uses the message brokers' internal protocol for data transmission. This protocol is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides. However, in some cases, you simply do not have the ability to run the Quix SDK on both sides. To cater for these cases we added the ability to write the raw, unformatted, messages as a byte array. This gives you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ). You can write messages with or without a key. In the following example, we show you how to write 2 messages to Kafka, one message with a key and one without. Python C# inp = client . open_raw_output_topic ( TOPIC_ID ) data = bytearray ( bytes ( \"TEXT CONVERTED TO BYTES\" , 'utf-8' )) #write value with KEY to kafka message = RawMessage ( data ) message . key = MESSAGE_KEY out . write ( message ) #write value without key into kafka out . write ( data ) var out = client . OpenRawOutputTopic ( TOPIC_ID ); inp = client . OpenRawInputTopic ( TOPIC_NAME ) var data = new byte []{ 1 , 3 , 5 , 7 , 1 , 43 }; //write value with KEY to kafka rawWriter . Write ( new Streaming . Raw . RawMessage ( MESSAGE_KEY , data )); //write value withhout key into kafka rawWriter . Write ( new Streaming . Raw . RawMessage ( data )); inp . StartReading ()","title":"Writing data"},{"location":"sdk/write/#writing-data","text":"You write data to Quix using streams in your topic. The Quix SDK allows you to create new streams, append data to existing streams, organize streams in folders, and add context data to the streams. All the necessary code to write data to your Quix Workspace is auto-generated when you create a project using the existing templates. In this section, we explain more in-depth how to write data using the Quix SDK. Tip The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples.","title":"Writing data"},{"location":"sdk/write/#connect-to-quix","text":"In order to start writing data to Quix you need an instance of the Quix client, QuixStreamingClient . This is the central point where you interact with the main SDK operations. You can create an instance of QuixStreamingClient using the proper constructor of the SDK. Python C# client = QuixStreamingClient () var client = new Quix . Sdk . Streaming . QuixStreamingClient (); You can find more advanced information on how to connect to Quix in the Connect to Quix section.","title":"Connect to Quix"},{"location":"sdk/write/#open-a-topic","text":"Topics are the default environment for input/output real-time operations on Quix. In order to access that topic for writing you need an instance of OutputTopic . You can create an instance of OutputTopic using the client\u2019s open_output_topic method, passing the TOPIC_ID or the TOPIC_NAME as a parameter. Python C# output_topic = client . open_output_topic ( TOPIC_ID ) var outputTopic = client . OpenOutputTopic ( TOPIC_ID );","title":"Open a topic"},{"location":"sdk/write/#create-attach-to-a-stream","text":"Streams are the central context of data in Quix. Streams make easy to manage, discover, and work with your data. They are key to good data governance in your organisation. Also, Streams are vital for parallelizing huge data loads with an infinite number of data sources. You can create as many streams as you want using the create_stream method of your OutputTopic instance. Python C# stream = output_topic . create_stream () var stream = outputTopic . CreateStream (); A stream ID is auto-generated but you can also pass a StreamId to the method to append or update data of an existing stream. Python C# stream = output_topic . create_stream ( \"existing-stream-id\" ) var stream = outputTopic . CreateStream ( \"existing-stream-id\" );","title":"Create / Attach to a Stream"},{"location":"sdk/write/#stream-properties","text":"As an option, you can add context to your streams by adding a name, some metadata, or a default location. You can add this metadata to a stream using the Properties options of the generated stream instance. Python C# stream . properties . name = \"Hello World Python stream\" stream . properties . location = \"/test/location\" stream . properties . metadata [ \"meta\" ] = \"is\" stream . properties . metadata [ \"working\" ] = \"well\" stream . Properties . Name = \"Hello World C# stream\" ; stream . Properties . Location = \"/test/location\" ; stream . Properties . Metadata [ \"meta\" ] = \"is\" ; stream . Properties . Metadata [ \"working\" ] = \"well\" ;","title":"Stream Properties"},{"location":"sdk/write/#stream-name","text":"The stream name is the display name of your stream in the platform. If you specify one, Quix will use it instead of the Stream Id to represent your stream inside the platform. For example, the following name: Python C# stream . properties . name = \"Hello World my first stream\" stream . Properties . Name = \"Hello World my first stream\" ; Would result in this visualization in the list of streams of your workspace:","title":"Stream Name"},{"location":"sdk/write/#stream-location","text":"The stream location property defines a default folder for the stream in the folder structure of your Persisted steams. For example, the following location: Python C# stream . properties . location = \"/Game/Codemasters/F1-2019/ {track} \" stream . Properties . Location = $\"/Game/Codemasters/F1-2019/{track}\" Would result in this hierarchy: Any streams sent without a location property will be located under the \"Root\" level by default.","title":"Stream Location"},{"location":"sdk/write/#close-a-stream","text":"Streams can be left open 24/7 if you aren\u2019t sure when the next data will arrive, but they can and should be closed when you know that you have all the data you need. They will also be closed automatically when your service stops. However, sometimes a stream can be closed for other reasons e.g. if an error occurrs in the writer code or something unexpected happens. These snippets show you how to close a stream and how to specify the StreamEndType. Python C# stream . close () stream . close ( StreamEndType . Closed ) stream . close ( StreamEndType . Aborted ) stream . close ( StreamEndType . Terminated ) stream . Close (); stream . Close ( StreamEndType . Closed ); stream . Close ( StreamEndType . Aborted ); stream . Close ( StreamEndType . Terminated ); The StreamEndType can be one of: StreamEndType Description Closed The stream was closed normally Aborted The stream was aborted by your code for your own reasons Terminated The stream was terminated unexpectedly while data was being written Possible end types","title":"Close a Stream"},{"location":"sdk/write/#writing-parameter-data","text":"You can now start writing data to your stream. ParameterData is the formal class in the SDK which represents a time-series data packet in memory. ParameterData is meant to be used for time-series data coming from sources that generate data at a regular time basis and with a fixed number of Parameters. Tip If your data source generates data at irregular time intervals and you don\u2019t have a defined list of regular Parameters, the EventData format is probably a better fit for your time-series data.","title":"Writing Parameter Data"},{"location":"sdk/write/#parameter-data-format","text":"ParameterData is the formal class in the SDK which represents a time series data packet in memory. ParameterData consists of a list of Timestamps with their corresponding Parameter Names and Values for each timestamp. You should imagine a Parameter Data as a table where the Timestamp is the first column of that table and where the Parameters are the columns for the Values of that table. Timestamp Speed Gear 1 120 3 2 123 3 3 125 3 6 110 2 An example of ParameterData Tip The Timestamp column plus the Tags assigned to it work as the index of that table. If you add values for the same Timestamp and Tags combination, only the last Values will be sent to the stream. The Quix SDK provides several helpers to create and send ParameterData packets through the stream. The following code would generate the previous ParameterData and send it to the stream: Python C# data = ParameterData () data . add_timestamp_nanoseconds ( 1 ) \\ . add_value ( \"Speed\" , 120 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 2 ) \\ . add_value ( \"Speed\" , 123 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 3 ) \\ . add_value ( \"Speed\" , 125 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 6 ) \\ . add_value ( \"Speed\" , 110 ) \\ . add_value ( \"Gear\" , 2 ) stream . parameters . write ( data ) var data = new ParameterData (); data . AddTimestampNanoseconds ( 1 ) . AddValue ( \"Speed\" , 120 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 2 ) . AddValue ( \"Speed\" , 123 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 3 ) . AddValue ( \"Speed\" , 125 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 6 ) . AddValue ( \"Speed\" , 110 ) . AddValue ( \"Gear\" , 2 ); stream . Parameters . Write ( data ); Although Quix allows you to send ParameterData to a stream directly, without any buffering, we recommended you use the built-in Buffer feature to achieve high throughput speeds. The following code would send the same ParameterData through a buffer: Python C# stream . parameters . buffer . write ( data ) stream . Parameters . Buffer . Write ( data ); Visit the Buffer section of this documentation to find out more about the built-in Buffer feature. The Quix SDK allows you to attach any type of data \u2014 Numbers, Strings, or raw Binary data \u2014 to your timestamps. The following code will attach one of each to the same timestamp: Python C# data = ParameterData () data . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . add_value ( \"ParameterC\" , bytearray ( \"hello, Quix!\" , 'utf-8' )) # use bytearray to write binary data to a stream. var data = new ParameterData (); data . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . AddValue ( \"ParameterC\" , Encoding . ASCII . GetBytes ( \"Hello Quix!\" )); // Write binary data as a byte array.","title":"Parameter Data format"},{"location":"sdk/write/#timestamps","text":"The Quix SDK supports common date and time formats for timestamps when adding data to a stream. The SDK gives you several helper functions to add new timestamps to Buffer , ParamaterData , and EventData instances with several types of date/time formats. These are all the common helper functions: Python C# add_timestamp(datetime: datetime) : Add a new timestamp in datetime format. Default epoch will never be added to this. add_timestamp(time: timedelta) : Add a new timestamp in timedelta format since the default epoch determined in the stream. add_timestamp_milliseconds(milliseconds: int) : Add a new timestamp in milliseconds since the default epoch determined in the stream. add_timestamp_nanoseconds(nanoseconds: int) : Add a new timestamp in nanoseconds since the default epoch determined in the stream. AddTimestamp(DateTime dateTime) : Add a new timestamp in DateTime format. Default Epoch will never be added to this. AddTimestamp(TimeSpan timeSpan) : Add a new timestamp in TimeSpan format since the default Epoch determined in the stream. AddTimestampMilliseconds(long timeMilliseconds) : Add a new timestamp in milliseconds since the default Epoch determined in the stream. AddTimestampNanoseconds(long timeNanoseconds) : Add a new timestamp in nanoseconds since the default Epoch determined in the stream.","title":"Timestamps"},{"location":"sdk/write/#epoch","text":"There is a stream property called Epoch (set to 0 by default) that is added to every timestamp (except for datetime formats) when it\u2019s added to the stream. You can use any value you like to act as a base, from which point timestamps will be relative to. The following code indicates to the SDK to add the current date/time to each timestamp added to the stream. Python C# stream . epoch = date . today () stream . Epoch = DateTime . Today ; Adding data without using Epoch property: Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . Write (); Or we can add a timestamp 1000ms from the epoch \"Today\" : Python C# stream . epoch = date . today () stream . parameters . buffer \\ . add_timestamp_milliseconds ( 1000 ) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . write () stream . Epoch = DateTime . Today ; stream . Parameters . Buffer . AddTimestampInMilliseconds ( 1000 ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . Write ();","title":"Epoch"},{"location":"sdk/write/#buffer","text":"The Quix SDK provides a built in Buffer to help you achieve high performance data streaming without the complexity of managing underlying streaming technologies. Instead, you just have to configure the buffer with your requirements using the property Buffer present in the Parameters property of your stream. For example the following configuration means that the SDK will send a packet when the size of the buffer reaches 100 timestamps: Python C# stream . parameters . buffer . packet_size = 100 stream . Parameters . Buffer . PacketSize = 100 ; Writing a ParameterData to that buffer is as simple as using the Write method of that built-in Buffer , passing the ParameterData to write: Python C# stream . parameters . buffer . write ( data ) stream . Parameters . Buffer . Write ( data ); The Quix SDK also allows you to write data to the buffer without creating a ParameterData instance explicitly. To do so, you can use the same helper methods that are supported by the ParameterData class like add_timestamp , add_value or add_tag . At the end, use the write method to write that timestamp to the buffer. This is an example of how to write data to the buffer without using an explicit ParameterData instance: Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , 10 ) \\ . add_value ( \"ParameterB\" , \"hello\" ) \\ . add_value ( \"ParameterC\" , bytearray ( \"hello, Quix!\" , 'utf-8' )) # use bytearray to write binary data to a stream. . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , 10 ) . AddValue ( \"ParameterB\" , \"hello\" ) . AddValue ( \"ParameterC\" , Encoding . ASCII . GetBytes ( \"Hello Quix!\" )) // Write binary data as a byte array. . Write (); You can configure multiple conditions to determine when the Buffer has to release data. If any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer: Python C# buffer.buffer_timeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. buffer.packet_size : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. buffer.time_span_in_nanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. buffer.time_span_in_milliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. buffer.custom_trigger_before_enqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. buffer.custom_trigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. buffer.filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t. Buffer.BufferTimeout : The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer. Buffer.PacketSize : The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released. Buffer.TimeSpanInNanoseconds : The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Buffer.TimeSpanInMilliseconds : The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of time_span_in_nanoseconds . They both work with the same underlying value. Buffer.CustomTriggerBeforeEnqueue : A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it. Buffer.CustomTrigger : A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content. Buffer.Filter : A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t.","title":"Buffer"},{"location":"sdk/write/#examples","text":"This buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timout period, it will flush and empty the buffer anyway. Python C# stream . parameters . buffer . time_span_in_milliseconds = 100 stream . parameters . buffer . buffer_timeout = 1000 stream . Parameters . Buffer . TimeSpanInMilliseconds = 100 ; stream . Parameters . Buffer . BufferTimeout = 1000 ; This buffer configuration will send data every 100ms window or if critical data arrives. Python C# stream . parameters . buffer . time_span_in_milliseconds = 100 stream . parameters . buffer . custom_trigger = lambda data : data . timestamps [ 0 ] . tags [ \"is_critical\" ] == 'True' stream . Parameters . Buffer . TimeSpanInMilliseconds = 100 ; stream . Parameters . Buffer . CustomTrigger = data => data . Timestamps [ 0 ]. Tags [ \"is_critical\" ] == \"True\" ;","title":"Examples"},{"location":"sdk/write/#parameter-definitions","text":"The Quix SDK allows you to define metadata for parameters and events, to describe them. You can define things like human readable names, descriptions, acceptable ranges of values, etc. Quix uses some of this configuration when visualising data on the platform, but you can use also them in your own models, bridges or visualization implementations. Python C# We call this parameter metadata ParameterDefinitions , and all you need to do is to use the add_definition helper function of the stream.parameters property: parameters . add_definition ( parameter_id : str , name : str = None , description : str = None ) We call this parameter metadata ParameterDefinitions , and all you need to do is to use the AddDefinition helper function of the stream.Parameters property: Parameters . AddDefinition ( parameterId : string , name : string = null , description : string = null ) Once you have added a new definition, you can attach some additional configuration to it. This is the whole list of visualization and metadata options you can attach to a ParameterDefinition : Python C# set_range(minimum_value: float, maximum_value: float) : Set the minimum and maximum range of the parameter. set_unit(unit: str) : Set the unit of the parameter. set_format(format: str) : Set the format of the parameter. set_custom_properties(custom_properties: str) : Set the custom properties of the parameter. Example: stream . parameters \\ . add_location ( \"vehicle/ecu\" ) \\ . add_definition ( \"vehicle-speed\" , \"Vehicle speed\" , \"Current vehicle speed measured using wheel sensor\" ) \\ . set_unit ( \"kmh\" ) \\ . set_range ( 0 , 400 ) SetRange(double minimumValue, double maximumValue) : Set the minimum and maximum range of the parameter. SetUnit(string unit) : Set the unit of the parameter. SetFormat(string format) : Set the format of the parameter. SetCustomProperties(string customProperties) : Set the custom properties of the parameter. Example: stream . Parameters . AddLocation ( \"vehicle/ecu\" ) . AddDefinition ( \"vehicle-speed\" , \"Vehicle speed\" , \"Current vehicle speed measured using wheel sensor\" ) . SetUnit ( \"kmh\" ) . SetRange ( 0 , 400 ); The Min and Max range definition sets the Y axis range in the waveform visualisation view. This definition: Python C# . add_definition ( \"Speed\" ) . set_range ( 0 , 400 ) . AddDefinition ( \"Speed\" ). SetRange ( 0 , 400 ) Will set up this view in Visualise: Adding additional Definitions for each parameter allows you to see data with different ranges on the same waveform view: You can also define a Location before adding parameter and event definitions. Locations are used to organize the Parameters and Events in hierarchy groups in the data catalogue. To add a Location you should use the add_location method before adding the definitions you want to include in that group. For example, setting this parameter location: Python C# stream . parameters \\ . add_location ( \"/Player/Motion/Car\" ) \\ . add_definition ( \"Pitch\" ) \\ . add_definition ( \"Roll\" ) \\ . add_definition ( \"Yaw\" ) stream . Parameters . AddLocation ( \"/Player/Motion/Car\" ) . AddDefinition ( \"Pitch\" ) . AddDefinition ( \"Roll\" ) . AddDefinition ( \"Yaw\" ); Will result in this parameter hierarchy in the parameter selection dialogue:","title":"Parameter Definitions"},{"location":"sdk/write/#using-data-frames","text":"If you use the Python version of the SDK you can use Pandas DataFrames for reading and writing ParameterData to Quix. The Pandas DataFrames format is just a representation of ParameterData format, where the Timestamp is mapped to a column named time and the rest of the parameters are mapped as columns named as the ParameterId of the parameter. Tags are mapped as columns with the prefix TAG__ and the TagId of the tag. For example, the following ParameterData : Timestamp CarId (tag) Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 An example of ParameterData Is represented as the following Pandas DataFrame: time TAG__CarId Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 A representation of ParameterData in a Pandas DataFrame The SDK allows you to write data to Quix using Pandas DataFrames directly. You just need to use the common write methods of the stream.parameters and buffer , passing the Data Frame instead of a ParameterData : df = data . to_panda_frame () stream . parameters . buffer . write ( df ) Alternatively, you can convert a Pandas Data Frame to a ParameterData using the method from_panda_frame : data = ParameterData . from_panda_frame ( df ) stream . parameters . buffer . write ( data ) Tip The conversions from Pandas DataFrames to ParameterData have an intrinsic cost overhead. For high-performance models using Pandas DataFrames, you probably want to directly use Pandas DataFrames methods provided by the SDK that are optimized for doing as few conversions as possible.","title":"Using Data Frames"},{"location":"sdk/write/#writing-events","text":"EventData is the formal class in the SDK which represents an Event data packet in memory. EventData is meant to be used for time series data coming from sources that generate data at irregular intervals or without a defined structure. Tip If your data source generates data at regular time intervals, or the information can be organized in a fixed list of Parameters, the ParameterData format is probably a better fit for your time-series data. Writing Events to a stream is identical to writing ParameterData values, although you don\u2019t need to use buffering features because events don\u2019t need high performance throughput.","title":"Writing Events"},{"location":"sdk/write/#event-data-format","text":"EventData consists of a record with a Timestamp, an EventId and an EventValue. You should imagine a list of Event Data instances as a simple table of three columns where the Timestamp is the first column of that table and the EventId and EventValue are the second and third columns. Timestamp EventId EventValue 1 failure23 Gearbox has a failure 2 box-event2 Car has entered to the box 3 motor-off Motor has stopped 6 race-event3 Race has finished An example of a list of EventData The Quix SDK provides several helpers to create and send EventData packets through the stream. The following code would generate the list of EventData shown in the previous example and send it to the stream: Python C# events = [] events . append ( EventData ( \"failure23\" , 1 , \"Gearbox has a failure\" )) events . append ( EventData ( \"box-event2\" , 2 , \"Car has entered to the box\" )) events . append ( EventData ( \"motor-off\" , 3 , \"Motor has stopped\" )) events . append ( EventData ( \"race-event3\" , 6 , \"Race has finished\" )) stream . events . write ( events ) var events = new List < EventData >(); events . Add ( new EventData ( \"failure23\" , 1 , \"Gearbox has a failure\" )); events . Add ( new EventData ( \"box-event2\" , 2 , \"Car has entered to the box\" )); events . Add ( new EventData ( \"motor-off\" , 3 , \"Motor has stopped\" )); events . Add ( new EventData ( \"race-event3\" , 6 , \"Race has finished\" )); stream . Events . Write ( events ) The Quix SDK lets you write Events without creating EventData instances explicitly. To do so, you can use the same helpers present in ParameterData format like add_timestamp , add_value or add_tag . At the end, use the write method to write that timestamp to the stream. This is an example of how to write Events to the stream without using explicit EventData instances: Python C# stream . events \\ . add_timestamp ( 1 ) \\ . add_value ( \"failure23\" , \"Gearbox has a failure\" ) \\ . write () stream . events \\ . add_timestamp ( 2 ) \\ . add_value ( \"box-event2\" , \"Car has entered to the box\" ) \\ . write () stream . events \\ . add_timestamp ( 3 ) \\ . add_value ( \"motor-off\" , \"Motor has stopped\" ) \\ . write () stream . events \\ . add_timestamp ( 6 ) \\ . add_value ( \"race-event3\" , \"Race has finished\" ) \\ . write () stream . Events . AddTimestamp ( 1 ) . AddValue ( \"failure23\" , \"Gearbox has a failure\" ) . Write (); stream . Events . AddTimestamp ( 2 ) . AddValue ( \"box-event2\" , \"Car has entered to the box\" ) . Write (); stream . Events . AddTimestamp ( 3 ) . AddValue ( \"motor-off\" , \"Motor has stopped\" ) . Write (); stream . Events . AddTimestamp ( 6 ) . AddValue ( \"race-event3\" , \"Race has finished\" ) . Write ();","title":"Event Data format"},{"location":"sdk/write/#event-definitions","text":"As with parameters, you can attach Definitions to each event. This is the whole list of visualization and metadata options we can attach to a EventDefinition : set_level(level: EventLevel) : Set severity level of the event. set_custom_properties(custom_properties: str) : Set the custom properties of the event. For example, the following code defines a human readable name and a Severity level for the EventA : Python C# stream . events \\ . add_definition ( \"EventA\" , \"The Event A\" ) \\ . set_level ( EventLevel . Critical ) stream . Events . AddDefinition ( \"EventA\" , \"The Event A\" ). SetLevel ( EventLevel . Critical );","title":"Event Definitions"},{"location":"sdk/write/#tags","text":"The Quix SDK allows you to tag data for ParameterData and EventData packets. Using tags alongside parameters and events helps when indexing persisted data in the database. Tags allow you to filter and group data with fast queries. Tags work as a part of the primary key inside ParameterData and EventData, in combination with the default Timestamp key. If you add data values with the same Timestamps, but a different combination of Tags, the timestamp will be treated as a separate row. For example, the following code: Python C# data = ParameterData () data . add_timestamp_nanoseconds ( 1 ) \\ . add_tag ( \"CarId\" , \"car1\" ) \\ . add_value ( \"Speed\" , 120 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 2 ) \\ . add_tag ( \"CarId\" , \"car1\" ) \\ . add_value ( \"Speed\" , 123 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 3 ) \\ . add_tag ( \"CarId\" , \"car1\" ) \\ . add_value ( \"Speed\" , 125 ) \\ . add_value ( \"Gear\" , 3 ) data . add_timestamp_nanoseconds ( 1 ) \\ . add_tag ( \"CarId\" , \"car2\" ) \\ . add_value ( \"Speed\" , 95 ) \\ . add_value ( \"Gear\" , 2 ) data . add_timestamp_nanoseconds ( 2 ) \\ . add_tag ( \"CarId\" , \"car2\" ) \\ . add_value ( \"Speed\" , 98 ) \\ . add_value ( \"Gear\" , 2 ) data . add_timestamp_nanoseconds ( 3 ) \\ . add_tag ( \"CarId\" , \"car2\" ) \\ . add_value ( \"Speed\" , 105 ) \\ . add_value ( \"Gear\" , 2 ) var data = new ParameterData (); data . AddTimestampNanoseconds ( 1 ) . AddTag ( \"CarId\" , \"car1\" ) . AddValue ( \"Speed\" , 120 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 2 ) . AddTag ( \"CarId\" , \"car1\" ) . AddValue ( \"Speed\" , 123 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 3 ) . AddTag ( \"CarId\" , \"car1\" ) . AddValue ( \"Speed\" , 125 ) . AddValue ( \"Gear\" , 3 ); data . AddTimestampNanoseconds ( 1 ) . AddTag ( \"CarId\" , \"car2\" ) . AddValue ( \"Speed\" , 95 ) . AddValue ( \"Gear\" , 2 ); data . AddTimestampNanoseconds ( 2 ) . AddTag ( \"CarId\" , \"car2\" ) . AddValue ( \"Speed\" , 98 ) . AddValue ( \"Gear\" , 2 ); data . AddTimestampNanoseconds ( 3 ) . AddTag ( \"CarId\" , \"car2\" ) . AddValue ( \"Speed\" , 105 ) . AddValue ( \"Gear\" , 2 ); Will generate the following ParameterData packet: Timestamp CarId Speed Gear 1 car1 120 3 1 car2 95 2 2 car1 123 3 2 car2 98 2 3 car1 125 3 3 car2 105 2 ParameterData with tagged data Warning Tags have to be chosen carefully as excessive cardinality leads to performance degradation in the database. You should use tags only for identifiers and not cardinal values. Good tagging: This will allow you to query the maximum speed for driver identifier \"Peter\". Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_tag ( \"vehicle-plate\" , \"SL96 XCX\" ) \\ . add_tag ( \"driver-id\" , \"Peter\" ) \\ . add_value ( \"Speed\" , 53 ) \\ . add_value ( \"Gear\" , 4 ) \\ . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddTag ( \"vehicle-plate\" , \"SL96 XCX\" ) . AddTag ( \"driver-id\" , \"Peter\" ) . AddValue ( \"Speed\" , 53 ) . AddValue ( \"Gear\" , 4 ) . Write (); Bad tagging: This will lead to excessive cardinality as there will be a massive number of different values for the specified tag, Speed. Python C# stream . parameters . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_tag ( \"Speed\" , 53 ) \\ . add_value ( \"Gear\" , 4 ) \\ . write () stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddTag ( \"Speed\" , 53 ) . AddValue ( \"Gear\" , 4 ) . Write ();","title":"Tags"},{"location":"sdk/write/#minimal-example","text":"This is a minimal code example you can use to write data to a topic using the Quix SDK. Python C# import time import datetime import math from quixstreaming import * # Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument. client = QuixStreamingClient () output_topic = client . open_output_topic ( TOPIC_ID ) stream = output_topic . create_stream () stream . properties . name = \"Hello World python stream\" for index in range ( 0 , 3000 ): stream . parameters \\ . buffer \\ . add_timestamp ( datetime . datetime . utcnow ()) \\ . add_value ( \"ParameterA\" , index ) \\ . write () time . sleep ( 0.01 ) print ( \"Closing stream\" ) stream . close () using System ; using System.Threading ; using Quix.Sdk.Streaming.Configuration ; namespace WriteHelloWorld { class Program { /// <summary> /// Main will be invoked when you run the application /// </summary> static void Main () { // Create a client which holds generic details for creating input and output topics var client = new Quix . Sdk . Streaming . QuixStreamingClient (); using var outputTopic = client . OpenOutputTopic ( TOPIC_ID ); var stream = outputTopic . CreateStream (); stream . Properties . Name = \"Hello World stream\" ; Console . WriteLine ( \"Sending values for 30 seconds\" ); for ( var index = 0 ; index < 3000 ; index ++) { stream . Parameters . Buffer . AddTimestamp ( DateTime . UtcNow ) . AddValue ( \"ParameterA\" , index ) . Write (); Thread . Sleep ( 10 ); } Console . WriteLine ( \"Closing stream\" ); stream . Close (); Console . WriteLine ( \"Done!\" ); } } }","title":"Minimal example"},{"location":"sdk/write/#write-raw-kafka-messages","text":"The Quix SDK uses the message brokers' internal protocol for data transmission. This protocol is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides. However, in some cases, you simply do not have the ability to run the Quix SDK on both sides. To cater for these cases we added the ability to write the raw, unformatted, messages as a byte array. This gives you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ). You can write messages with or without a key. In the following example, we show you how to write 2 messages to Kafka, one message with a key and one without. Python C# inp = client . open_raw_output_topic ( TOPIC_ID ) data = bytearray ( bytes ( \"TEXT CONVERTED TO BYTES\" , 'utf-8' )) #write value with KEY to kafka message = RawMessage ( data ) message . key = MESSAGE_KEY out . write ( message ) #write value without key into kafka out . write ( data ) var out = client . OpenRawOutputTopic ( TOPIC_ID ); inp = client . OpenRawInputTopic ( TOPIC_NAME ) var data = new byte []{ 1 , 3 , 5 , 7 , 1 , 43 }; //write value with KEY to kafka rawWriter . Write ( new Streaming . Raw . RawMessage ( MESSAGE_KEY , data )); //write value withhout key into kafka rawWriter . Write ( new Streaming . Raw . RawMessage ( data )); inp . StartReading ()","title":"Write raw kafka messages"},{"location":"sdk/features/broker-configuration/","text":"Broker configuration Message Broker configurations are not an easy part of setting up a new streaming infrastructure. For instance, Kafka configuration has more than 500 different properties across Server, Topic, Consumer and Producer configurations. Quix handles Kafka configuration efficiently and reliably. We maintain these configurations in order to achieve the best performance and reliability when using the Quix SDK together with managed Kafka topics created inside Quix workspaces. In addition, the Quix SDK uses high-security connections using SSL certificates that encrypt all your data between producers and consumers. These types of secured connection require some complex configuration in the Broker and Client side. We take care of this in the SDK so you don\u2019t have to. Our templates come with pre-configured client certificates and connection settings, so you don\u2019t need to worry about this configuration complexity. Refer to the Connecting to Quix section for more information.","title":"Broker configuration"},{"location":"sdk/features/broker-configuration/#broker-configuration","text":"Message Broker configurations are not an easy part of setting up a new streaming infrastructure. For instance, Kafka configuration has more than 500 different properties across Server, Topic, Consumer and Producer configurations. Quix handles Kafka configuration efficiently and reliably. We maintain these configurations in order to achieve the best performance and reliability when using the Quix SDK together with managed Kafka topics created inside Quix workspaces. In addition, the Quix SDK uses high-security connections using SSL certificates that encrypt all your data between producers and consumers. These types of secured connection require some complex configuration in the Broker and Client side. We take care of this in the SDK so you don\u2019t have to. Our templates come with pre-configured client certificates and connection settings, so you don\u2019t need to worry about this configuration complexity. Refer to the Connecting to Quix section for more information.","title":"Broker configuration"},{"location":"sdk/features/builtin-buffers/","text":"Built-in buffers If you\u2019re sending data at a high frequency, processing time-series without any buffering involved can be very costly. With smaller messages, data compression cannot be implemented as efficiently. The time spent in serializing and deserializing messages is too high to do it at very high rates. On the other hand, an incorrect buffer strategy can introduce high latency values between extremes, that could be not acceptable in some use cases. The Quix SDK provides you with a very high performance, low latency buffer, combined with easy-to-use configurations for reading and writing to give you absolute freedom in balancing between latency and cost. Buffers in the Quix SDK work at the timestamp level. A buffer accumulates certain timestamps with their related values. A packet containing those timestamps and values as a ParameterData package is then released when certain configured conditions match. The logic looks simple, but it\u2019s actually quite complicated when you try to reach a very high performance level, creating at the same time an easy interface for reading and writing time-series data. Our buffer implementation uses short memory allocations and minimizes conversions between raw transport packages and ParameterData format to achieve low CPU and memory consumption, and high throughput. We are happy to say that we have achieved these three things in the SDK \u2014 simplicity, low resource consumption, and very high performance \u2014 and you don\u2019t need to even worry about buffering because it\u2019s provided out-of-box.","title":"Built-in buffers"},{"location":"sdk/features/builtin-buffers/#built-in-buffers","text":"If you\u2019re sending data at a high frequency, processing time-series without any buffering involved can be very costly. With smaller messages, data compression cannot be implemented as efficiently. The time spent in serializing and deserializing messages is too high to do it at very high rates. On the other hand, an incorrect buffer strategy can introduce high latency values between extremes, that could be not acceptable in some use cases. The Quix SDK provides you with a very high performance, low latency buffer, combined with easy-to-use configurations for reading and writing to give you absolute freedom in balancing between latency and cost. Buffers in the Quix SDK work at the timestamp level. A buffer accumulates certain timestamps with their related values. A packet containing those timestamps and values as a ParameterData package is then released when certain configured conditions match. The logic looks simple, but it\u2019s actually quite complicated when you try to reach a very high performance level, creating at the same time an easy interface for reading and writing time-series data. Our buffer implementation uses short memory allocations and minimizes conversions between raw transport packages and ParameterData format to achieve low CPU and memory consumption, and high throughput. We are happy to say that we have achieved these three things in the SDK \u2014 simplicity, low resource consumption, and very high performance \u2014 and you don\u2019t need to even worry about buffering because it\u2019s provided out-of-box.","title":"Built-in buffers"},{"location":"sdk/features/checkpointing/","text":"Checkpointing The Quix SDK allows you to do manual checkpointing when you read data from a Topic. This gives you the ability to inform the Message Broker that you have already processed messages up to one point, usually called a checkpoint . This is a very important concept when you are developing high-performance, streaming applications, processing tons of data in memory. You don\u2019t want to persist a state for each message received because it would cause an unaffordable processing cost, slowing down your streaming speeds and performance. Checkpointing lets you do some of this costly processing at a very low frequency, without having to worry about losing data. If, for some reason, your process is restarted or crashes and you haven\u2019t saved all the in-memory data you are processing, the Message Broker will resend all the messages from the last Checkpoint when you reconnect to the topic. Refer to the Committing / checkpointing section of this documentation to find out how to do Checkpointing when reading data with the Quix Sdk. Checkpointing example Let\u2019s explain the checkpointing concept and its benefits with an easy example. One process is reading and processing data, without saving its state after each input message. This allows good performance and high throughtput of the service but, without checkpointing, risks data loss in the case of failure. The process reads the first four messages, keeping its state in memory. The process commits the messages of the topic (checkpointing) just after reading the first four and saves the in-memory state to the database. The process reads the next four messages, but it crashes just after that, without time to commit the messages. This will not result in data loss because it will begin from the last checkpoint after it restarts. The process restarts and reopens the input topic. It will start reading messages from the last checkpoint resulting in no data loss from the previous crash. The process resumes reading the next five messages, keeping its state in memory. The process commits the messages of the topic just after reading the previous five messages and saves the in-memory state to the database.","title":"Checkpointing"},{"location":"sdk/features/checkpointing/#checkpointing","text":"The Quix SDK allows you to do manual checkpointing when you read data from a Topic. This gives you the ability to inform the Message Broker that you have already processed messages up to one point, usually called a checkpoint . This is a very important concept when you are developing high-performance, streaming applications, processing tons of data in memory. You don\u2019t want to persist a state for each message received because it would cause an unaffordable processing cost, slowing down your streaming speeds and performance. Checkpointing lets you do some of this costly processing at a very low frequency, without having to worry about losing data. If, for some reason, your process is restarted or crashes and you haven\u2019t saved all the in-memory data you are processing, the Message Broker will resend all the messages from the last Checkpoint when you reconnect to the topic. Refer to the Committing / checkpointing section of this documentation to find out how to do Checkpointing when reading data with the Quix Sdk.","title":"Checkpointing"},{"location":"sdk/features/checkpointing/#checkpointing-example","text":"Let\u2019s explain the checkpointing concept and its benefits with an easy example. One process is reading and processing data, without saving its state after each input message. This allows good performance and high throughtput of the service but, without checkpointing, risks data loss in the case of failure. The process reads the first four messages, keeping its state in memory. The process commits the messages of the topic (checkpointing) just after reading the first four and saves the in-memory state to the database. The process reads the next four messages, but it crashes just after that, without time to commit the messages. This will not result in data loss because it will begin from the last checkpoint after it restarts. The process restarts and reopens the input topic. It will start reading messages from the last checkpoint resulting in no data loss from the previous crash. The process resumes reading the next five messages, keeping its state in memory. The process commits the messages of the topic just after reading the previous five messages and saves the in-memory state to the database.","title":"Checkpointing example"},{"location":"sdk/features/data-frames/","text":"Support for Data Frames The Quix SDK supports reading and writing data using Pandas DataFrames . If you use the Python version of the SDK you can make use of this library together with Quix, ensuring maximum optimization for your real-time applications. The SDK uses Pandas DataFrames just as a representation of the common ParameterData format used to read and write data to Quix. For example, the following ParameterData : Timestamp CarId (tag) Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 An example of ParameterData Is represented as the following Pandas Data Frame: time TAG__CarId Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 A representation of ParameterData in a Pandas Data Frame The Quix SDK provides multiple methods and events that work directly with Pandas DataFrames . Please refer to the sections Using Data Frames for reading from Quix and Using Data Frames for writing to Quix for extended information.","title":"Support for Data Frames"},{"location":"sdk/features/data-frames/#support-for-data-frames","text":"The Quix SDK supports reading and writing data using Pandas DataFrames . If you use the Python version of the SDK you can make use of this library together with Quix, ensuring maximum optimization for your real-time applications. The SDK uses Pandas DataFrames just as a representation of the common ParameterData format used to read and write data to Quix. For example, the following ParameterData : Timestamp CarId (tag) Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 An example of ParameterData Is represented as the following Pandas Data Frame: time TAG__CarId Speed Gear 1 car-1 120 3 2 car-2 123 3 3 car-1 125 3 6 car-2 110 2 A representation of ParameterData in a Pandas Data Frame The Quix SDK provides multiple methods and events that work directly with Pandas DataFrames . Please refer to the sections Using Data Frames for reading from Quix and Using Data Frames for writing to Quix for extended information.","title":"Support for Data Frames"},{"location":"sdk/features/data-serialization/","text":"Data serialization Serialization can be painful, especially if it\u2019s done with performance in mind. We serialize and deserialize native ParameterData transport objects, created specifically to be efficient with time series data. On top of that we use codecs like Protobuf that improve overall performance of the serialization and deserialization process by some orders of magnitude. The Quix SDK automatically serializes data from native types in your language. You can work with familiar types, such as Pandas DataFrames , or use our own ParameterData class without worrying about the conversions that happen behind the scenes.","title":"Data serialization"},{"location":"sdk/features/data-serialization/#data-serialization","text":"Serialization can be painful, especially if it\u2019s done with performance in mind. We serialize and deserialize native ParameterData transport objects, created specifically to be efficient with time series data. On top of that we use codecs like Protobuf that improve overall performance of the serialization and deserialization process by some orders of magnitude. The Quix SDK automatically serializes data from native types in your language. You can work with familiar types, such as Pandas DataFrames , or use our own ParameterData class without worrying about the conversions that happen behind the scenes.","title":"Data serialization"},{"location":"sdk/features/horizontal-scaling/","text":"Horizontal scaling The Quix SDK provides horizontal scaling out of the box via the streaming context . This means a data scientist or data engineer does not have to implement parallel processing themselves. Imagine the following example: Each car produces one stream with its own time-series data, and each stream is processed by each replica of the deployment, labelled \"Process\". By default the message broker will assign each stream to one replica via the RangeAssignor strategy . Now, one of the replicas crashes (the purple one), and the \"stream 4\" is assigned automatically to the blue replica. This situation will trigger an event on the SDK in the blue replica indicating that \"stream 4\" has been received: Python C# def read_stream ( new_stream : StreamReader ): print ( \"New stream received:\" + new_stream . stream_id ) input_topic . on_stream_received += read_stream inputTopic . OnStreamReceived += ( s , newStream ) => { Console . WriteLine ( $\"New stream received: {newStream.StreamId}\" ); }; output on blue replica: New stream received: stream 4 When the purple replica has restarted and becomes available again, it takes back control of \"stream 4\". This will trigger two events, one in the blue replica indicating that \"stream 4\" has been revoked, and one in the purple replica indicating that \"stream 4\" has been assigned again: Python C# def read_stream ( new_stream : StreamReader ): print ( \"New stream received:\" + new_stream . stream_id ) def streams_revoked ( streams_revoked : [ StreamReader ]): for stream in streams_revoked : print ( \"Stream revoked:\" + stream . stream_id ) input_topic . on_stream_received += read_stream input_topic . on_streams_revoked += streams_revoked inputTopic . OnStreamReceived += ( s , newStream ) => { Console . WriteLine ( $\"New stream received: {newStream.StreamId}\" ); }; inputTopic . OnStreamsRevoked += ( s , streamsRevoked ) => { foreach ( var stream in streamsRevoked ) { Console . WriteLine ( $\"Stream revoked: {stream.StreamId}\" ); } }; Output on the blue replica: Stream revoked: stream 4 Output on the purple replica: New stream received: stream 4 The same behaviour will happen if we scale the \"Process\" deployment up or down, increasing or decreasing the number of replicas. Kafka will trigger the rebalacing mechanism internally and this will trigger the same events on the Quix SDK. Rebalancing mechanism and Partitions Kafka uses partitions and the RangeAssignor strategy to decide which consumers receive which messages. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation behind the Quix SDK. You don\u2019t need to worry about them because everything is abstracted within the Streaming Context feature of the SDK. The events described above will remain the same, even if the SDK uses another Message Broker technology or another rebalancing mechanism in the future. Warning Because of how the Kafka rebalancing mechanism works, you should follow one golden rule: you cannot have more replicas than the number of partitions the input Topic has.","title":"Horizontal scaling"},{"location":"sdk/features/horizontal-scaling/#horizontal-scaling","text":"The Quix SDK provides horizontal scaling out of the box via the streaming context . This means a data scientist or data engineer does not have to implement parallel processing themselves. Imagine the following example: Each car produces one stream with its own time-series data, and each stream is processed by each replica of the deployment, labelled \"Process\". By default the message broker will assign each stream to one replica via the RangeAssignor strategy . Now, one of the replicas crashes (the purple one), and the \"stream 4\" is assigned automatically to the blue replica. This situation will trigger an event on the SDK in the blue replica indicating that \"stream 4\" has been received: Python C# def read_stream ( new_stream : StreamReader ): print ( \"New stream received:\" + new_stream . stream_id ) input_topic . on_stream_received += read_stream inputTopic . OnStreamReceived += ( s , newStream ) => { Console . WriteLine ( $\"New stream received: {newStream.StreamId}\" ); }; output on blue replica: New stream received: stream 4 When the purple replica has restarted and becomes available again, it takes back control of \"stream 4\". This will trigger two events, one in the blue replica indicating that \"stream 4\" has been revoked, and one in the purple replica indicating that \"stream 4\" has been assigned again: Python C# def read_stream ( new_stream : StreamReader ): print ( \"New stream received:\" + new_stream . stream_id ) def streams_revoked ( streams_revoked : [ StreamReader ]): for stream in streams_revoked : print ( \"Stream revoked:\" + stream . stream_id ) input_topic . on_stream_received += read_stream input_topic . on_streams_revoked += streams_revoked inputTopic . OnStreamReceived += ( s , newStream ) => { Console . WriteLine ( $\"New stream received: {newStream.StreamId}\" ); }; inputTopic . OnStreamsRevoked += ( s , streamsRevoked ) => { foreach ( var stream in streamsRevoked ) { Console . WriteLine ( $\"Stream revoked: {stream.StreamId}\" ); } }; Output on the blue replica: Stream revoked: stream 4 Output on the purple replica: New stream received: stream 4 The same behaviour will happen if we scale the \"Process\" deployment up or down, increasing or decreasing the number of replicas. Kafka will trigger the rebalacing mechanism internally and this will trigger the same events on the Quix SDK.","title":"Horizontal scaling"},{"location":"sdk/features/horizontal-scaling/#rebalancing-mechanism-and-partitions","text":"Kafka uses partitions and the RangeAssignor strategy to decide which consumers receive which messages. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation behind the Quix SDK. You don\u2019t need to worry about them because everything is abstracted within the Streaming Context feature of the SDK. The events described above will remain the same, even if the SDK uses another Message Broker technology or another rebalancing mechanism in the future. Warning Because of how the Kafka rebalancing mechanism works, you should follow one golden rule: you cannot have more replicas than the number of partitions the input Topic has.","title":"Rebalancing mechanism and Partitions"},{"location":"sdk/features/in-memory-processing/","text":"In-memory processing Traditional architectures for applications that need to process data have always been very database-centric. This means that, when you needed to process data and get some value out of it, everything had to pass through a database several times. This approach worked when the amount of data to process was relatively low, and the latency needed was on the scale of \"days\". But with a world changing to more real-time use cases where you need results on the scale of seconds or nanoseconds, and where you can get millions of IoT devices sending data to process at the same time, traditional database-centric architectures just explode. The Quix SDK uses a message broker and it puts it at the very center of the application, enabling a new approach for processing data without the need to save and pass all the information through a database. By using in-memory processing , you can persist only the data you\u2019re really interested in keeping. This approach lowers the complexity and cost of real-time data processing by several orders of magnitude and, in fact, it is the only possible approach when you need to process a huge amount of data per second with low latency requirements. The SDK offers you a very simple way of processing time-series data in real-time. Refer to the Processing data section of this documentation for further details.","title":"In-memory processing"},{"location":"sdk/features/in-memory-processing/#in-memory-processing","text":"Traditional architectures for applications that need to process data have always been very database-centric. This means that, when you needed to process data and get some value out of it, everything had to pass through a database several times. This approach worked when the amount of data to process was relatively low, and the latency needed was on the scale of \"days\". But with a world changing to more real-time use cases where you need results on the scale of seconds or nanoseconds, and where you can get millions of IoT devices sending data to process at the same time, traditional database-centric architectures just explode. The Quix SDK uses a message broker and it puts it at the very center of the application, enabling a new approach for processing data without the need to save and pass all the information through a database. By using in-memory processing , you can persist only the data you\u2019re really interested in keeping. This approach lowers the complexity and cost of real-time data processing by several orders of magnitude and, in fact, it is the only possible approach when you need to process a huge amount of data per second with low latency requirements. The SDK offers you a very simple way of processing time-series data in real-time. Refer to the Processing data section of this documentation for further details.","title":"In-memory processing"},{"location":"sdk/features/integrations/","text":"Integrations The Quix SDK offers very useful integrations with the Quix SaaS platform out of the box, some of them to persist and query historical data, and some of them to produce and consume data for uses cases where you cannot use the Quix SDK directly, like Web applications: Data persistence: Make any Topic persist all the streaming data it receives, with the click of a button on Quix Portal. Query API : Query historical data with powerful filters and aggregation capabilities. Streaming Reader API : Consume stream data in real time with a Websockets interface. Used for Web applications use cases. Streaming Writer API : Push streaming data into a Topic using REST via HTTP endpoints. Used for Web applications use cases.","title":"Integrations"},{"location":"sdk/features/integrations/#integrations","text":"The Quix SDK offers very useful integrations with the Quix SaaS platform out of the box, some of them to persist and query historical data, and some of them to produce and consume data for uses cases where you cannot use the Quix SDK directly, like Web applications: Data persistence: Make any Topic persist all the streaming data it receives, with the click of a button on Quix Portal. Query API : Query historical data with powerful filters and aggregation capabilities. Streaming Reader API : Consume stream data in real time with a Websockets interface. Used for Web applications use cases. Streaming Writer API : Push streaming data into a Topic using REST via HTTP endpoints. Used for Web applications use cases.","title":"Integrations"},{"location":"sdk/features/message-compression/","text":"Message compression The Quix SDK uses efficient ParameterData transport objects to transmit messages through the message broker, reducing them by an average factor of 10 times compared with plain JSON conversion. On top of that, we apply codecs like Protobuf and Gzip compression to achieve the best performance possible with the minimum payload. All these improvements are completely transparent and you don\u2019t even need to worry about them because everything happens behind the scenes.","title":"Message compression"},{"location":"sdk/features/message-compression/#message-compression","text":"The Quix SDK uses efficient ParameterData transport objects to transmit messages through the message broker, reducing them by an average factor of 10 times compared with plain JSON conversion. On top of that, we apply codecs like Protobuf and Gzip compression to achieve the best performance possible with the minimum payload. All these improvements are completely transparent and you don\u2019t even need to worry about them because everything happens behind the scenes.","title":"Message compression"},{"location":"sdk/features/message-splitting/","text":"Message splitting Message Brokers always have some limitations, by design, with the maximum size of message they can handle. For example, Kafka has a 1MB limit by default, and it\u2019s not recommended to increase this number, for performance reasons. This can be a problem in some use cases where time-series data are big binary chunks, such as sound or video. The Quix SDK automatically handles large messages on the producer side, splitting them up if required and merging them back together at the consumer side, in a totally transparent way. This feature gives you an unlimited message size, independent of the Message Broker used behind.","title":"Message splitting"},{"location":"sdk/features/message-splitting/#message-splitting","text":"Message Brokers always have some limitations, by design, with the maximum size of message they can handle. For example, Kafka has a 1MB limit by default, and it\u2019s not recommended to increase this number, for performance reasons. This can be a problem in some use cases where time-series data are big binary chunks, such as sound or video. The Quix SDK automatically handles large messages on the producer side, splitting them up if required and merging them back together at the consumer side, in a totally transparent way. This feature gives you an unlimited message size, independent of the Message Broker used behind.","title":"Message splitting"},{"location":"sdk/features/multiple-data-types/","text":"Multiple data types The Quix SDK lets you attach any type of data \u2014 Numbers, Strings, or raw Binary data \u2014 to your timestamps. For example, with Quix you can send telemetry data from your vehicle or IoT device and attach a picture or video frame to the same timestamp. This gives the SDK the ability to adapt to any streaming application use case, from plain telemetry data to video streaming, or a mix of both.","title":"Multiple data types"},{"location":"sdk/features/multiple-data-types/#multiple-data-types","text":"The Quix SDK lets you attach any type of data \u2014 Numbers, Strings, or raw Binary data \u2014 to your timestamps. For example, with Quix you can send telemetry data from your vehicle or IoT device and attach a picture or video frame to the same timestamp. This gives the SDK the ability to adapt to any streaming application use case, from plain telemetry data to video streaming, or a mix of both.","title":"Multiple data types"},{"location":"sdk/features/portability/","text":"Portability The Quix SDK works as an abstraction layer on top of a concrete broker technology like Kafka. The SDK sits on top of a Kafka layer and solves all the common problems you might face when processing time-series data using the Kafka technology. However, in future, we can use different message broker technologies without changing the common SDK interface. Using the Quix SDK, you\u2019re not locked into a specific broker technology. You can innovate over time and change the underlying technologies without changing a single line of code of your solution.","title":"Portability"},{"location":"sdk/features/portability/#portability","text":"The Quix SDK works as an abstraction layer on top of a concrete broker technology like Kafka. The SDK sits on top of a Kafka layer and solves all the common problems you might face when processing time-series data using the Kafka technology. However, in future, we can use different message broker technologies without changing the common SDK interface. Using the Quix SDK, you\u2019re not locked into a specific broker technology. You can innovate over time and change the underlying technologies without changing a single line of code of your solution.","title":"Portability"},{"location":"sdk/features/streaming-context/","text":"Streaming context Using a plain Broker SDK out of box only enables you to send messages independently, without any relationship between them. The Quix SDK handles stream context for you, so all the data from one data source is bundled in the same scope. This supports, among other things, automatic horizontal scaling of your models when you deal with a undertermined or big number of data sources. The SDK simplifies the processing of streams by providing callbacks on the reading side. When processing stream data, you can identify data from different streams more easily than with the key-value approach and single messages used by other technologies. The SDK also allows you to attach metadata to streams, like ids, references, or any other type of information related to the data source. This metadata can be read in real time by the SDK itself or via the Query API , if you choose to persist the streams into the database.","title":"Streaming context"},{"location":"sdk/features/streaming-context/#streaming-context","text":"Using a plain Broker SDK out of box only enables you to send messages independently, without any relationship between them. The Quix SDK handles stream context for you, so all the data from one data source is bundled in the same scope. This supports, among other things, automatic horizontal scaling of your models when you deal with a undertermined or big number of data sources. The SDK simplifies the processing of streams by providing callbacks on the reading side. When processing stream data, you can identify data from different streams more easily than with the key-value approach and single messages used by other technologies. The SDK also allows you to attach metadata to streams, like ids, references, or any other type of information related to the data source. This metadata can be read in real time by the SDK itself or via the Query API , if you choose to persist the streams into the database.","title":"Streaming context"}]}